[{"content":" â€œ ç®€å•çš„ä»‹ç» â€ Â· åˆè§LSTMğŸ¤¦â€â™‚ï¸ 2022å¹´çš„ç¾èµ›ä¹‹é™…ï¼Œå·¨å¼±ç¬¬ä¸€æ¬¡é‚‚é€…LSTMï¼Œå½“æ—¶å·¨å¼±çš„ç¾èµ›é¢˜æ˜¯å…³äºæ—¶åºé¢„æµ‹ã€åˆ¶å®šäº¤æ˜“ç­–ç•¥æ–¹é¢ï¼Œèµ›å‰é˜Ÿé•¿è®©å·¨å¼±å»æ¥è§¦RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰å’Œ Transformerï¼Œäºæ˜¯ä¹åœ¨å¯’å‡äº†è§£è¿™ä¸¤æ¨¡å‹æ—¶ï¼Œå·¨å¼±ç¢°å·§ä¹Ÿæ¥è§¦åˆ°äº†LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†æ¨¡å‹ï¼‰ï¼Œå½“æ—¶çš„å·¨å¼±éš¾ä»¥å¯é½¿ï¼Œä»€ä¹ˆè®°å¿†é—¨ã€é—å¿˜é—¨ï¼Œå®åœ¨æ˜¯è®©ä¸€ä¸ªä½æ°´å¹³çš„äººèƒ½ä»¥ç†è§£ï¼Œå¥½åœ¨å½“æ—¶çš„å·¨å¼±æœ‰äº›ç¥ç»ç½‘ç»œçš„åŸºç¡€å¹¶ä¸”ç¡¬ç€å¤´çš®åœ¨ç”ŸåƒRNNèµ„æ–™åï¼Œå†å»ç†è§£LSTMæœ€åç»ˆäºç®—æ˜¯å…¥äº†ä¸ªé—¨ã€‚ä¸è¿‡å¯æƒœåæ¥ç¾èµ›å¹¶æœªä½¿ç”¨åˆ°LSTMï¼Œå› æ­¤å·¨å¼±ä¸å®ƒçš„å…³ç³»ç®—æ˜¯å‘Šä¸€æ®µè½ã€‚\nÂ· å†è§LSTMğŸ¤¦â€â™€ï¸ ä¸Šå¤©æ³¨å®šï¼Œæœºç¼˜å·§åˆï¼Œå‰æ®µæ—¶é—´å·¨å¼±é‡åˆ°äº†æ—¶é—´åºåˆ—æ–¹é¢çš„é¢„æµ‹é—®é¢˜éœ€è¦è§£å†³ï¼Œå·¨å¼±æœéè„‘æµ·é™¤äº†ARIMAç­‰ï¼Œå°±æ˜¯LSTMäº†ï¼Œäºæ˜¯ä¹è¿™æ®µå°å°˜å·²ä¹…çš„æ•…äº‹åˆå†æ¬¡ç»­å†™ã€‚è¿™æ¬¡æ­å»ºçš„LSTMç¡®å®æ²¡è®©äººå¤±æœ›ï¼Œæ‹Ÿåˆåº¦ç›´æ¥æ¥è¿‘å®Œç¾ï¼Œä¸å¾—ä¸è¯´è¿™è®©æˆ‘å¯¹LSTMçš„æ„Ÿæƒ…å¥½åƒä¸Šå‡äº†ä¸€ä¸ªçº§åˆ«(/Ï‰ï¼¼)ã€‚ ä¸‹é¢æ˜¯ä¸€å¼ å·¨å¼±ä½¿ç”¨pythonä»£ç å¯¹ kaggle: ads æ•°æ®é›†æ­å»ºLSTMæ¨¡å‹çš„è®­ç»ƒæ‹Ÿåˆå›¾ï¼Œæ•ˆæœçœ‹ä¸Šå»æ˜¯ä¸æ˜¯è¿˜å¯ä»¥ğŸ˜Šï¼š\nâ€œ LSTMç®€ä»‹ â€ LSTMï¼ˆLong short-term memory, é•¿çŸ­æœŸè®°å¿†ï¼‰ æ˜¯ä¸€ç§ç‰¹æ®Šçš„ RNNï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³é•¿åºåˆ—è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œç›¸æ¯”æ™®é€šçš„RNNï¼ŒLSTMèƒ½å¤Ÿåœ¨æ›´é•¿çš„åºåˆ—ä¸­æœ‰æ›´å¥½çš„è¡¨ç°ã€‚LSTM é€šè¿‡é—¨æ§çŠ¶æ€æ¥æ§åˆ¶ä¼ è¾“çŠ¶æ€ï¼Œä»è€Œè®°ä½éœ€è¦é•¿æ—¶é—´è®°å¿†çš„ï¼ŒåŒæ—¶å¿˜è®°ä¸é‡è¦çš„ä¿¡æ¯ï¼Œå¯¹å¾ˆå¤šéœ€è¦ â€œé•¿æœŸè®°å¿†â€ çš„ä»»åŠ¡å°¤å…¶å¥½ç”¨ã€‚\nè¿™é‡Œæ˜¯å·¨å¼±æ‰¾çš„ä¸€ç¯‡å…³äºLSTMåŸç†çš„å®Œæ•´è®²è§£ï¼šUnderstanding LSTM Networks\nâ€œ ä»£ç è¯´æ˜ â€ åªéœ€è¦å‡†å¤‡å¥½indexä¸ºæ—¶é—´ä¸”è¿ç»­ä»¥åŠå°†éœ€è¦æ‹Ÿåˆé¢„æµ‹çš„æ—¶åºå˜é‡å•ç‹¬ä½œä¸º1åˆ—çš„df_xæ•°æ®é›†ï¼Œå³å¯è¿è¯¥æ¨¡å‹ä»£ç ï¼Œä»£ç çš„æœ«å°¾è®¾ç½®äº†look_backå’Œpredict_lengthå‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºæ¨¡å‹æ‹Ÿåˆé¢„æµ‹çš„ç»“æœä¸»è¦ä»¥è¿‡å» look_back = nå¤©ä½œä¸ºå‚è€ƒè¿›è¡Œé¢„æµ‹å’Œä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹é¢„æµ‹é•¿åº¦ä¸º predict_length çš„æœªæ¥æ•°æ®ï¼Œé»˜è®¤ä¸ºlook_back = 1ï¼Œpredict_length = 30 å¤§å®¶å¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚\né™¤æ­¤ä¹‹å¤–ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥å¯¹lstm_modelå‡½æ•°ä¸­çš„ç»“æ„è¿›è¡Œä¿®æ”¹ï¼Œä»¥æ­¤æ ¹æ®éœ€æ±‚ä¼˜åŒ–å¹¶é€‰æ‹©æœ€ä¼˜çš„LSTMæ¨¡å‹ã€‚\nå¯¼å…¥å¿…è¦çš„åŒ… 1 2 3 4 5 6 7 8 9 10 import statsmodels.api as sm from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_absolute_error , mean_squared_error from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization from tensorflow.keras.models import Sequential from pylab import * import seaborn as sns from matplotlib.font_manager import FontProperties mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] æ¨¡å‹çš„æ­å»ºå’Œè®­ç»ƒ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def data_lstm(df): # 1. æ—¶åºæ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†åŠè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ† scale = MinMaxScaler(feature_range = (0, 1)) df = scale.fit_transform(df) train_size = int(len(df) * 0.80) test_size = len(df) - train_size # 2. ç”±äºæ—¶åºå˜é‡å…·æœ‰å…ˆåå…³ç³»ï¼Œå› æ­¤åˆ’åˆ†æ•°æ®é›†æ—¶ä¸€èˆ¬å…ˆå‰ä½œä¸ºè®­ç»ƒé›†ã€åè€…ä½œä¸ºéªŒè¯é›† train, test = df[0:train_size, :], df[train_size:len(df), :] return(train_size, test_size, train, test, df, scale) # 2. lookback è¡¨ç¤ºä»¥è¿‡å»çš„å‡ ä¸ªæ—¥æœŸä½œä¸ºä¸»è¦é¢„æµ‹å˜é‡,è¿™é‡Œæˆ‘é€‰æ‹©çš„é»˜è®¤ä¸º1 # è¾“å…¥æ•°æ®é›† å’Œ è¾“å‡ºæ•°æ®é›† çš„çš„å»ºç«‹ def create_data_set(dataset, look_back=1): data_x, data_y = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i:(i+look_back), 0] data_x.append(a) data_y.append(dataset[i + look_back, 0]) return np.array(data_x), np.array(data_y) # 3. è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®è½¬åŒ– def lstm(train, test, look_back=1): X_train,Y_train,X_test,Y_test = [],[],[],[] X_train,Y_train = create_data_set(train, look_back) X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) X_test, Y_test = create_data_set(test, look_back) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) return (X_train, Y_train, X_test, Y_test) # 4. å®šä¹‰LSTMæ¨¡å‹ç»“æ„ï¼Œ å†…éƒ¨çš„ç»“æ„å‚æ•°å¯ä»¥æ ¹æ¨¡å‹çš„æ‹Ÿåˆç»“æœè¿›è¡Œä¿®æ”¹ def lstm_model(X_train, Y_train, X_test, Y_test): # ç¬¬ä¸€å±‚ï¼Œ256ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor = Sequential() regressor.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1))) regressor.add(Dropout(0.3)) # ç¬¬äºŒå±‚ï¼Œ128ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor.add(LSTM(units = 128, return_sequences = True)) regressor.add(Dropout(0.3)) # ç¬¬ä¸‰å±‚ï¼Œ128ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor.add(LSTM(units = 128)) regressor.add(Dropout(0.3)) regressor.add(Dense(units = 1)) regressor.compile(optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mean_squared_error\u0026#39;) # æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·® reduce_lr = ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, patience=5) # ä¸‹é¢çš„å‚æ•°éƒ½å¯ä»¥è¿›è¡Œä¿®æ”¹ï¼Œä¸€èˆ¬è€Œè¨€batchsizeè¶Šå¤§ä¼šè¶Šå¥½äº›ï¼Œepochsè¡¨ç¤ºè¿­ä»£æ¬¡æ•°ï¼Œå¤§å®¶æ ¹æ®ç»“æœï¼Œå¤§æ¦‚ä½•æ—¶æ”¶æ•›å³å¯å®šä¸ºå¤šå°‘ history =regressor.fit(X_train, Y_train, epochs = 80, batch_size = 8,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False) return(regressor, history) # 5. æ¨¡å‹è®­ç»ƒ def loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history): train_predict = regressor.predict(X_train) test_predict = regressor.predict(X_test) # å°†é¢„æµ‹å€¼è¿›è¡Œåæ ‡å‡†åŒ–ï¼Œå³è¿˜åŸ train_predict = scale.inverse_transform(train_predict) Y_train = scale.inverse_transform([Y_train]) test_predict = scale.inverse_transform(test_predict) Y_test = scale.inverse_transform([Y_test]) # è¾“å‡ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç»å¯¹è¯¯å·®å’Œå‡æ–¹è¯¯å·® print(\u0026#39;Train Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_train[0], train_predict[:,0])) print(\u0026#39;Train Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))) print(\u0026#39;Test Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_test[0], test_predict[:,0])) print(\u0026#39;Test Root Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))) # æŸå¤±å€¼ç»“æœ å¯è§†åŒ– plt.figure(figsize=(16,8)) plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Train Loss\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;Test Loss\u0026#39;) plt.title(\u0026#39;model loss\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.xlabel(\u0026#39;epochs\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() return(train_predict, test_predict, Y_train, Y_test) # 6. ç»˜åˆ¶æ‹Ÿåˆå›¾ï¼Œå¯¹æœªæ¥è¿›è¡Œé¢„æµ‹ def Y_pre(Y_train, Y_test, train_predict, test_predict): Y_real = np.vstack((Y_train.reshape(-1,1), Y_test.reshape(-1,1))) Y_pred = np.vstack((train_predict[:,0].reshape(-1,1), test_predict[:,0].reshape(-1,1))) return(Y_real, Y_pred) def plot_compare(n, Y_real, Y_pred): aa=[x for x in range(n)] plt.figure(figsize=(14,6)) plt.plot(aa, Y_real, marker=\u0026#39;.\u0026#39;, label=\u0026#34;actual\u0026#34;) plt.plot(aa, Y_pred, \u0026#39;r\u0026#39;, label=\u0026#34;prediction\u0026#34;) plt.tight_layout() sns.despine(top=True) plt.subplots_adjust(left=0.07) plt.xticks(size= 15) plt.yticks(size= 15) plt.xlabel(\u0026#39;Time step\u0026#39;, size=15) plt.legend(fontsize=15) plt.show() # 7. æ ¹æ®ä¸€ä¸ªçœŸå®çš„å€¼é¢„æµ‹è¿ç»­çš„é•¿åº¦ def predict_sequences_multiple(model, firstValue, length, look_back=1): prediction_seqs = [] curr_frame = firstValue for i in range(length): predicted = [] predicted.append(model.predict(curr_frame[-look_back:])[0,0]) curr_frame = np.insert(curr_frame, i+look_back, predicted[-1], axis=0) prediction_seqs.append(predicted[-1]) return prediction_seqs è¿è¡Œæ¨¡å‹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026#39;\u0026#39;\u0026#39;åªéœ€è¦å‡†å¤‡å¥½indexä¸ºæ—¶é—´ä¸”è¿ç»­ï¼Œæ—¶åºå˜é‡å•ç‹¬ä¸º1åˆ—çš„df_xæ•°æ®æ¡†ï¼Œå³å¯è¿è¯¥æ¨¡å‹ä»£ç \u0026#39;\u0026#39;\u0026#39; look_back = 1 # å†æ¬¡è¯´æ˜ look_back = n è¡¨ç¤ºé¢„æµ‹ç»“æœä¸»è¦ä»¥è¿‡å» n å¤©ä½œä¸ºå‚è€ƒè¿›è¡Œé¢„æµ‹ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æƒ³æ³•è‡ªè¡Œä¿®æ”¹ predict_length = 30 # é¢„æµ‹æœªæ¥30å¤©çš„æ•°æ®ï¼Œä¸ªäººéœ€è¦æ ¹æ®è‡ªå·±å¯¹æœªæ¥é¢„æµ‹å¤©æ•°çš„éœ€æ±‚è¿›è¡Œé•¿åº¦æ”¹å˜ï¼Œæ­£å¸¸è€Œè¨€çŸ­æœŸå†…çš„é¢„æµ‹ä¼šè¾ƒä¸ºå‡†ç¡® train_size, test_size, train, test, df_x, scale = data_lstm(df_x) X_train, Y_train, X_test, Y_test = lstm(train[:, 0].reshape(train_size, 1), test[:, 0].reshape(test_size, 1), look_back = look_back) regressor, history = lstm_model(X_train, Y_train, X_test, Y_test) train_predict, test_predict, Y_train, Y_test = loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history) Y_real, Y_pre = Y_pre(Y_train, Y_test, train_predict, test_predict) plot_compare(len(Y_real), Y_real, Y_pre) predictions = predict_sequences_multiple(regressor, X_test[-1,:], predict_length, look_back = look_back) \u0026#39;\u0026#39;\u0026#39;é¢„æµ‹æœªæ¥30å¤©çš„æ•°æ®å¹¶ä¿å­˜è‡³pre_30æ•°æ®æ¡†\u0026#39;\u0026#39;\u0026#39; pre_30 = scale.inverse_transform(np.array(predictions).reshape(-1, 1)) ","permalink":"https://soso010816.github.io/posts/lstm/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ Â· åˆè§LSTMğŸ¤¦â€â™‚ï¸ 2022å¹´çš„ç¾èµ›ä¹‹é™…ï¼Œå·¨å¼±ç¬¬ä¸€æ¬¡é‚‚é€…LSTMï¼Œå½“æ—¶å·¨å¼±çš„ç¾èµ›é¢˜æ˜¯å…³äºæ—¶åºé¢„æµ‹ã€åˆ¶å®šäº¤æ˜“ç­–ç•¥æ–¹é¢ï¼Œèµ›å‰é˜Ÿé•¿","title":"DLï¼šLSTM With Python(Time Series Question)"},{"content":" â€œ ç®€å•çš„ä»‹ç» â€ åˆæ¬¡æ¥è§¦XGBoost ç›¸ä¿¡å¤§éƒ¨åˆ†æ¥è§¦è¿‡æ•°æ¨¡ã€æœºå™¨å­¦ä¹ çš„æœ‹å‹éƒ½å¯¹XGBoostæœ‰æ‰€è€³é—»ï¼Œå·¨å¼±æˆ‘ç¬¬ä¸€æ¬¡æ¥è§¦å®ƒæ˜¯åœ¨2021å¹´12æœˆç¬¬ä¸€æ¬¡å‚åŠ æ•°æ¨¡æ¯”èµ›æ—¶ï¼Œå½“æ—¶è‡ªå·±çš„ä»£ç æ°´å¹³è¿˜åªæ˜¯å…¥é—¨ï¼Œä½œä¸ºwrite è®ºæ–‡å’Œåˆ†æ‹…éƒ¨åˆ†å»ºæ¨¡çš„æˆ‘ä¸ºäº†èƒ½å¤Ÿç»™é˜Ÿå‹æ­å»ºçš„XGBoostè¿›è¡Œè¡¨é¢æ¶¦è‰²å’Œæå‡ï¼Œå·¨å¼±åªèƒ½ä¸€ä¸ªåŠ²åœ°ç¿»æ‰¾å„ç§èµ„æ–™ã€æ•™ç¨‹å»ç†è§£å…³äºXGBoostçš„åŸç†ã€‚\nç”±äºåˆšæ¥è§¦ç»Ÿè®¡å­¦ä¸åˆ°åŠå¹´ï¼Œä»å®ƒçš„åº•å±‚æ¶æ„å†³ç­–æ ‘ã€GBDTæ¢¯åº¦æå‡æ ‘å†åˆ°XGBoostä¸€ä¸ªä¸ªæ…¢æ…¢æ¶ˆåŒ–ï¼Œè¿™é˜¶æ®µå±å®èŠ±äº†å·¨å¼±å¤§é‡çš„æ—¶é—´ã€‚æœ€ååœ¨å°†æ¨¡å‹çš„å†…éƒ¨ç»“æ„ã€æ•°å­¦æ¨å¯¼åŸºæœ¬ç†è§£åï¼Œç»“åˆå¤§ä½¬é˜Ÿå‹çš„æœ€ä¼˜å»ºæ¨¡æ‹¿åˆ°äº†å…¨å›½ä¸€ç­‰å¥–ï¼ˆå½“ç„¶æˆ‘è®¤ä¸ºè·å¥–ä¸»è¦çš„åŸå› è¿˜æ˜¯åœ¨è§£å†³å¦ä¸€ä¸ªé—®é¢˜æ—¶ç”¨åˆ°äº†å¤æ‚çš„çº¯æ•°å­¦æ¨å¯¼å’Œæ¦‚ç‡è®ºå»ºæ¨¡ï¼‰ã€‚\nå› æ­¤ä¸å¯å¦è®¤XGBoostä½œä¸ºâ€œæœºå™¨å­¦ä¹ å¤§æ€å™¨â€çš„é­…åŠ›æ‰€åœ¨ã€‚\nXGBooståŸºæœ¬ä»‹ç» XGBoostç”±GBDTä¼˜åŒ–è€Œæˆï¼ŒXGBooståœ¨GBDTçš„åŸºç¡€ä¹‹ä¸Šï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æ­£åˆ™é¡¹ï¼Œå¹¶å¯¹æŸå¤±å‡½æ•°è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ã€å‰”é™¤å¸¸æ•°é¡¹ï¼Œæœ€åä½¿ç”¨è´ªå¿ƒç®—æ³•è®¡ç®—å¢ç›Šåå¯¹å…¶æ ‘çš„æœ€ä½³åˆ†å‰²ç‚¹è¿›è¡Œåˆ†å‰²ï¼Œæå¤§ç¨‹åº¦æé«˜äº†ç›®æ ‡çš„ä¼˜åŒ–æ•ˆç‡ï¼Œå‡å°‘äº†å†…å­˜çš„æ¶ˆè€—ï¼›è€ŒGBDTåˆæ˜¯ç”±å¤šæ£µè¿­ä»£çš„å›å½’æ ‘ç´¯åŠ æ„æˆï¼Œæ¯ä¸€æ£µå›å½’æ ‘å­¦ä¹ çš„æ˜¯ä¹‹å‰æ‰€æœ‰æ ‘çš„ç»“è®ºå’Œæ®‹å·®ï¼Œæ‹Ÿåˆå¾—åˆ°ä¸€ä¸ªå½“å‰çš„æ®‹å·®å›å½’æ ‘ï¼Œæ®‹å·®çš„å…¬å¼ï¼šæ®‹å·® = çœŸå®å€¼ - é¢„æµ‹å€¼ã€‚\nå…·ä½“çš„åŸç†ä»‹ç»å¦‚ä¸‹ï¼šXGBoostçš„åŸºæœ¬ä»‹ç»(å‡ºè‡ªçŸ¥ä¹)\nâ€œ pythonä»£ç çš„ä»‹ç» â€ æœ¬æ¬¡ä»£ç æºäºå·¨å¼±å‰æ®µæ—¶é—´åœ¨è§£å†³ä¸€äº›ä½ç»´æ•°æ®åˆ†ç±»çš„é—®é¢˜ä¸Šå¤šæ¬¡ä½¿ç”¨åˆ°XGBoostï¼Œå¹¶ä¸”æ•ˆæœéå¸¸ä¸é”™ï¼Œå¾®è°ƒå‚æ•°ä¹‹åå‡†ç¡®ç‡å¤§éƒ¨åˆ†éƒ½è¾¾åˆ°äº† 95% ä»¥ä¸Šï¼Œå› æ­¤å·¨å¼±å°†å…¶è¿›è¡Œå†å°è£…ï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨æ—¶èƒ½å‡å°‘æ—¶é—´ï¼Œæå‡æ•ˆç‡ã€‚ä»£ç å¦‚ä¸‹ï¼š\nå¯¼å…¥åŒ… 1 2 3 4 from xgboost import plot_importance from sklearn.model_selection import train_test_split # åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›† from xgboost import XGBRegressor from sklearn.preprocessing import StandardScaler åªéœ€è¦å‡†å¤‡å¥½df_xï¼Œdf_yï¼Œåˆ†åˆ«ä¸ºè‡ªå˜é‡é›†å’Œè¾“å‡ºå€¼çš„æ•°æ®é›†ï¼Œå³å¯è¿è¡Œåˆ†è£…å¥½çš„XGBoostæ¨¡å‹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=7) #7 def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst def xgboost_plot(i = \u0026#39;æ•°å€¼\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False): # i ä¸ºè¾“å‡ºå˜é‡åç§°ï¼Œå¯ä»¥è¿›è¡Œä¿®æ”¹ # nä¸ºè¾“å‡ºå˜é‡åœ¨df_yä¸­ç¬¬å‡ åˆ—,é»˜è®¤æ˜¯ç¬¬ä¸€åˆ— # model_ouputæ˜¯å¦è¿”å›model # mæ˜¯å¦æ”¹å˜æ¨¡å‹å‚æ•° # scaleæ˜¯å¦å¯¹ç‰¹å¾å€¼è¿›è¡Œæ ‡å‡†åŒ– scaler = StandardScaler() if scale == True: x_train = scaler.fit_transform(x_train) x_test = scaler.transform(x_test) if m == True: xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs = 4) y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train, early_stopping_rounds=5, eval_set=[(x_test, y_test)], verbose=False) else: xgb = XGBRegressor() y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train) # æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–ï¼Œxgboost ç»“æœéƒ½ä¸€æ · y_pred = model_xgb.predict(x_test) y_pred_train = model_xgb.predict(x_train) predictions = [round(value) for value in y_pred] plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;è®­ç»ƒé›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, y_train.tolist(), label = \u0026#39;è®­ç»ƒé›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;éªŒè¯é›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, y_test.tolist(), label = \u0026#39;éªŒè¯é›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # ç»˜åˆ¶ç‰¹å¾å€¼å›¾ plot_importance(model_xgb) plt.show() r2_train = R_2(y_train, y_pred_train) r2_test = R_2(y_test, y_pred) print([r2_train, r2_test]) if model_output==True: return model_xgb å¤§å®¶å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œä¿®æ”¹å…¶ä¸­çš„å‚æ•°ï¼Œä»¥æ­¤ä¼˜åŒ–æ¨¡å‹æ•ˆæœã€‚ 1 model = xgboost_plot(i = \u0026#39;æ•°å€¼\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False) ä¸‹é¢æ”¾ä¸€å¼ å·¨å¼±ä½¿ç”¨xgboostå»ºç«‹çš„ä¸€æ¬¡å·¥ä¸šæ¨¡å‹å›¾ï¼Œç”±å›¾å¯è§æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æ‹Ÿåˆåº¦é«˜è¾¾ 99.9% ï¼Œå®åœ¨æ˜¯å¼ºï¼Œå¤§å®¶å¯ä»¥æ‰‹åŠ¨è¯•ä¸€è¯•ï¼š ","permalink":"https://soso010816.github.io/posts/xgboost-python/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ åˆæ¬¡æ¥è§¦XGBoost ç›¸ä¿¡å¤§éƒ¨åˆ†æ¥è§¦è¿‡æ•°æ¨¡ã€æœºå™¨å­¦ä¹ çš„æœ‹å‹éƒ½å¯¹XGBoostæœ‰æ‰€è€³é—»ï¼Œå·¨å¼±æˆ‘ç¬¬ä¸€æ¬¡æ¥è§¦å®ƒæ˜¯åœ¨2021å¹´12æœˆç¬¬","title":"MLï¼šSimple XGBoost With Python"},{"content":" â€œ ç®€å•çš„ä»‹ç» â€ åœ¨ç»Ÿè®¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿç»å¸¸é‡åˆ°é«˜ç»´æ•°æ®çš„é—®é¢˜ï¼Œæ¯”å¦‚å›¾ç‰‡å¤„ç†ï¼Œå›¾ç‰‡çš„å¤„ç†æŠ€æœ¯åœ¨ç›®å‰ä¹Ÿæ˜¯éå¸¸çƒ­é—¨ï¼Œä¸æ–­è¢«æ¢ç´¢çš„é¢†åŸŸï¼Œæœ¬æ¬¡å­¦ä¹ blogä¸ºå¤§å®¶å¸¦æ¥é„™äººå°è£…çš„å·ç§¯ç¥ç»ç½‘ç»œpythonä»£ç ï¼Œä¾›å¤§å®¶å¤„ç†åŸºæœ¬çš„å›¾ç‰‡åˆ†ç±»é¢„æµ‹é—®é¢˜å¹¶å°†ç»“æœè¿›è¡Œå¯è§†åŒ–ï¼Œå½“ç„¶å¤§å®¶ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±éœ€æ±‚ä¿®æ”¹ä»£ç ä¸­çš„parameså‚æ•°ï¼Œä»è€Œé€‰æ‹©å‡ºé¢„æµ‹æ•ˆæœæœ€ä½³çš„æ¨¡å‹ã€‚ â€œ CNNçš„åŸºæœ¬åŸç† â€ å…³äºCNNçš„åŸºæœ¬ä»‹ç»å¤§å®¶å¯ä»¥åœ¨ A Simple Introduction About CNN é‡Œè¿›è¡Œå­¦ä¹ ã€‚\nâ€œ pythonä»£ç  â€ æœ¬æ¬¡çš„å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨çš„pytorchåŒ…ï¼Œåªè¦æ±‚æœ‰å‹å‹æ‰€ä½¿ç”¨çš„è®­ç»ƒå›¾ç‰‡æ•°æ®é›†æ ‡è®°å¥½å¹¶æ”¾è‡³æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç›®å½•ï¼Œå³å¯ä»¥è¿è¡Œé„™äººçš„CNNå‡½æ•°ã€‚å¹¶ä¸”ä¸€èˆ¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸»è¦ç”¨äºå›¾åƒå¤„ç†æŠ€æœ¯ï¼Œ å› æ­¤æœ¬ä»£ç é’ˆå¯¹çš„æ•°æ®é›†ä¸º jpgã€pngç­‰å›¾ç‰‡æ•°æ®ã€‚\nå¯¼å…¥æ‰€éœ€çš„åº“ pytorchã€globã€numpyã€sklearnã€matplotlibã€copy\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rfrom torch import optim\rfrom torchvision import utils\rfrom torch.optim.lr_scheduler import ReduceLROnPlateau\rfrom torchvision import datasets, models, transforms\rfrom torch.utils.data import DataLoader, Dataset\rimport copy\rimport glob\rimport numpy as np\rimport plotly.graph_objs as go\rimport matplotlib.pyplot as plt\rfrom plotly.subplots import make_subplots\rfrom sklearn.model_selection import train_test_split\r%matplotlib inline\ræ•°æ®é›†çš„æ„å»º CPU or CUDA\n# 1. é€‰æ‹© CPU è¿˜æ˜¯ GPU ç‰ˆçš„ pytorch è¿›è¡Œå»ºæ¨¡\rdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r# 2. è®¾ç½®éšæœºç§å­\rtorch.manual_seed(816)\rif device =='cuda':\rtorch.cuda.manual_seed_all(816)\r# å°†æ‰€æœ‰å›¾ç‰‡æ•°æ®çš„è·¯å¾„å­˜å‚¨è‡³åˆ—è¡¨ä¸­, train_dirå’Œ test_dirä¸ºè®­ç»ƒé›†å›¾ç‰‡å’ŒéªŒè¯é›†å›¾ç‰‡æ‰€åœ¨çš„æ–‡ä»¶å¤¹\rtrain_list = glob.glob(os.path.join(train_dir,'*.jpg')) # å¦‚æœå›¾ç‰‡ä¸ºpngå½¢å¼ï¼Œåˆ™å°†jpgæ”¹æˆpngå³å¯\rtest_list = glob.glob(os.path.join(test_dir, '*.jpg')) # å¦‚æœæœ‰æµ‹è¯•é›†åˆ™ä½¿ç”¨è¿™è¡Œä»£ç ï¼Œå¦åˆ™å¯ä»¥åˆ é™¤\ræ¨¡å‹çš„æ­å»º æ•°æ®å¢å¼ºã€æ¨¡å‹ç»“æ„è®¾è®¡ã€æ¨¡å‹çš„è®­ç»ƒä¸ä¿æŒã€ç»“æœå¯è§†åŒ–\n# 3. å®šä¹‰å®Œæ•´çš„ CNN æ¡†æ¶æ¨¡å‹\rdef So_CNN_model(train_list, test_list = None):\r# è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å‰²\rtrain_list, val_list = train_test_split(train_list, test_size=0.2)\r# 4. å›¾ç‰‡çš„é¢„å¤„ç†ï¼Œå›¾ç‰‡å¢å¼º\rtrain_transforms = transforms.Compose([\rtransforms.Resize((224, 224)), # è®¾è®¡è®­ç»ƒå›¾ç‰‡è½¬åŒ–ä¸º224*224å›¾ç‰‡å¤§å°ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œä¿®æ”¹å¤§å°\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\r])\rval_transforms = transforms.Compose([\rtransforms.Resize((224, 224)), # è®¾è®¡éªŒè¯é›†å›¾ç‰‡è½¬åŒ–ä¸º224*224å›¾ç‰‡å¤§å°\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\r])\r# å¦‚æœæœ‰æµ‹è¯•é›†éœ€è¦è¾“å‡ºï¼Œåˆ™ä¹Ÿå¯¹éªŒè¯é›†è¿›è¡Œå›¾ç‰‡è½¬åŒ–å¢å¼º\rif test_list != None:\rtest_transforms = transforms.Compose([ transforms.Resize((224, 224)),\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor()\r])\r# 5. å®šä¹‰ä¸€ä¸ªç±»è¿›è¡Œæ•°æ®è½¬åŒ–ã€å›¾ç‰‡æ•°æ®é›†çš„å¤„ç†ã€ä»¥åŠå›¾ç‰‡çš„æ ‡æ³¨è½¬åŒ–\rclass dataset(torch.utils.data.Dataset):\rdef __init__(self, file_list, transform=None):\rself.file_list = file_list # å›¾ç‰‡åå­—åˆ—è¡¨\rself.transform = transform # è½¬åŒ–å™¨\rself.label_list = label_list # æ‰€æœ‰çš„æ ‡ç­¾ç§ç±»\rdef __len__(self):\rself.filelength = len(self.file_list)\rreturn self.filelength # å›¾ç‰‡æ•°ç›®\r# å¯¹äºæœ¬åœ°å›¾ç‰‡çš„ä¸‹è½½ä¸æ ‡æ³¨\rdef __getitem__(self,idx):\rimg_path = self.file_list[idx]\rimg = Image.open(img_path) # æ‰“å¼€æœ¬åœ°å›¾ç‰‡æ•°æ®é›†æ‰€åœ¨çš„ä½ç½®\rimg_transformed = self.transform(img) # æ•°æ®å¢å¼º\rlabel = img_path.split('/')[-1].split('.')[0] # å¯¹å›¾ç‰‡åå­—åˆ†å‰²ï¼Œå‰ææ˜¯å›¾ç‰‡åå­—å³ä¸ºæ ‡æ³¨\rlabel = label_list.index(label) # æœç´¢è¯¥æ ‡ç­¾åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œå¹¶å°†å…¶è¿›è¡Œæ•°å€¼æ ‡æ³¨ï¼Œæœ‰å‡ ä¸ªç§ç±»æ•°å€¼å°±æœ‰å‡ ç§\rreturn img_transformed, label\r# 6. å®šä¹‰å„ä¸ªç±»\rtrain_data = dataset(train_list, transform=train_transforms)\rif test_list != None:\rtest_data = dataset(test_list, transform=test_transforms)\rval_data = dataset(val_list, transform=test_transforms)\r# 7. æ•°æ®åŠ è½½\rbatch_size = 100\rtrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True )\rif test_list != None:\rtest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle=True)\rval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle=True)\r# 8. CNNæ¨¡å‹æ¶æ„çš„è®¾è®¡\rclass Cnn(nn.Module):\rdef __init__(self):\rsuper(Cnn,self).__init__()\r'''ä»¥ä¸‹æ‰€æœ‰å±‚çš„ç»“æ„å‚æ•°å’Œå±‚æ•°æ•°é‡éƒ½å¯ä»¥è¿›è¡Œéœ€ä¿®æ”¹\rå¯ä»¥æ¯”è¾ƒå‚æ•°ä¸åŒäº§ç”Ÿçš„æ¨¡å‹ç»“æœï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜å‚æ•°'''\rc,h,w = params['shape_in'] # åˆå§‹ æ•°æ®ç»“æ„\rf = params['initial_filters'] # åˆå§‹ æ•°æ®è½¬åŒ–çš„å±‚æ•°\rnum_classes = params['num_classes'] # éœ€è¦åˆ†ç±»çš„æ€»æ•°\rnum_fc1 = params['num_fc1'] # å…¨è¿æ¥å±‚çš„ç¬¬ä¸€å±‚\rdropout_rate = params['dropout_rate']\r# ç¬¬ä¸€å±‚å·ç§¯å±‚ï¼Œå°†3é€šé“ c ç»´æ•°æ®è½¬åŒ–ä¸º f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0\rself.layer1 = nn.Sequential(\rnn.Conv2d(c, f, kernel_size = 3, padding=0, stride=2),\rnn.BatchNorm2d(f), # f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†\rnn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹©\rnn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2 çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ–\r)\r# ç¬¬ä¸€å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦\rh,w = findConv2dOutShape(h, w, nn.MaxPool2d(2))\rh,w = h/2, w/2\r# ç¬¬äºŒå±‚å·ç§¯å±‚ï¼Œå°† f ç»´æ•°æ®è½¬åŒ–ä¸º 2*f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0\rself.layer2 = nn.Sequential(\rnn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(2*f), # 2*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†\rnn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹©\rnn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ–\r)\r# ç¬¬äºŒå±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦\rh,w = findConv2dOutShape(h, w, nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2))\rh,w = h/2, w/2\r# ç¬¬ä¸‰å±‚å·ç§¯å±‚ï¼Œå°† 32 ç»´æ•°æ®è½¬åŒ–ä¸º 64 ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0\rself.layer3 = nn.Sequential(\rnn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(4*f), # 4*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†\rnn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹©\rnn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ–\r)\r# ç¬¬ä¸‰å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦\th,w = findConv2dOutShape(h, w, nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2))\rh,w = h/2, w/2\r# ç¬¬å››å±‚å·ç§¯å±‚ï¼Œå°† 4*f ç»´æ•°æ®è½¬åŒ–ä¸º 8*f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0\rself.layer4 = nn.Sequential(\rnn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(8*f), # 8*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†\rnn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹©\rnn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ–\r)\r# ç¬¬å››å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦\rh,w = findConv2dOutShape(h, w, nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2\r# æœ€å æˆ‘è®¾è®¡äº†2å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œç»“æ„\rself.num_flatten= h * w* 8*f\rself.fc1 = nn.Linear(self.num_flatten, num_fc1) self.dropout = nn.Dropout(dropout_rate) # ä»¥ 0.5 çš„æ¦‚ç‡å¯¹å…¶è¿›è¡Œå‰”é™¤\rself.fc2 = nn.Linear(num_fc1, num_class)\rself.relu = nn.ReLU() # å®šä¹‰æ¿€æ´»å‡½æ•° RELU\rself.softmax = nn.log_softmax() # å®šä¹‰ æœ€åçš„è¾“å‡ºå‡½æ•° Softmax\r# å®šä¹‰å‘å‰ä¼ æ’­\rdef forward(self,x):\rout = self.layer1(x)\rout = self.layer2(out)\rout = self.layer3(out)\rout = self.layer4(out)\rout = out.view(-1, self.num_flatten)\rout = self.relu(self.fc1(out))\rout = self.softmax(self.fc2(out), dim = 1)\rreturn out\rparams_model={\r\u0026quot;shape_in\u0026quot;: (3, 224, 224), \u0026quot;initial_filters\u0026quot;: 8, \u0026quot;num_fc1\u0026quot;: 100,\r\u0026quot;dropout_rate\u0026quot;: 0.25,\r\u0026quot;num_classes\u0026quot;: len(label_list)} # num_class,æ ¹æ®ç±»åˆ«çš„æ€»æ•°è€Œå®š\r# ä¼ è¾¾æ¨¡å‹ç»“æ„ç»™cnn_model\rcnn_model = Cnn(params_model)\rdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\rmodel = cnn_model.to(device)\r# 9. å®šä¹‰æŸå¤±å‡½æ•°\rloss_func = nn.NLLLoss(reduction=\u0026quot;sum\u0026quot;)\r# 10. å®šä¹‰ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–å™¨å°†ä¿æŒå½“å‰çŠ¶æ€ï¼Œå¹¶æ ¹æ®è®¡ç®—çš„æ¢¯åº¦æ›´æ–°å‚æ•°\ropt = optim.Adam(cnn_model.parameters(), lr=3e-4)\rlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)\r# 11. å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°\rdef train_val(model, params,verbose=False):\r# è·å–è®­ç»ƒå‚æ•°\repochs = params[\u0026quot;epochs\u0026quot;]\rloss_func = params[\u0026quot;loss_func\u0026quot;]\ropt = params[\u0026quot;optimiser\u0026quot;]\rtrain_dl = params[\u0026quot;train\u0026quot;]\rval_dl = params[\u0026quot;val\u0026quot;]\rcheck = params[\u0026quot;check\u0026quot;]\rlr_scheduler = params[\u0026quot;lr_change\u0026quot;]\rweight_path = params[\u0026quot;weight_path\u0026quot;]\rloss_history = {\u0026quot;train\u0026quot;: [],\u0026quot;val\u0026quot;: []} # æ¯æ¬¡ epoch çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æŸå¤±å€¼\rmetric_history = {\u0026quot;train\u0026quot;: [],\u0026quot;val\u0026quot;: []} # æ¯æ¬¡ epoch çš„ metricå€¼\rbest_model_wts = copy.deepcopy(model.state_dict()) # æ·±åº¦å¤åˆ¶æœ€ä½³æ€§èƒ½æ¨¡å‹çš„æƒé‡\rbest_loss = float('inf') # å°†æœ€ä½³çš„æŸå¤±å€¼åˆå§‹åŒ–ä¸ºæå¤§å€¼\r# è¿­ä»£å¾ªç¯\rfor epoch in range(epochs):\r# è·å–å­¦ä¹ ç‡\rcurrent_lr = get_lr(opt)\rif(verbose):\rprint('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\r# ä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒ CNN æ¨¡å‹\rmodel.train()\rtrain_loss, train_metric = loss_epoch(model,loss_func,train_dl,check,opt)\r# æ”¶é›†è®­ç»ƒæ•°æ®é›†çš„æŸå¤±å’Œè¡¡é‡æ ‡å‡†\rloss_history[\u0026quot;train\u0026quot;].append(train_loss)\rmetric_history[\u0026quot;train\u0026quot;].append(train_metric)\r# ä½¿ç”¨éªŒè¯é›†å¯¹æ¨¡å‹ç»“æœè¿›è¡Œè¯„ä¼°\rmodel.eval()\rwith torch.no_grad():\rval_loss, val_metric = loss_epoch(model, loss_func, val_dl,check)\r# é€‰æ‹©æœ€å¥½çš„å‚æ•°æ¨¡å‹\rif val_loss \u0026lt; best_loss:\rbest_loss = val_loss\rbest_model_wts = copy.deepcopy(model.state_dict())\r# å­˜å‚¨æ¨¡å‹å‚æ•°è‡³æœ¬åœ°æ–‡ä»¶\rtorch.save(model.state_dict(), weight_path)\rif(verbose):\rprint(\u0026quot;å·²ç»ä¿å­˜å®Œè®­ç»ƒå¾—åˆ°çš„æœ€å¥½æ¨¡å‹ï¼\u0026quot;)\r# å­˜å‚¨éªŒè¯æ•°æ®é›†çš„æŸå¤±å’Œè¡¡é‡æ ‡å‡†\rloss_history[\u0026quot;val\u0026quot;].append(val_loss)\rmetric_history[\u0026quot;val\u0026quot;].append(val_metric)\r# å­¦ä¹ ç‡ç­›é€‰\rlr_scheduler.step(val_loss)\rif current_lr != get_lr(opt):\rif(verbose):\rprint(\u0026quot;å·²ç»åŠ è½½å®ŒCNNæ¨¡å‹ï¼\u0026quot;)\rmodel.load_state_dict(best_model_wts) if(verbose):\rprint(f\u0026quot;train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\u0026quot;)\rprint(\u0026quot;-\u0026quot;*10) # å­˜å‚¨æ¨¡å‹çš„æƒé‡å’Œå‚æ•°æ•°æ®è‡³æœ¬åœ°\rmodel.load_state_dict(best_model_wts)\rreturn model, loss_history, metric_history\rparams_train={\r\u0026quot;train\u0026quot;: train_dl,\u0026quot;val\u0026quot;: val_dl,\r\u0026quot;epochs\u0026quot;: 50, # è¿­ä»£ 50 æ¬¡\r\u0026quot;optimiser\u0026quot;: optim.Adam(cnn_model.parameters(),\rlr=3e-4),\r\u0026quot;lr_change\u0026quot;: ReduceLROnPlateau(opt,\rmode = 'min',\rfactor = 0.5,\rpatience = 20,\rverbose = 0),\r\u0026quot;loss_func\u0026quot;: nn.NLLLoss(reduction = \u0026quot;sum\u0026quot;),\r\u0026quot;weight_path\u0026quot;: \u0026quot;weights.pt\u0026quot;,\r\u0026quot;check\u0026quot;: False, }\r# è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹\rcnn_model,loss_hist,metric_hist = train_val(cnn_model, params_train)\r# è®­ç»ƒå‚æ•°è¿›ç¨‹\repochs = params_train[\u0026quot;epochs\u0026quot;]\r# ç»˜åˆ¶ç»“æœå›¾\rfig = make_subplots(rows = 1, cols = 2, subplot_titles = ['æŸå¤±å€¼-æŠ˜çº¿å›¾','å‡†ç¡®ç‡-æŠ˜çº¿å›¾'])\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026quot;train\u0026quot;], name = 'è®­ç»ƒé›†çš„æŸå¤±å€¼'), row = 1, col = 1)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026quot;val\u0026quot;], name = 'éªŒè¯é›†çš„æŸå¤±å€¼'), row = 1, col = 1)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026quot;train\u0026quot;], name = 'è®­ç»ƒé›†çš„å‡†ç¡®ç‡'), row = 1, col = 2)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026quot;val\u0026quot;], name = 'éªŒè¯é›†çš„å‡†ç¡®ç‡'), row = 1, col = 2)\rfig.update_layout(template = 'plotly_white'); fig.update_layout(margin = {\u0026quot;r\u0026quot;:0,\u0026quot;t\u0026quot;:60,\u0026quot;l\u0026quot;:0,\u0026quot;b\u0026quot;:0}, height= 300)\rfig.show()\rSo_CNN_model(train_list, test_list)\ræ„Ÿè°¢å„ä½å‹å‹èƒ½çœ‹åˆ°æœ€åï¼é™„ä¸€å¼ æˆ‘è¶…çº§å–œæ¬¢çš„æ•°å­¦å®‡å®™gifä»£è¡¨ç»“æŸï¼\nThe Endï¼\n","permalink":"https://soso010816.github.io/posts/cnn-python/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ åœ¨ç»Ÿè®¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿç»å¸¸é‡åˆ°é«˜ç»´æ•°æ®çš„é—®é¢˜ï¼Œæ¯”å¦‚å›¾ç‰‡å¤„ç†ï¼Œå›¾ç‰‡çš„å¤„ç†æŠ€æœ¯åœ¨ç›®å‰ä¹Ÿæ˜¯éå¸¸çƒ­é—¨ï¼Œä¸æ–­è¢«æ¢ç´¢çš„é¢†åŸŸï¼Œæœ¬æ¬¡å­¦ä¹ blog","title":"DLï¼šCNN With Python"},{"content":" â€œ ç®€çŸ­çš„ä»‹ç» â€ è¿™æ®µæ—¶é—´å‚åŠ çš„æ•°æ¨¡æ¨¡æ‹Ÿåˆšå¥½ç”¨åˆ°äº†æ·±åº¦å­¦ä¹ çš„åº•å±‚æ¶æ„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œäºæ˜¯è‡ªå·±ç´¢æ€§å°±å°†æˆ‘æ•°æ¨¡ä¸­ç”¨åˆ°çš„ä»£ç å°è£…äº†ä»¥ä¸‹ï¼Œåšæˆå¦‚ä¸‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹å‡½æ•°ä»¥åŠåŒ…æ‹¬ç»“æœçš„å¯è§†åŒ–ã€æ‹Ÿåˆåº¦çš„è®¡ç®—ç»“æœè¾“å‡ºï¼Œæ–¹ä¾¿å„ä½å‹å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\nåæœŸæœ‰æ—¶é—´çš„è¯ï¼Œæˆ‘ä¹Ÿä¼šå†™ä¸€ä¸ªé—ä¼ ç®—æ³•æˆ–è€…ç²’å­ç¾¤ç®—æ³•ï¼ˆmaybeæ˜¯å…¶ä»–å¯å‘å¼ç®—æ³•ï¼‰ç”¨æ¥å’Œä¸‹é¢çš„å‡½æ•°ç»“åˆï¼Œè‡ªåŠ¨å¸®å„ä½æ‰¾åˆ°é¢„æµ‹ç»“æœæœ€ä¼˜çš„æ¨¡å‹å‚æ•°ã€‚\nè¿™é‡Œæ˜¯ä¸€ç¯‡æˆ‘çš„å…³äºç¥ç»ç½‘ç»œçš„åŸç†ä»‹ç»ï¼Œé‡Œé¢æœ‰å…³äºç¥ç»ç½‘ç»œéå¸¸è¯¦ç»†çš„ä»‹ç»ï¼š ç¥ç»ç½‘ç»œç”±æ¥åŠåŸç†\nâ€œ Tips â€ å½“ç„¶é¢å¯¹ä¸åŒçš„æ•°æ®ï¼Œæœ€ä¼˜çš„ç¥ç»ç½‘ç»œç»“æ„å’Œå‚æ•°ä¼šæœ‰æ‰€ä¸åŒï¼Œå¤§å®¶å¯ä»¥æ ¹æ®è‡ªå·±çš„æ‹Ÿåˆç»“æœï¼Œä¿®æ”¹æˆ‘ä¸‹é¢çš„å‡½æ•°å‚æ•°ï¼Œä»è€Œè·å–æœ€ä¼˜æ¨¡å‹ã€‚\nå¦‚æœå„ä½å‹å‹é¢å¯¹çš„æ˜¯åˆ†ç±»é—®é¢˜ï¼Œåªéœ€æŠŠæ¿€æ´»å‡½æ•°æ”¹æˆsoftmaxå³å¯ï¼Œå½“ç„¶æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥è¿›è¡Œä¿®æ”¹ï¼Œå…³äºæŸå¤±å‡½æ•°çš„ç ”ç©¶æ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„Huber losså’ŒM-regressionï¼Œæ„Ÿå…´è¶£çš„å‹å‹å¯ä»¥è‡ªè¡ŒæŸ¥é˜…ç›¸å…³æ–‡çŒ®ã€‚\nâ€œ Simple Test â€ ä¸‹é¢æ˜¯æœ¬äººç”¨ä¸‹è¿°è‡ªå·±å†™çš„ä»£ç å»ºç«‹çš„å·¥ä¸šæ¨¡å‹æ‹Ÿåˆç»“æœï¼Œæ‹Ÿåˆåº¦è¾¾åˆ°äº† 95% ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚å’Œç»“æœçš„æ•ˆæœä¿®æ”¹å…¶ä¸­çš„å‚æ•°ï¼Œæˆ–è€…å¢åŠ éšè—å±‚ï¼Œä»è€Œä¼˜åŒ–è‡ªå·±çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚\nâ€œ pythonä»£ç  â€ from sklearn.model_selection import train_test_split\rfrom sklearn.metrics import accuracy_scoreã€\rimport matplotlib.pyplot as plt\rfrom keras import regularizers\rfrom sklearn.preprocessing import MinMaxScaler\rfrom keras.models import Sequential\rfrom keras.layers import Dense, Dropout from sklearn import preprocessing\rdef NN_Plot(i, X, Y, model_output = False):\r# i ä¸ºéœ€è¦è¾“å‡ºå›¾è¡¨çš„yè½´æ ‡ç­¾ï¼›\r# X ä¸ºè‡ªå˜é‡çš„æ•°æ®é›†ï¼›\r# Y ä¸ºè¾“å‡ºå˜é‡çš„æ•°æ®ï¼›\r# model_output è¡¨ç¤ºæ˜¯å¦è¿”å› è®­ç»ƒåçš„æ¨¡å‹ï¼›\r# 1. æ•°æ®é›†æ ‡å‡†åŒ–\rmin_max_scaler = preprocessing.MinMaxScaler()\rX_scale = min_max_scaler.fit_transform(X)\r# 2.è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ’åˆ† X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, test_size=0.3, random_state = n)\r# 3. æ¨¡å‹çš„ç»“æ„è®¾è®¡\rmodel = Sequential() # åˆå§‹åŒ–ï¼Œå¾ˆé‡è¦\rmodel.add(Dense(units = 1000, # è¾“å‡ºå¤§å°ï¼Œä¹Ÿæ˜¯è¯¥å±‚ç¥ç»å…ƒçš„ä¸ªæ•° activation='relu', # æ¿€åŠ±å‡½æ•°-RELU input_shape=(X_train.shape[1],) # è¾“å…¥å¤§å°, ä¹Ÿå°±æ˜¯åˆ—çš„å¤§å° )) model.add(Dropout(0.3)) # ä¸¢å¼ƒç¥ç»å…ƒé“¾æ¥æ¦‚ç‡ model.add(Dense(units = 1000, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation='relu' # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.15))\rmodel.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation='relu' # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.15))\rmodel.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation='relu' # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.2))\rmodel.add(Dense(units = 1, activation='linear',\rkernel_regularizer=regularizers.l2(0.01) # çº¿æ€§æ¿€åŠ±å‡½æ•°å›å½’ä¸€èˆ¬åœ¨è¾“å‡ºå±‚ç”¨è¿™ä¸ªæ¿€åŠ±å‡½æ•° )) model.compile(optimizer='adam',\rloss='mse', # æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®\rmetrics=['accuracy'])\r# 4. æ¨¡å‹çš„è®­ç»ƒï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹batchâ€”â€”sizeå¤§å°å’Œepochå¤§å°\rhist = model.fit(X_train, Y_train,\rbatch_size = 32, epochs=250, verbose = 2,\rvalidation_data=(X_test, Y_test))\r# 5. æ¨¡å‹çš„æŸå¤±å€¼å˜åŒ–å›¾ç»˜åˆ¶\rplt.plot(hist.history['loss'])\rplt.plot(hist.history['val_loss'])\rplt.title('Model loss')\rplt.ylabel('Loss')\rplt.xlabel('Epoch')\rplt.legend(['Train', 'Val'], loc='upper right')\rplt.show()\ry_pred = model.predict(X_test)\ry_pred_train = model.predict(X_train)\r# 6. æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ‹Ÿåˆç¨‹åº¦å›¾ç»˜åˆ¶\rplt.figure(figsize=(30,9),dpi = 200)\rplt.subplot(1,2,1)\rls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)]\rplt.plot(ls_x_train, y_pred_train.tolist(), label = 'è®­ç»ƒé›†çš„é¢„æµ‹å€¼' , marker = 'o')\rplt.plot(ls_x_train, Y_train.iloc[:,0].tolist(), label = 'è®­ç»ƒé›†çš„çœŸå®å€¼',linestyle='--', marker = 'o' )\rplt.ylabel(i, fontsize = 15)\rplt.legend(fontsize = 15)\rplt.xticks(fontsize = 12)\rplt.yticks(fontsize = 12)\rplt.subplot(1,2,2)\rls_x = [x for x in range(1, len(y_pred.tolist())+1)]\rplt.plot(ls_x, y_pred.tolist(), label = 'éªŒè¯é›†çš„é¢„æµ‹å€¼' , marker = 'o')\rplt.plot(ls_x, Y_test.iloc[:,0].tolist(), label = 'éªŒè¯é›†çš„çœŸå®å€¼',linestyle='--',marker = 'o')\rplt.ylabel(i, fontsize = 15)\rplt.xticks(fontsize = 12)\rplt.yticks(fontsize = 12)\rplt.legend(fontsize = 15)\r# Ræ–¹çš„è®¡ç®—\rr2_train = R_2(Y_train.iloc[:,0].tolist(), y_pred_train)\rr2_test = R_2(Y_test.iloc[:,0].tolist(), y_pred)\rprint([r2_train, r2_test, (r2_train+r2_test)/2 ])\r# æ˜¯å¦è¿”å›è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹\rif model_output==True:\rreturn [model, min_max_scaler]\rdef R_2(y, y_pred):\ry_mean = mean(y)\rsst = sum([(x-y_mean)**2 for x in y])\rssr = sum([(x-y_mean)**2 for x in y_pred])\rsse = sum([(x-y)**2 for x,y in zip(y_pred, y)])\rreturn 1-sse/sst\ræ„Ÿè°¢è§‚çœ‹ï¼The Endï¼\n","permalink":"https://soso010816.github.io/posts/nn-python/","summary":"â€œ ç®€çŸ­çš„ä»‹ç» â€ è¿™æ®µæ—¶é—´å‚åŠ çš„æ•°æ¨¡æ¨¡æ‹Ÿåˆšå¥½ç”¨åˆ°äº†æ·±åº¦å­¦ä¹ çš„åº•å±‚æ¶æ„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œäºæ˜¯è‡ªå·±ç´¢æ€§å°±å°†æˆ‘æ•°æ¨¡ä¸­ç”¨åˆ°çš„ä»£ç å°è£…äº†ä»¥ä¸‹ï¼Œåšæˆå¦‚ä¸‹çš„ç¥ç»ç½‘","title":"DLï¼šNeural Network With Python"},{"content":"","permalink":"https://soso010816.github.io/about/","summary":"","title":"About"}]