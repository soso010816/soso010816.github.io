[{"content":" åœ¨ [ä¸Šä¸€ç¯‡post](https://soso010816.github.io/posts/TM tree1) ä¸­ï¼Œæˆ‘ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­æ ‘æ¨¡å‹åœ¨æ—¶é—´åºåˆ—ä¸­çš„è¿ç”¨æ–¹å¼ï¼Œä¸¾ä¾‹äº†å¦‚ä½•æ„å»ºç‰¹å¾å˜é‡å¹¶XGBoostè¿›è¡Œæ—¶åºé¢„æµ‹ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„æ—¶åºé¢„æµ‹éƒ½å¯ä»¥ä½¿ç”¨æ ‘æ¨¡å‹è¿›è¡Œå»ºæ¨¡ã€‚\nåœ¨è¿™ç¯‡postä¸­ï¼Œæˆ‘å°†å¸¦æ¥ä¸€äº›å¯¹æ ‘æ¨¡å‹åœ¨æ—¶åºæ–¹é¢çš„è¿ç”¨æ¡ä»¶çš„æ€è€ƒï¼Œè¯¥æƒ³æ³•æºäºä¹‹å‰æˆ‘åœ¨åšæ—¶åºé¢„æµ‹æ—¶å‡ºç°çš„æ”¶æ•›é—®é¢˜ã€‚\n1. éœ€è¦è‡³å°‘ä¸€ä¸ªä»¥ä¸Šçš„ç‰¹å¾å˜é‡ï¼Œå¹¶ä¸”å°½é‡ä¿è¯ç‰¹å¾å˜é‡è¶³å¤Ÿå¤šã€‚å¯ä»¥ä»æ—¶åºå˜é‡ Y ä»¥å¤–å¯»æ‰¾ç›¸å…³çš„å½±å“å› ç´ ä½œä¸ºç‰¹å¾å˜é‡ï¼Œä¹Ÿå¯ä»¥ä» Y ä¸­æå–ç‰¹å¾å˜é‡ æ ‘æ¨¡å‹çš„åŸºæœ¬è¿ä½œåŸç†æ˜¯é€šè¿‡å¯¹å˜é‡ä¸æ–­è¿›è¡Œåˆ†è£‚ï¼Œæ¯ä¸ªç‰¹å¾åˆ†å‰²ç‚¹ä½œä¸ºæèŠ‚ç‚¹ï¼Œæ‰€æœ‰çš„å¶å­å³ä¸ºæœ€ç»ˆæ ‘æ¨¡å‹è¦è¾“å‡ºçš„å„ç§ç»“æœã€‚å¦‚æœæ²¡æœ‰ç‰¹å¾å˜é‡æˆ–è€…ç‰¹å¾å˜é‡è¿‡å°‘ï¼Œå°†å¯¼è‡´æ— è¾“å‡ºç»“æœæˆ–è€…å¶å­å¤ªå°‘ï¼Œè¯¯å·®è¿‡å¤§ã€‚\n2. ç›´æ¥å¯¹æ—¶åºå˜é‡é¢„æµ‹æ—¶ï¼Œè¦æ±‚æ—¶åºå˜é‡åŸºæœ¬å¹³ç¨³ï¼Œæ— è¶‹åŠ¿ åœ¨ä¸Šä¸€ç¯‡postä¸­çš„ä¾‹å­å¯ä»¥çœ‹åˆ°ï¼Œxgboostçš„æ‹Ÿåˆé¢„æµ‹æ•ˆæœä¸é”™ï¼Œä½†æ˜¯å¯ä»¥å‘ç°ä½¿ç”¨åˆ°çš„æ•°æ®æ˜¯åŸºæœ¬å¹³ç¨³ï¼Œæ— è¶‹åŠ¿çš„ã€‚\nä¸ºä»€ä¹ˆéœ€è¦è¢«é¢„æµ‹çš„æ—¶åºå˜é‡æ˜¯å¹³ç¨³çš„å‘¢\nå¯¹è¿™ä¸ªé—®é¢˜çš„æ€è€ƒåœ¨æŸæ¬¡æ¨¡æ‹Ÿæ—¶ï¼Œæˆ‘åˆ©ç”¨è®­ç»ƒæ ·æœ¬è®­ç»ƒxgboosté¢„æµ‹è¾“å‡ºå˜é‡åœ¨æŸä¸ªåŒºé—´çš„æœ€å¤§å€¼å‘ç°çš„ï¼Œæˆ‘å°è¯•ä¸æ–­çš„ä¿®æ”¹å‚æ•°ï¼Œåˆ©ç”¨å¯å‘å¼ç®—æ³•æ‰¾åŒºé—´å†…çš„æ¨¡å‹æœ€ä¼˜è¾“å‡ºè§£ï¼Œä½†æ˜¯ç»“æœéƒ½æ˜¯ï¼šå¾—åˆ°çš„æœ€ä¼˜è§£æ²¡æ³•è¶…è¿‡è®­ç»ƒæ ·æœ¬ä¸­çš„æœ€å¤§å€¼ï¼Œè¿™è¯´æ˜æ¨¡å‹è¾“å‡ºæ¥çš„æœ€ä¼˜è§£ç»“æœè¯¯å·®å¾ˆå¤§ï¼Œäºæ˜¯æˆ‘é‡æ–°å›å¿†æ ‘æ¨¡å‹çš„æ„å»ºåŸç†ï¼Œæ€è€ƒè‰¯ä¹…åï¼Œæœ€ç»ˆæ‰¾åˆ°äº†åŸå› ï¼šæ ‘æ¨¡å‹æ²¡æœ‰åŠæ³•è¿›è¡Œâ€œå¤–æ¨â€ã€‚å…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š\næ ‘æ¨¡å‹æ˜¯é€šè¿‡å¯å‘å¼ç®—æ³•ä¸ç›®æ ‡å‡½æ•°ã€æŸå¤±å‡½æ•°ç›¸ç»“åˆï¼Œä»è€Œå¯¹è®­ç»ƒæ•°æ®æ±‚è§£å‡ºæœ€ä½³åˆ†å‰²ç‚¹ä¸æœ€ä½³åˆ†å‰²æ•°ï¼Œç„¶åå¯¹è¯¥èŠ‚ç‚¹è¿›è¡Œå¶å­åˆ†å‰²ï¼Œä¸æ–­é‡å¤æœ€åæ„å»ºå‡ºæ ‘æ¨¡å‹å’Œæ¨¡å‹ä¸­çš„ M ç‰‡æœ‰é™çš„å¶å­ï¼ˆå³ M ä¸ªç»“æœï¼‰ï¼Œè¿™ä¹Ÿå†³å®šäº†ä¸è®ºè¾“å…¥æ¨¡å‹çš„æ–°ç‰¹å¾å€¼ä¸ºå¤šå°‘ï¼Œæœ€åéƒ½å°†è¾“å‡ºä¸º M ä¸ªç»“æœä¸­çš„ä¸€ä¸ªï¼Œå°¤å…¶å½“é¢å¯¹æ±‚æœ€ä¼˜è§£ã€é¢„æµ‹è¶‹åŠ¿å˜é‡æ—¶ï¼Œæ ‘æ¨¡å‹é€šè¿‡æŸ¥çœ‹æ•°æ®ç‚¹å±äºå“ªä¸ªâ€œå¶å­â€å¹¶å°†è®­ç»ƒé›†ä¸­ç›®æ ‡å˜é‡çš„å¹³å‡å€¼åˆ†é…ç»™è¯¥ç‚¹æ¥è¿›è¡Œå›å½’é¢„æµ‹ï¼Œå³ä¸€æ—¦æ¨¡å‹è®­ç»ƒç»“æŸï¼Œä¸ç®¡è¾“å…¥ä»€ä¹ˆå˜é‡ï¼Œç»“æœï¼šå‰è€…æ”¶æ•›ï¼Œä»‹äºè®­ç»ƒæ•°æ®çš„æœ€å¤§å’Œæœ€å°å€¼ä¹‹é—´ï¼›åè€…ï¼šç»“æœä¹Ÿå°†ä½äº M ä¸ªæ•°å€¼ä¸­é—´ï¼Œæ— æ³•â€œæ¨æ–­â€åˆ°æ¨¡å‹å°šæœªçœ‹åˆ°çš„æ•°æ®ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ ‘å½¢æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ—¶è¦æ±‚åºåˆ—ç»“æœä¸å­˜åœ¨è¶‹åŠ¿ï¼Œå¦åˆ™æ³›åŒ–æ€§å¾ˆä½ã€è¯¯å·®æå¤§ã€‚\nä¸¾ä¸ªä¾‹å­ï¼š\nå½“ä½¿ç”¨æ ‘æ¨¡å‹æ‹Ÿåˆé¢„æµ‹ä¸‹åˆ—å›¾ä¸­çš„ç¬¬äºŒä¸ªæ—¶åºæ•°æ®æ—¶ï¼Œæ ‘æ¨¡å‹çš„æ‹Ÿåˆæ•ˆæœå°†ä¼šå¾ˆå¥½ï¼Œä½†æ˜¯ä¸€æ—¦ä½¿ç”¨å…¶é¢„æµ‹çº¢çº¿ä»¥å¤–çš„éƒ¨åˆ†æ•°æ®æ—¶ï¼Œæ¨¡å‹å¾—åˆ°ç»“æœå°†å¯èƒ½æ—¶å…¶ä»–ä¸‰å¹…å›¾ï¼Œæ­¤æ—¶æŒ‰æˆ‘ä»¬çš„ç›´è§‰åˆ¤æ–­ï¼Œçº¢çº¿ä¹‹å¤–ï¼Œå˜é‡æ•´ä½“ä¸Šå¤§æ¦‚ç‡ä¼šç»§ç»­å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå› æ­¤æ¨¡å‹é¢„æµ‹çš„ç»“æœå°†çš„å¤§æ¦‚ç‡ä¸å®é™…æƒ…å†µæœ‰è¾ƒå¤§çš„è¯¯å·®ã€‚\nä½†æ˜¯è¿™æ˜¯ä¸æ˜¯è¡¨æ˜ä¸èƒ½ä½¿ç”¨æ ‘æ¨¡å‹å¯¹æ—¶åºå˜é‡è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿ\nç­”æ¡ˆæ˜¯è‚¯å®šæ˜¯å¦å®šçš„ã€‚\næŸ¥é˜…äº†ä¸€äº›èµ„æ–™åï¼Œå‘ç°å¯ä»¥æ¢ä¸€ç§æ€è·¯æ‰¾åˆ°è§£å†³æ–¹å¼ï¼š\né‡æ–°æ„å»ºé¢„æµ‹çš„ç›®æ ‡æ—¶åºå˜é‡ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å…ˆä½¿ç”¨ç®€å•çš„çº¿æ€§æ¨¡å‹ä¸æ—¶é—´å˜é‡ Y æ‹Ÿåˆï¼Œä½¿ç”¨çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„æ®‹å·®ä½œä¸ºç›®æ ‡è¾“å‡ºï¼Œä»è€Œè®­ç»ƒæ ‘æ¨¡å‹ï¼Œå³é¢„æµ‹çº¿æ€§æ¨¡å‹ä¸æ—¶é—´å˜é‡ä¹‹é—´çš„è¯¯å·®ï¼Œæ­¤æ—¶å¾—åˆ°æœ€ç»ˆé¢„æµ‹ç»“æœä¸ºï¼šæ ‘æ¨¡å‹é¢„æµ‹çš„è¯¯å·®+çº¿æ€§æ¨¡å‹çš„é¢„æµ‹å€¼ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨å˜åŒ–ç‡ä½œä¸ºç‰¹å¾ç­‰ç­‰ã€‚ å¯¹è¶‹åŠ¿çš„æ—¶é—´åºåˆ—è¿›è¡Œå·®åˆ†ã€‚ä½¿ä¹‹è½¬åŒ–ä¸ºå¹³ç¨³çš„æ—¶é—´åºåˆ—ï¼Œä»¥æ­¤å†æ„å»ºæ ‘æ¨¡å‹å¯¹å·®åˆ†åçš„å¹³ç¨³åºåˆ—è¿›è¡Œé¢„æµ‹ï¼›å¦‚ä¸‹å›¾ï¼š ","permalink":"https://soso010816.github.io/posts/tm-tree2/","summary":"åœ¨ [ä¸Šä¸€ç¯‡post](https://soso010816.github.io/posts/TM tree1) ä¸­ï¼Œæˆ‘ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­æ ‘æ¨¡å‹åœ¨æ—¶é—´åºåˆ—ä¸­çš„","title":"ML|TSï¼šReflections on Tree models in Time Series"},{"content":" â€œ å…³äºMLæ ‘æ¨¡å‹åœ¨æ—¶åºæ–¹é¢è¿ç”¨çš„Post â€ ä¸€ã€å¯¹äºå•ä¸ªæ—¶åºå˜é‡çš„é¢„æµ‹ï¼Œå¦‚ä½•è¿›è¡Œç‰¹å¾å˜é‡çš„æ„å»º 1. å¯¹æ—¶åºå˜é‡ Y è¿›è¡Œæ»å\nå¯¹äºæ—¶åºå˜é‡ Y è¿›è¡Œ æ»åNæ­¥ ï¼Œä»è€Œå½¢æˆä¸€ä¸ªæ»åNæœŸçš„ç‰¹å¾å˜é‡ï¼Œå…¶ä¸­è¯¥åˆ—ç‰¹å¾å˜é‡çš„ å½“å‰å€¼ ä¸åœ¨æ—¶åºå˜é‡ Y åœ¨ æ—¶é—´ t-Nçš„å€¼ ä¸€è‡´ã€‚\nå³å¦‚æœæˆ‘ä»¬å¯¹ Y åšä¸€ä¸ªæ»å1æ­¥çš„è½¬ç§»ï¼Œå¹¶åœ¨è¯¥ç‰¹å¾ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œåˆ™è¯¥æ¨¡å‹å°†èƒ½å¤Ÿåœ¨è§‚å¯Ÿåˆ°è¯¥ç³»åˆ—çš„å½“å‰çŠ¶æ€å æå‰1æ­¥ è¿›è¡Œé¢„æµ‹ã€‚\nå¦‚æœå¢åŠ æ»åæœŸï¼Œæ¯”å¦‚ï¼šå°†æ»åæ­¥æ•°å¢åŠ åˆ° 7ï¼Œå°†å…è®¸æ¨¡å‹æå‰7æ­¥è¿›è¡Œé¢„æµ‹ã€‚å¦‚æœåœ¨7å¤©å‰çš„æœªè§‚å¯Ÿåˆ°çš„æ—¶é—´é‡Œï¼Œæœ‰æŸäº›å¤–éƒ¨å› ç´ ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è¯¥ç³»åˆ—ï¼Œè¯¥æ¨¡å‹å°†æ— æ³•æ•æ‰åˆ°è¿™äº›å˜åŒ–ï¼Œå¹¶å°†è¿”å›å…·æœ‰è¾ƒå¤§è¯¯å·®çš„é¢„æµ‹ç»“æœã€‚å› æ­¤ï¼Œåœ¨æœ€åˆçš„æ»åæœŸé€‰æ‹©ä¸­ï¼Œå¿…é¡»åœ¨ æœ€ä½³é¢„æµ‹è´¨é‡å’Œé¢„æµ‹èŒƒå›´çš„é•¿åº¦ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ã€‚\n2. è®¡ç®—è§‚å¯Ÿçª—å£çš„ç»Ÿè®¡é‡\né€šè¿‡è®¾ç½® N å¤©çš„è§‚å¯Ÿçª—å£ ï¼Œé€šè¿‡è®¡ç®— N å¤©å†… æ—¶åºå˜é‡ Y çš„ç»Ÿè®¡é‡ ï¼Œå³ç§»åŠ¨ç»Ÿè®¡é‡ï¼Œä»¥æ­¤ä½œä¸ºå½“å¤©çš„ç‰¹å¾å˜é‡ï¼Œä»è€Œæ„å»ºå‡ºä¸€ä¸ªç‰¹å¾å˜é‡ã€‚\nä¾‹å¦‚ï¼šå‡å¦‚æ—¶åºå˜é‡æ˜¯ä»¥ â€œå¤©â€ ä¸ºæ—¶é—´é—´éš”ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥ä»¥å½“å‰æ—¥æœŸå¾€å‰ 7 å¤©ä½œä¸ºè§‚å¯Ÿçª—å£ï¼Œè®¡ç®—æ»å7å¤©è‡³æ»å1å¤©å†…æ—¶åºå˜é‡ Y çš„æ–¹å·®ï¼Œä»¥æ­¤ä½œä¸ºå½“å‰å€¼æ‰€å¯¹åº”çš„ä¸€ä¸ªç‰¹å¾å€¼ã€‚ä½¿ç”¨è¯¥ç‰¹å¾è®­ç»ƒæ¨¡å‹ï¼Œåˆ™è¯¥æ¨¡å‹å°†èƒ½å¤Ÿè§‚å¯Ÿåˆ°è¯¥æ—¶åºå˜é‡ Y çš„å½“å‰çŠ¶æ€çš„ å‰ 7 å¤©çš„æ³¢åŠ¨æƒ…å†µ ï¼Œä»¥æ­¤è¿›è¡Œé¢„æµ‹ã€‚\né™¤äº†æ–¹å·®ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€‰æ‹©è§‚æµ‹çª—å£ä¸­çš„æœ€å¤§å€¼ã€æœ€å°å€¼ã€å¹³å‡å€¼ã€ä¸­ä½æ•°ç­‰ç»Ÿè®¡é‡ä½œä¸ºç‰¹å¾å€¼ï¼Œå¹¶ä¸”æ¯ä¸ªç‰¹å¾å€¼æ‰€è§£é‡Šçš„æ•ˆæœä¼šæœ‰æ‰€ä¸åŒï¼Œç”šè‡³ä¹Ÿå¯ä»¥æ”¹å˜çª—å£å†…æ¯ä¸€æœŸçš„æƒé‡ä»è€Œè®¡ç®—ã€‚\n3. ç»™æ—¥æœŸå’Œæ—¶é—´è¿›è¡Œæ ‡è®°\nå¯¹äºæ—¶é—´ä¸ºå…·ä½“æ—¥æœŸæˆ–è€…æ—¶é—´çš„æ—¶åºå˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…·ä½“æ—¶é—´è´´ä¸Šæ ‡ç­¾ï¼Œå¹¶è½¬åŒ–ä¸ºå¸ƒå°”å€¼æˆ–è€…æ•°å€¼ä»è€Œæ„å»ºå‡ºç‰¹å¾å€¼ã€‚\nä¾‹å¦‚ï¼šæ—¶é—´æ˜¯å…·ä½“çš„æ—¥æœŸï¼Œæ—¶åºå˜é‡ Y ä¸ºå•†åŸçš„äººæµé‡ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥å¯¹ Y æ‰€å¯¹åº”çš„æ¯ä¸ªå…·ä½“æ—¥æœŸæŒ‰å‘¨æ ‡è®°æˆ–è€…ç‰¹æ®ŠèŠ‚å‡æ—¥æ ‡è®°ï¼Œæ¯”å¦‚å¯¹æ¯ä¸ªæ—¥æœŸæ˜¯ä¸€å‘¨ä¸­çš„ç¬¬å‡ å¤©å¹¶å¯¹å…¶è¿›è¡Œæ ‡è®°ï¼›æˆ–è€…è¯¥æ—¥æœŸæ˜¯å¦å¯¹åº”ç‰¹æ®Šçš„èŠ‚å‡æ—¥ï¼Œå¹¶å°†å…¶æ ‡è®°ä¸ºå¸ƒå°”å€¼ï¼Œä»è€Œæ„å»ºå‡ºæ–°çš„ç‰¹å¾å€¼ã€‚ä½¿ç”¨è¯¥ç‰¹å¾è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ï¼Œå°†èƒ½å¤Ÿè§‚å¯Ÿåˆ°å·¥ä½œæ—¥å’Œå‘¨æœ«ã€èŠ‚å‡æ—¥å¯¹æ—¶åºå˜é‡ Y å¸¦æ¥çš„å½±å“ã€‚\né™¤æ­¤ä¹‹å¤–ï¼Œé¢å¯¹ä¸åŒç±»å‹çš„æ•°æ®ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ ‡è®°å¹´ä»½ã€å­£èŠ‚ã€æœˆä»½ã€ä¸€å°æ—¶ä¸­çš„åˆ†é’Ÿã€ä¸€å¤©ä¸­çš„å°æ—¶ã€ç‰¹æ®Šäº‹ä»¶çš„å‘ç”Ÿç­‰ç­‰ä»¥æ­¤ç±»æ¨ï¼Œä¸åŒçš„æ ‡è®°ä¼šä½¿æ¨¡å‹è§‚å¯Ÿå¾—åˆ°ä¸åŒçš„æ•ˆæœï¼Œå¯ä»¥æ ¹æ®é¢„æµ‹æ•ˆæœçš„è´¨é‡ï¼Œè®¾ç½®æ ‡ç­¾ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹ä¼˜åŒ–é¢„æµ‹æ•ˆæœã€‚\näºŒã€ æ ‘æ¨¡å‹XGBooståœ¨æ—¶é—´åºåˆ—çš„è¿ç”¨ä¾‹å­ ä¸‹ä¾‹å­ä½¿ç”¨çš„æ—¶åºå˜é‡ Y ä¸ºPJMä¸œéƒ¨åœ°åŒº2001-2018å¹´çš„æ¯å°æ—¶èƒ½è€—ã€‚\n1. å¯¼å…¥åŒ…\n1 2 3 4 5 6 7 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import xgboost as xgb from xgboost import plot_importance, plot_tree from sklearn.metrics import mean_squared_error, mean_absolute_error 2. è¯»å–æ•°æ®ã€åŒºåˆ†éªŒè¯é›†å’Œè®­ç»ƒé›†\n1 2 3 4 5 6 7 8 9 10 pjme = pd.read_csv(\u0026#39;../input/PJME_hourly.csv\u0026#39;, index_col=[0], parse_dates=[0]) # ä»¥2015å¹´ä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å‰²ç‚¹ split_date = \u0026#39;01-Jan-2015\u0026#39; pjme_train = pjme.loc[pjme.index \u0026lt;= split_date].copy() pjme_test = pjme.loc[pjme.index \u0026gt; split_date].copy() # ç»˜åˆ¶æ•°æ®çš„è§‚æµ‹å›¾ color_pal = [\u0026#34;#F8766D\u0026#34;, \u0026#34;#D39200\u0026#34;, \u0026#34;#93AA00\u0026#34;, \u0026#34;#00BA38\u0026#34;, \u0026#34;#00C19F\u0026#34;, \u0026#34;#00B9E3\u0026#34;, \u0026#34;#619CFF\u0026#34;, \u0026#34;#DB72FB\u0026#34;] _ = pjme.plot(style=\u0026#39;.\u0026#39;, figsize=(15,5), color=color_pal[0], title=\u0026#39;PJM East\u0026#39;) ç»“æœå¦‚ä¸‹ï¼ˆå¯ä»¥çœ‹å‡ºåŸºæœ¬ä¸ºå¹³ç¨³åºåˆ—ï¼‰ï¼š 3. è¯»å–æ•°æ®ï¼Œæ„å»ºç‰¹å¾å˜é‡\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # ä»¥æ‹†åˆ†ã€æ ‡è®°æ—¶é—´æ—¥æœŸä½œä¸ºç‰¹å¾å€¼ def create_features(df, label=None): df[\u0026#39;date\u0026#39;] = df.index df[\u0026#39;hour\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.hour df[\u0026#39;dayofweek\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.dayofweek df[\u0026#39;quarter\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.quarter df[\u0026#39;month\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.month df[\u0026#39;year\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.year df[\u0026#39;dayofyear\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.dayofyear df[\u0026#39;dayofmonth\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.day df[\u0026#39;weekofyear\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.weekofyear X = df[[\u0026#39;hour\u0026#39;,\u0026#39;dayofweek\u0026#39;,\u0026#39;quarter\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;year\u0026#39;, \u0026#39;dayofyear\u0026#39;,\u0026#39;dayofmonth\u0026#39;,\u0026#39;weekofyear\u0026#39;]] if label: y = df[label] return X, y return X X_train, y_train = create_features(pjme_train, label=\u0026#39;PJME_MW\u0026#39;) X_test, y_test = create_features(pjme_test, label=\u0026#39;PJME_MW\u0026#39;) 4. å»ºç«‹å’Œè®­ç»ƒæ¨¡å‹\n1 2 3 4 5 reg = xgb.XGBRegressor(n_estimators=1000) reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=False) 5. åœ¨éªŒè¯é›†ä¸ŠéªŒè¯ç»“æœ\n1 2 3 4 pjme_test[\u0026#39;MW_Prediction\u0026#39;] = reg.predict(X_test) pjme_all = pd.concat([pjme_test, pjme_train], sort=False) # ç»“æœå¯è§†åŒ– pjme_all[[\u0026#39;PJME_MW\u0026#39;,\u0026#39;MW_Prediction\u0026#39;]].plot(figsize=(15, 5)) ç»“æœå¦‚ä¸‹ï¼š 6. è¾“å‡ºæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡æ–¹è¯¯å·®ï¼ˆRMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAPEï¼‰\nå‡æ–¹è¯¯å·®è®¡ç®—å…¬å¼ï¼š$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2$ å¹³å‡ç»å¯¹è¯¯å·®è®¡ç®—å…¬å¼ï¼š$MAE =\\frac{\\sum_{i=1}^{n}|y_i-\\hat{y_i}|}{n}$ å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®è®¡ç®—å…¬å¼ï¼š$\\frac{100}{n}\\sum_{i=1}^n\\frac{|y_i-\\hat{y_i}|}{y_i}$ 1 2 3 4 5 6 7 8 9 10 11 12 13 # å‡æ–¹è¯¯å·® mean_squared_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) # å¹³å‡ç»å¯¹è¯¯å·® mean_absolute_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® def mean_absolute_percentage_error(y_true, y_pred): \u0026#34;\u0026#34;\u0026#34;Calculates MAPE given y_true and y_pred\u0026#34;\u0026#34;\u0026#34; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 mean_absolute_percentage_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) ç»“æœä¸ºï¼šRMSE ä¸º 13780445ï¼›MAE ä¸º2848.89ï¼›MAPE ä¸º*8.9%\n","permalink":"https://soso010816.github.io/posts/tm-tree1/","summary":"â€œ å…³äºMLæ ‘æ¨¡å‹åœ¨æ—¶åºæ–¹é¢è¿ç”¨çš„Post â€ ä¸€ã€å¯¹äºå•ä¸ªæ—¶åºå˜é‡çš„é¢„æµ‹ï¼Œå¦‚ä½•è¿›è¡Œç‰¹å¾å˜é‡çš„æ„å»º 1. å¯¹æ—¶åºå˜é‡ Y è¿›è¡Œæ»å å¯¹äºæ—¶åºå˜é‡ Y è¿›è¡Œ æ»åNæ­¥","title":"ML|TSï¼šTree Models With Time Series"},{"content":" â€œ ç®€å•çš„ä»‹ç» â€ Â· åˆè§LSTMğŸ¤¦â€â™‚ï¸ 2022å¹´çš„ç¾èµ›ä¹‹é™…ï¼Œå·¨å¼±ç¬¬ä¸€æ¬¡é‚‚é€…LSTMï¼Œå½“æ—¶å·¨å¼±çš„ç¾èµ›é¢˜æ˜¯å…³äºæ—¶åºé¢„æµ‹ã€åˆ¶å®šäº¤æ˜“ç­–ç•¥æ–¹é¢ï¼Œèµ›å‰é˜Ÿé•¿è®©å·¨å¼±å»æ¥è§¦RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰å’Œ Transformerï¼Œäºæ˜¯ä¹åœ¨å¯’å‡äº†è§£è¿™ä¸¤æ¨¡å‹æ—¶ï¼Œå·¨å¼±ç¢°å·§ä¹Ÿæ¥è§¦åˆ°äº†LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†æ¨¡å‹ï¼‰ï¼Œå½“æ—¶çš„å·¨å¼±éš¾ä»¥å¯é½¿ï¼Œä»€ä¹ˆè®°å¿†é—¨ã€é—å¿˜é—¨ï¼Œå®åœ¨æ˜¯è®©ä¸€ä¸ªä½æ°´å¹³çš„äººèƒ½ä»¥ç†è§£ï¼Œå¥½åœ¨å½“æ—¶çš„å·¨å¼±æœ‰äº›ç¥ç»ç½‘ç»œçš„åŸºç¡€å¹¶ä¸”ç¡¬ç€å¤´çš®åœ¨ç”ŸåƒRNNèµ„æ–™åï¼Œå†å»ç†è§£LSTMæœ€åç»ˆäºç®—æ˜¯å…¥äº†ä¸ªé—¨ã€‚ä¸è¿‡å¯æƒœåæ¥ç¾èµ›å¹¶æœªä½¿ç”¨åˆ°LSTMï¼Œå› æ­¤å·¨å¼±ä¸å®ƒçš„å…³ç³»ç®—æ˜¯å‘Šä¸€æ®µè½ã€‚\nÂ· å†è§LSTMğŸ¤¦â€â™€ï¸ ä¸Šå¤©æ³¨å®šï¼Œæœºç¼˜å·§åˆï¼Œå‰æ®µæ—¶é—´å·¨å¼±é‡åˆ°äº†æ—¶é—´åºåˆ—æ–¹é¢çš„é¢„æµ‹é—®é¢˜éœ€è¦è§£å†³ï¼Œå·¨å¼±æœéè„‘æµ·é™¤äº†ARIMAç­‰ï¼Œå°±æ˜¯LSTMäº†ï¼Œäºæ˜¯ä¹è¿™æ®µå°å°˜å·²ä¹…çš„æ•…äº‹åˆå†æ¬¡ç»­å†™ã€‚è¿™æ¬¡æ­å»ºçš„LSTMç¡®å®æ²¡è®©äººå¤±æœ›ï¼Œæ‹Ÿåˆåº¦ç›´æ¥æ¥è¿‘å®Œç¾ï¼Œä¸å¾—ä¸è¯´è¿™è®©æˆ‘å¯¹LSTMçš„æ„Ÿæƒ…å¥½åƒä¸Šå‡äº†ä¸€ä¸ªçº§åˆ«(/Ï‰ï¼¼)ã€‚ ä¸‹é¢æ˜¯ä¸€å¼ å·¨å¼±ä½¿ç”¨pythonä»£ç å¯¹ kaggle: ads æ•°æ®é›†æ­å»ºLSTMæ¨¡å‹çš„è®­ç»ƒæ‹Ÿåˆå›¾ï¼Œæ•ˆæœçœ‹ä¸Šå»æ˜¯ä¸æ˜¯è¿˜å¯ä»¥ğŸ˜Šï¼š\nâ€œ LSTMç®€ä»‹ â€ LSTMï¼ˆLong short-term memory, é•¿çŸ­æœŸè®°å¿†ï¼‰ æ˜¯ä¸€ç§ç‰¹æ®Šçš„ RNNï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³é•¿åºåˆ—è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œç›¸æ¯”æ™®é€šçš„RNNï¼ŒLSTMèƒ½å¤Ÿåœ¨æ›´é•¿çš„åºåˆ—ä¸­æœ‰æ›´å¥½çš„è¡¨ç°ã€‚LSTM é€šè¿‡é—¨æ§çŠ¶æ€æ¥æ§åˆ¶ä¼ è¾“çŠ¶æ€ï¼Œä»è€Œè®°ä½éœ€è¦é•¿æ—¶é—´è®°å¿†çš„ï¼ŒåŒæ—¶å¿˜è®°ä¸é‡è¦çš„ä¿¡æ¯ï¼Œå¯¹å¾ˆå¤šéœ€è¦ â€œé•¿æœŸè®°å¿†â€ çš„ä»»åŠ¡å°¤å…¶å¥½ç”¨ã€‚\nè¿™é‡Œæ˜¯å·¨å¼±æ‰¾çš„ä¸€ç¯‡å…³äºLSTMåŸç†çš„å®Œæ•´è®²è§£ï¼šUnderstanding LSTM Networks\nâ€œ ä»£ç è¯´æ˜ â€ åªéœ€è¦å‡†å¤‡å¥½indexä¸ºæ—¶é—´ä¸”è¿ç»­ä»¥åŠå°†éœ€è¦æ‹Ÿåˆé¢„æµ‹çš„æ—¶åºå˜é‡å•ç‹¬ä½œä¸º1åˆ—çš„df_xæ•°æ®é›†ï¼Œå³å¯è¿è¯¥æ¨¡å‹ä»£ç ï¼Œä»£ç çš„æœ«å°¾è®¾ç½®äº†look_backå’Œpredict_lengthå‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºæ¨¡å‹æ‹Ÿåˆé¢„æµ‹çš„ç»“æœä¸»è¦ä»¥è¿‡å» look_back = nå¤©ä½œä¸ºå‚è€ƒè¿›è¡Œé¢„æµ‹å’Œä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹é¢„æµ‹é•¿åº¦ä¸º predict_length çš„æœªæ¥æ•°æ®ï¼Œé»˜è®¤ä¸ºlook_back = 1ï¼Œpredict_length = 30 å¤§å®¶å¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚\né™¤æ­¤ä¹‹å¤–ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥å¯¹lstm_modelå‡½æ•°ä¸­çš„ç»“æ„è¿›è¡Œä¿®æ”¹ï¼Œä»¥æ­¤æ ¹æ®éœ€æ±‚ä¼˜åŒ–å¹¶é€‰æ‹©æœ€ä¼˜çš„LSTMæ¨¡å‹ã€‚\nå¯¼å…¥å¿…è¦çš„åŒ… 1 2 3 4 5 6 7 8 9 10 import statsmodels.api as sm from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_absolute_error , mean_squared_error from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization from tensorflow.keras.models import Sequential from pylab import * import seaborn as sns from matplotlib.font_manager import FontProperties mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] æ¨¡å‹çš„æ­å»ºå’Œè®­ç»ƒ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def data_lstm(df): # 1. æ—¶åºæ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†åŠè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ† scale = MinMaxScaler(feature_range = (0, 1)) df = scale.fit_transform(df) train_size = int(len(df) * 0.80) test_size = len(df) - train_size # 2. ç”±äºæ—¶åºå˜é‡å…·æœ‰å…ˆåå…³ç³»ï¼Œå› æ­¤åˆ’åˆ†æ•°æ®é›†æ—¶ä¸€èˆ¬å…ˆå‰ä½œä¸ºè®­ç»ƒé›†ã€åè€…ä½œä¸ºéªŒè¯é›† train, test = df[0:train_size, :], df[train_size:len(df), :] return(train_size, test_size, train, test, df, scale) # 2. lookback è¡¨ç¤ºä»¥è¿‡å»çš„å‡ ä¸ªæ—¥æœŸä½œä¸ºä¸»è¦é¢„æµ‹å˜é‡,è¿™é‡Œæˆ‘é€‰æ‹©çš„é»˜è®¤ä¸º1 # è¾“å…¥æ•°æ®é›† å’Œ è¾“å‡ºæ•°æ®é›† çš„çš„å»ºç«‹ def create_data_set(dataset, look_back=1): data_x, data_y = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i:(i+look_back), 0] data_x.append(a) data_y.append(dataset[i + look_back, 0]) return np.array(data_x), np.array(data_y) # 3. è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®è½¬åŒ– def lstm(train, test, look_back=1): X_train,Y_train,X_test,Y_test = [],[],[],[] X_train,Y_train = create_data_set(train, look_back) X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) X_test, Y_test = create_data_set(test, look_back) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) return (X_train, Y_train, X_test, Y_test) # 4. å®šä¹‰LSTMæ¨¡å‹ç»“æ„ï¼Œ å†…éƒ¨çš„ç»“æ„å‚æ•°å¯ä»¥æ ¹æ¨¡å‹çš„æ‹Ÿåˆç»“æœè¿›è¡Œä¿®æ”¹ def lstm_model(X_train, Y_train, X_test, Y_test): # ç¬¬ä¸€å±‚ï¼Œ256ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor = Sequential() regressor.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1))) regressor.add(Dropout(0.3)) # ç¬¬äºŒå±‚ï¼Œ128ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor.add(LSTM(units = 128, return_sequences = True)) regressor.add(Dropout(0.3)) # ç¬¬ä¸‰å±‚ï¼Œ128ä¸ªç¥ç»å…ƒï¼Œä»¥åŠ0.3çš„æ¦‚ç‡dropoutè¿›è¡Œæ­£åˆ™ regressor.add(LSTM(units = 128)) regressor.add(Dropout(0.3)) regressor.add(Dense(units = 1)) regressor.compile(optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mean_squared_error\u0026#39;) # æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·® reduce_lr = ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, patience=5) # ä¸‹é¢çš„å‚æ•°éƒ½å¯ä»¥è¿›è¡Œä¿®æ”¹ï¼Œä¸€èˆ¬è€Œè¨€batchsizeè¶Šå¤§ä¼šè¶Šå¥½äº›ï¼Œepochsè¡¨ç¤ºè¿­ä»£æ¬¡æ•°ï¼Œå¤§å®¶æ ¹æ®ç»“æœï¼Œå¤§æ¦‚ä½•æ—¶æ”¶æ•›å³å¯å®šä¸ºå¤šå°‘ history =regressor.fit(X_train, Y_train, epochs = 80, batch_size = 8,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False) return(regressor, history) # 5. æ¨¡å‹è®­ç»ƒ def loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history): train_predict = regressor.predict(X_train) test_predict = regressor.predict(X_test) # å°†é¢„æµ‹å€¼è¿›è¡Œåæ ‡å‡†åŒ–ï¼Œå³è¿˜åŸ train_predict = scale.inverse_transform(train_predict) Y_train = scale.inverse_transform([Y_train]) test_predict = scale.inverse_transform(test_predict) Y_test = scale.inverse_transform([Y_test]) # è¾“å‡ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç»å¯¹è¯¯å·®å’Œå‡æ–¹è¯¯å·® print(\u0026#39;Train Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_train[0], train_predict[:,0])) print(\u0026#39;Train Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))) print(\u0026#39;Test Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_test[0], test_predict[:,0])) print(\u0026#39;Test Root Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))) # æŸå¤±å€¼ç»“æœ å¯è§†åŒ– plt.figure(figsize=(16,8)) plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Train Loss\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;Test Loss\u0026#39;) plt.title(\u0026#39;model loss\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.xlabel(\u0026#39;epochs\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() return(train_predict, test_predict, Y_train, Y_test) # 6. ç»˜åˆ¶æ‹Ÿåˆå›¾ï¼Œå¯¹æœªæ¥è¿›è¡Œé¢„æµ‹ def Y_pre(Y_train, Y_test, train_predict, test_predict): Y_real = np.vstack((Y_train.reshape(-1,1), Y_test.reshape(-1,1))) Y_pred = np.vstack((train_predict[:,0].reshape(-1,1), test_predict[:,0].reshape(-1,1))) return(Y_real, Y_pred) def plot_compare(n, Y_real, Y_pred): aa=[x for x in range(n)] plt.figure(figsize=(14,6)) plt.plot(aa, Y_real, marker=\u0026#39;.\u0026#39;, label=\u0026#34;actual\u0026#34;) plt.plot(aa, Y_pred, \u0026#39;r\u0026#39;, label=\u0026#34;prediction\u0026#34;) plt.tight_layout() sns.despine(top=True) plt.subplots_adjust(left=0.07) plt.xticks(size= 15) plt.yticks(size= 15) plt.xlabel(\u0026#39;Time step\u0026#39;, size=15) plt.legend(fontsize=15) plt.show() # 7. æ ¹æ®ä¸€ä¸ªçœŸå®çš„å€¼é¢„æµ‹è¿ç»­çš„é•¿åº¦ def predict_sequences_multiple(model, firstValue, length, look_back=1): prediction_seqs = [] curr_frame = firstValue for i in range(length): predicted = [] predicted.append(model.predict(curr_frame[-look_back:])[0,0]) curr_frame = np.insert(curr_frame, i+look_back, predicted[-1], axis=0) prediction_seqs.append(predicted[-1]) return prediction_seqs è¿è¡Œæ¨¡å‹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026#39;\u0026#39;\u0026#39;åªéœ€è¦å‡†å¤‡å¥½indexä¸ºæ—¶é—´ä¸”è¿ç»­ï¼Œæ—¶åºå˜é‡å•ç‹¬ä¸º1åˆ—çš„df_xæ•°æ®æ¡†ï¼Œå³å¯è¿è¯¥æ¨¡å‹ä»£ç \u0026#39;\u0026#39;\u0026#39; look_back = 1 # å†æ¬¡è¯´æ˜ look_back = n è¡¨ç¤ºé¢„æµ‹ç»“æœä¸»è¦ä»¥è¿‡å» n å¤©ä½œä¸ºå‚è€ƒè¿›è¡Œé¢„æµ‹ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æƒ³æ³•è‡ªè¡Œä¿®æ”¹ predict_length = 30 # é¢„æµ‹æœªæ¥30å¤©çš„æ•°æ®ï¼Œä¸ªäººéœ€è¦æ ¹æ®è‡ªå·±å¯¹æœªæ¥é¢„æµ‹å¤©æ•°çš„éœ€æ±‚è¿›è¡Œé•¿åº¦æ”¹å˜ï¼Œæ­£å¸¸è€Œè¨€çŸ­æœŸå†…çš„é¢„æµ‹ä¼šè¾ƒä¸ºå‡†ç¡® train_size, test_size, train, test, df_x, scale = data_lstm(df_x) X_train, Y_train, X_test, Y_test = lstm(train[:, 0].reshape(train_size, 1), test[:, 0].reshape(test_size, 1), look_back = look_back) regressor, history = lstm_model(X_train, Y_train, X_test, Y_test) train_predict, test_predict, Y_train, Y_test = loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history) Y_real, Y_pre = Y_pre(Y_train, Y_test, train_predict, test_predict) plot_compare(len(Y_real), Y_real, Y_pre) predictions = predict_sequences_multiple(regressor, X_test[-1,:], predict_length, look_back = look_back) \u0026#39;\u0026#39;\u0026#39;é¢„æµ‹æœªæ¥30å¤©çš„æ•°æ®å¹¶ä¿å­˜è‡³pre_30æ•°æ®æ¡†\u0026#39;\u0026#39;\u0026#39; pre_30 = scale.inverse_transform(np.array(predictions).reshape(-1, 1)) ","permalink":"https://soso010816.github.io/posts/lstm/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ Â· åˆè§LSTMğŸ¤¦â€â™‚ï¸ 2022å¹´çš„ç¾èµ›ä¹‹é™…ï¼Œå·¨å¼±ç¬¬ä¸€æ¬¡é‚‚é€…LSTMï¼Œå½“æ—¶å·¨å¼±çš„ç¾èµ›é¢˜æ˜¯å…³äºæ—¶åºé¢„æµ‹ã€åˆ¶å®šäº¤æ˜“ç­–ç•¥æ–¹é¢ï¼Œèµ›å‰é˜Ÿé•¿","title":"DL|TSï¼šLSTM With Python(Time Series Question)"},{"content":" â€œ ç®€å•çš„ä»‹ç» â€ åˆæ¬¡æ¥è§¦XGBoost ç›¸ä¿¡å¤§éƒ¨åˆ†æ¥è§¦è¿‡æ•°æ¨¡ã€æœºå™¨å­¦ä¹ çš„æœ‹å‹éƒ½å¯¹XGBoostæœ‰æ‰€è€³é—»ï¼Œå·¨å¼±æˆ‘ç¬¬ä¸€æ¬¡æ¥è§¦å®ƒæ˜¯åœ¨2021å¹´12æœˆç¬¬ä¸€æ¬¡å‚åŠ æ•°æ¨¡æ¯”èµ›æ—¶ï¼Œå½“æ—¶è‡ªå·±çš„ä»£ç æ°´å¹³è¿˜åªæ˜¯å…¥é—¨ï¼Œä½œä¸ºwrite è®ºæ–‡å’Œåˆ†æ‹…éƒ¨åˆ†å»ºæ¨¡çš„æˆ‘ä¸ºäº†èƒ½å¤Ÿç»™é˜Ÿå‹æ­å»ºçš„XGBoostè¿›è¡Œè¡¨é¢æ¶¦è‰²å’Œæå‡ï¼Œå·¨å¼±åªèƒ½ä¸€ä¸ªåŠ²åœ°ç¿»æ‰¾å„ç§èµ„æ–™ã€æ•™ç¨‹å»ç†è§£å…³äºXGBoostçš„åŸç†ã€‚\nç”±äºåˆšæ¥è§¦ç»Ÿè®¡å­¦ä¸åˆ°åŠå¹´ï¼Œä»å®ƒçš„åº•å±‚æ¶æ„å†³ç­–æ ‘ã€GBDTæ¢¯åº¦æå‡æ ‘å†åˆ°XGBoostä¸€ä¸ªä¸ªæ…¢æ…¢æ¶ˆåŒ–ï¼Œè¿™é˜¶æ®µå±å®èŠ±äº†å·¨å¼±å¤§é‡çš„æ—¶é—´ã€‚æœ€ååœ¨å°†æ¨¡å‹çš„å†…éƒ¨ç»“æ„ã€æ•°å­¦æ¨å¯¼åŸºæœ¬ç†è§£åï¼Œç»“åˆå¤§ä½¬é˜Ÿå‹çš„æœ€ä¼˜å»ºæ¨¡æ‹¿åˆ°äº†å…¨å›½ä¸€ç­‰å¥–ï¼ˆå½“ç„¶æˆ‘è®¤ä¸ºè·å¥–ä¸»è¦çš„åŸå› è¿˜æ˜¯åœ¨è§£å†³å¦ä¸€ä¸ªé—®é¢˜æ—¶ç”¨åˆ°äº†å¤æ‚çš„çº¯æ•°å­¦æ¨å¯¼å’Œæ¦‚ç‡è®ºå»ºæ¨¡ï¼‰ã€‚\nå› æ­¤ä¸å¯å¦è®¤XGBoostä½œä¸ºâ€œæœºå™¨å­¦ä¹ å¤§æ€å™¨â€çš„é­…åŠ›æ‰€åœ¨ã€‚\nXGBooståŸºæœ¬ä»‹ç» XGBoostç”±GBDTä¼˜åŒ–è€Œæˆï¼ŒXGBooståœ¨GBDTçš„åŸºç¡€ä¹‹ä¸Šï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æ­£åˆ™é¡¹ï¼Œå¹¶å¯¹æŸå¤±å‡½æ•°è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ã€å‰”é™¤å¸¸æ•°é¡¹ï¼Œæœ€åä½¿ç”¨è´ªå¿ƒç®—æ³•è®¡ç®—å¢ç›Šåå¯¹å…¶æ ‘çš„æœ€ä½³åˆ†å‰²ç‚¹è¿›è¡Œåˆ†å‰²ï¼Œæå¤§ç¨‹åº¦æé«˜äº†ç›®æ ‡çš„ä¼˜åŒ–æ•ˆç‡ï¼Œå‡å°‘äº†å†…å­˜çš„æ¶ˆè€—ï¼›è€ŒGBDTåˆæ˜¯ç”±å¤šæ£µè¿­ä»£çš„å›å½’æ ‘ç´¯åŠ æ„æˆï¼Œæ¯ä¸€æ£µå›å½’æ ‘å­¦ä¹ çš„æ˜¯ä¹‹å‰æ‰€æœ‰æ ‘çš„ç»“è®ºå’Œæ®‹å·®ï¼Œæ‹Ÿåˆå¾—åˆ°ä¸€ä¸ªå½“å‰çš„æ®‹å·®å›å½’æ ‘ï¼Œæ®‹å·®çš„å…¬å¼ï¼šæ®‹å·® = çœŸå®å€¼ - é¢„æµ‹å€¼ã€‚\nå…·ä½“çš„åŸç†ä»‹ç»å¦‚ä¸‹ï¼šXGBoostçš„åŸºæœ¬ä»‹ç»(å‡ºè‡ªçŸ¥ä¹)\nâ€œ pythonä»£ç çš„ä»‹ç» â€ æœ¬æ¬¡ä»£ç æºäºå·¨å¼±å‰æ®µæ—¶é—´åœ¨è§£å†³ä¸€äº›ä½ç»´æ•°æ®åˆ†ç±»çš„é—®é¢˜ä¸Šå¤šæ¬¡ä½¿ç”¨åˆ°XGBoostï¼Œå¹¶ä¸”æ•ˆæœéå¸¸ä¸é”™ï¼Œå¾®è°ƒå‚æ•°ä¹‹åå‡†ç¡®ç‡å¤§éƒ¨åˆ†éƒ½è¾¾åˆ°äº† 95% ä»¥ä¸Šï¼Œå› æ­¤å·¨å¼±å°†å…¶è¿›è¡Œå†å°è£…ï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨æ—¶èƒ½å‡å°‘æ—¶é—´ï¼Œæå‡æ•ˆç‡ã€‚ä»£ç å¦‚ä¸‹ï¼š\nå¯¼å…¥åŒ… 1 2 3 4 from xgboost import plot_importance from sklearn.model_selection import train_test_split # åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›† from xgboost import XGBRegressor from sklearn.preprocessing import StandardScaler åªéœ€è¦å‡†å¤‡å¥½df_xï¼Œdf_yï¼Œåˆ†åˆ«ä¸ºè‡ªå˜é‡é›†å’Œè¾“å‡ºå€¼çš„æ•°æ®é›†ï¼Œå³å¯è¿è¡Œåˆ†è£…å¥½çš„XGBoostæ¨¡å‹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=7) #7 def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst def xgboost_plot(i = \u0026#39;æ•°å€¼\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False): # i ä¸ºè¾“å‡ºå˜é‡åç§°ï¼Œå¯ä»¥è¿›è¡Œä¿®æ”¹ # nä¸ºè¾“å‡ºå˜é‡åœ¨df_yä¸­ç¬¬å‡ åˆ—,é»˜è®¤æ˜¯ç¬¬ä¸€åˆ— # model_ouputæ˜¯å¦è¿”å›model # mæ˜¯å¦æ”¹å˜æ¨¡å‹å‚æ•° # scaleæ˜¯å¦å¯¹ç‰¹å¾å€¼è¿›è¡Œæ ‡å‡†åŒ– scaler = StandardScaler() if scale == True: x_train = scaler.fit_transform(x_train) x_test = scaler.transform(x_test) if m == True: xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs = 4) y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train, early_stopping_rounds=5, eval_set=[(x_test, y_test)], verbose=False) else: xgb = XGBRegressor() y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train) # æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–ï¼Œxgboost ç»“æœéƒ½ä¸€æ · y_pred = model_xgb.predict(x_test) y_pred_train = model_xgb.predict(x_train) predictions = [round(value) for value in y_pred] plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;è®­ç»ƒé›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, y_train.tolist(), label = \u0026#39;è®­ç»ƒé›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;éªŒè¯é›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, y_test.tolist(), label = \u0026#39;éªŒè¯é›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # ç»˜åˆ¶ç‰¹å¾å€¼å›¾ plot_importance(model_xgb) plt.show() r2_train = R_2(y_train, y_pred_train) r2_test = R_2(y_test, y_pred) print([r2_train, r2_test]) if model_output==True: return model_xgb å¤§å®¶å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œä¿®æ”¹å…¶ä¸­çš„å‚æ•°ï¼Œä»¥æ­¤ä¼˜åŒ–æ¨¡å‹æ•ˆæœã€‚ 1 model = xgboost_plot(i = \u0026#39;æ•°å€¼\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False) ä¸‹é¢æ”¾ä¸€å¼ å·¨å¼±ä½¿ç”¨xgboostå»ºç«‹çš„ä¸€æ¬¡å·¥ä¸šæ¨¡å‹å›¾ï¼Œç”±å›¾å¯è§æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æ‹Ÿåˆåº¦é«˜è¾¾ 99.9% ï¼Œå®åœ¨æ˜¯å¼ºï¼Œå¤§å®¶å¯ä»¥æ‰‹åŠ¨è¯•ä¸€è¯•ï¼š ","permalink":"https://soso010816.github.io/posts/xgboost-python/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ åˆæ¬¡æ¥è§¦XGBoost ç›¸ä¿¡å¤§éƒ¨åˆ†æ¥è§¦è¿‡æ•°æ¨¡ã€æœºå™¨å­¦ä¹ çš„æœ‹å‹éƒ½å¯¹XGBoostæœ‰æ‰€è€³é—»ï¼Œå·¨å¼±æˆ‘ç¬¬ä¸€æ¬¡æ¥è§¦å®ƒæ˜¯åœ¨2021å¹´12æœˆç¬¬","title":"MLï¼šSimple XGBoost With Python"},{"content":" â€œ ç®€å•çš„ä»‹ç» â€ åœ¨ç»Ÿè®¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿç»å¸¸é‡åˆ°é«˜ç»´æ•°æ®çš„é—®é¢˜ï¼Œæ¯”å¦‚å›¾ç‰‡å¤„ç†ï¼Œå›¾ç‰‡çš„å¤„ç†æŠ€æœ¯åœ¨ç›®å‰ä¹Ÿæ˜¯éå¸¸çƒ­é—¨ï¼Œä¸æ–­è¢«æ¢ç´¢çš„é¢†åŸŸï¼Œæœ¬æ¬¡å­¦ä¹ blogä¸ºå¤§å®¶å¸¦æ¥é„™äººå°è£…çš„å·ç§¯ç¥ç»ç½‘ç»œpythonä»£ç ï¼Œä¾›å¤§å®¶å¤„ç†åŸºæœ¬çš„å›¾ç‰‡åˆ†ç±»é¢„æµ‹é—®é¢˜å¹¶å°†ç»“æœè¿›è¡Œå¯è§†åŒ–ï¼Œå½“ç„¶å¤§å®¶ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±éœ€æ±‚ä¿®æ”¹ä»£ç ä¸­çš„parameså‚æ•°ï¼Œä»è€Œé€‰æ‹©å‡ºé¢„æµ‹æ•ˆæœæœ€ä½³çš„æ¨¡å‹ã€‚ â€œ CNNçš„åŸºæœ¬åŸç† â€ å…³äºCNNçš„åŸºæœ¬ä»‹ç»å¤§å®¶å¯ä»¥åœ¨ A Simple Introduction About CNN é‡Œè¿›è¡Œå­¦ä¹ ã€‚\nâ€œ pythonä»£ç  â€ æœ¬æ¬¡çš„å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨çš„pytorchåŒ…ï¼Œåªè¦æ±‚æœ‰å‹å‹æ‰€ä½¿ç”¨çš„è®­ç»ƒå›¾ç‰‡æ•°æ®é›†æ ‡è®°å¥½å¹¶æ”¾è‡³æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç›®å½•ï¼Œå³å¯ä»¥è¿è¡Œé„™äººçš„CNNå‡½æ•°ã€‚å¹¶ä¸”ä¸€èˆ¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸»è¦ç”¨äºå›¾åƒå¤„ç†æŠ€æœ¯ï¼Œ å› æ­¤æœ¬ä»£ç é’ˆå¯¹çš„æ•°æ®é›†ä¸º jpgã€pngç­‰å›¾ç‰‡æ•°æ®ã€‚\nå¯¼å…¥æ‰€éœ€çš„åº“ pytorchã€globã€numpyã€sklearnã€matplotlibã€copy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch import torch.nn as nn import torch.nn.functional as F from torch import optim from torchvision import utils from torch.optim.lr_scheduler import ReduceLROnPlateau from torchvision import datasets, models, transforms from torch.utils.data import DataLoader, Dataset import copy import glob import numpy as np import plotly.graph_objs as go import matplotlib.pyplot as plt from plotly.subplots import make_subplots from sklearn.model_selection import train_test_split %matplotlib inline æ•°æ®é›†çš„æ„å»º CPU or CUDA\n1 2 3 4 5 6 7 8 9 10 11 # 1. é€‰æ‹© CPU è¿˜æ˜¯ GPU ç‰ˆçš„ pytorch è¿›è¡Œå»ºæ¨¡ device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; # 2. è®¾ç½®éšæœºç§å­ torch.manual_seed(816) if device ==\u0026#39;cuda\u0026#39;: torch.cuda.manual_seed_all(816) # å°†æ‰€æœ‰å›¾ç‰‡æ•°æ®çš„è·¯å¾„å­˜å‚¨è‡³åˆ—è¡¨ä¸­, train_dirå’Œ test_dirä¸ºè®­ç»ƒé›†å›¾ç‰‡å’ŒéªŒè¯é›†å›¾ç‰‡æ‰€åœ¨çš„æ–‡ä»¶å¤¹ train_list = glob.glob(os.path.join(train_dir,\u0026#39;*.jpg\u0026#39;)) # å¦‚æœå›¾ç‰‡ä¸ºpngå½¢å¼ï¼Œåˆ™å°†jpgæ”¹æˆpngå³å¯ test_list = glob.glob(os.path.join(test_dir, \u0026#39;*.jpg\u0026#39;)) # å¦‚æœæœ‰æµ‹è¯•é›†åˆ™ä½¿ç”¨è¿™è¡Œä»£ç ï¼Œå¦åˆ™å¯ä»¥åˆ é™¤ æ¨¡å‹çš„æ­å»º æ•°æ®å¢å¼ºã€æ¨¡å‹ç»“æ„è®¾è®¡ã€æ¨¡å‹çš„è®­ç»ƒä¸ä¿æŒã€ç»“æœå¯è§†åŒ–\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 # 3. å®šä¹‰å®Œæ•´çš„ CNN æ¡†æ¶æ¨¡å‹ def So_CNN_model(train_list, test_list = None): # è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å‰² train_list, val_list = train_test_split(train_list, test_size=0.2) # 4. å›¾ç‰‡çš„é¢„å¤„ç†ï¼Œå›¾ç‰‡å¢å¼º train_transforms = transforms.Compose([ transforms.Resize((224, 224)), # è®¾è®¡è®­ç»ƒå›¾ç‰‡è½¬åŒ–ä¸º224*224å›¾ç‰‡å¤§å°ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œä¿®æ”¹å¤§å° transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ]) val_transforms = transforms.Compose([ transforms.Resize((224, 224)), # è®¾è®¡éªŒè¯é›†å›¾ç‰‡è½¬åŒ–ä¸º224*224å›¾ç‰‡å¤§å° transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ]) # å¦‚æœæœ‰æµ‹è¯•é›†éœ€è¦è¾“å‡ºï¼Œåˆ™ä¹Ÿå¯¹éªŒè¯é›†è¿›è¡Œå›¾ç‰‡è½¬åŒ–å¢å¼º if test_list != None: test_transforms = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor() ]) # 5. å®šä¹‰ä¸€ä¸ªç±»è¿›è¡Œæ•°æ®è½¬åŒ–ã€å›¾ç‰‡æ•°æ®é›†çš„å¤„ç†ã€ä»¥åŠå›¾ç‰‡çš„æ ‡æ³¨è½¬åŒ– class dataset(torch.utils.data.Dataset): def __init__(self, file_list, transform=None): self.file_list = file_list # å›¾ç‰‡åå­—åˆ—è¡¨ self.transform = transform # è½¬åŒ–å™¨ self.label_list = label_list # æ‰€æœ‰çš„æ ‡ç­¾ç§ç±» def __len__(self): self.filelength = len(self.file_list) return self.filelength # å›¾ç‰‡æ•°ç›® # å¯¹äºæœ¬åœ°å›¾ç‰‡çš„ä¸‹è½½ä¸æ ‡æ³¨ def __getitem__(self,idx): img_path = self.file_list[idx] img = Image.open(img_path) # æ‰“å¼€æœ¬åœ°å›¾ç‰‡æ•°æ®é›†æ‰€åœ¨çš„ä½ç½® img_transformed = self.transform(img) # æ•°æ®å¢å¼º label = img_path.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;.\u0026#39;)[0] # å¯¹å›¾ç‰‡åå­—åˆ†å‰²ï¼Œå‰ææ˜¯å›¾ç‰‡åå­—å³ä¸ºæ ‡æ³¨ label = label_list.index(label) # æœç´¢è¯¥æ ‡ç­¾åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œå¹¶å°†å…¶è¿›è¡Œæ•°å€¼æ ‡æ³¨ï¼Œæœ‰å‡ ä¸ªç§ç±»æ•°å€¼å°±æœ‰å‡ ç§ return img_transformed, label # 6. å®šä¹‰å„ä¸ªç±» train_data = dataset(train_list, transform=train_transforms) if test_list != None: test_data = dataset(test_list, transform=test_transforms) val_data = dataset(val_list, transform=test_transforms) # 7. æ•°æ®åŠ è½½ batch_size = 100 train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True ) if test_list != None: test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle=True) val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle=True) # 8. CNNæ¨¡å‹æ¶æ„çš„è®¾è®¡ class Cnn(nn.Module): def __init__(self): super(Cnn,self).__init__() \u0026#39;\u0026#39;\u0026#39;ä»¥ä¸‹æ‰€æœ‰å±‚çš„ç»“æ„å‚æ•°å’Œå±‚æ•°æ•°é‡éƒ½å¯ä»¥è¿›è¡Œéœ€ä¿®æ”¹ å¯ä»¥æ¯”è¾ƒå‚æ•°ä¸åŒäº§ç”Ÿçš„æ¨¡å‹ç»“æœï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜å‚æ•°\u0026#39;\u0026#39;\u0026#39; c,h,w = params[\u0026#39;shape_in\u0026#39;] # åˆå§‹ æ•°æ®ç»“æ„ f = params[\u0026#39;initial_filters\u0026#39;] # åˆå§‹ æ•°æ®è½¬åŒ–çš„å±‚æ•° num_classes = params[\u0026#39;num_classes\u0026#39;] # éœ€è¦åˆ†ç±»çš„æ€»æ•° num_fc1 = params[\u0026#39;num_fc1\u0026#39;] # å…¨è¿æ¥å±‚çš„ç¬¬ä¸€å±‚ dropout_rate = params[\u0026#39;dropout_rate\u0026#39;] # ç¬¬ä¸€å±‚å·ç§¯å±‚ï¼Œå°†3é€šé“ c ç»´æ•°æ®è½¬åŒ–ä¸º f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0 self.layer1 = nn.Sequential( nn.Conv2d(c, f, kernel_size = 3, padding=0, stride=2), nn.BatchNorm2d(f), # f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç† nn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹© nn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2 çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ– ) # ç¬¬ä¸€å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦ h,w = findConv2dOutShape(h, w, nn.MaxPool2d(2)) h,w = h/2, w/2 # ç¬¬äºŒå±‚å·ç§¯å±‚ï¼Œå°† f ç»´æ•°æ®è½¬åŒ–ä¸º 2*f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0 self.layer2 = nn.Sequential( nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(2*f), # 2*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç† nn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹© nn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ– ) # ç¬¬äºŒå±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦ h,w = findConv2dOutShape(h, w, nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # ç¬¬ä¸‰å±‚å·ç§¯å±‚ï¼Œå°† 32 ç»´æ•°æ®è½¬åŒ–ä¸º 64 ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0 self.layer3 = nn.Sequential( nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(4*f), # 4*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç† nn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹© nn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ– ) # ç¬¬ä¸‰å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦\th,w = findConv2dOutShape(h, w, nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # ç¬¬å››å±‚å·ç§¯å±‚ï¼Œå°† 4*f ç»´æ•°æ®è½¬åŒ–ä¸º 8*f ç»´æ•°æ®ï¼Œå·ç§¯çš„çŸ©é˜µå¤§å°ä¸º3*3ï¼Œæ­¥é•¿ä¸º2ï¼Œå¡«ç™½å¤§å°ä¸º0 self.layer4 = nn.Sequential( nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(8*f), # 8*f ç»´ æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç† nn.ReLU(), # æ¿€æ´»å‡½æ•°ä¸º ReLU å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œé‡æ–°é€‰æ‹© nn.MaxPool2d(2) # æ± åŒ–å±‚ï¼Œ2*2çŸ©é˜µå¤§å°è¿›è¡Œæ± åŒ– ) # ç¬¬å››å±‚åï¼Œè½¬åŒ–å¾—åˆ°çš„ç»´åº¦ h,w = findConv2dOutShape(h, w, nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # æœ€å æˆ‘è®¾è®¡äº†2å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œç»“æ„ self.num_flatten= h * w* 8*f self.fc1 = nn.Linear(self.num_flatten, num_fc1) self.dropout = nn.Dropout(dropout_rate) # ä»¥ 0.5 çš„æ¦‚ç‡å¯¹å…¶è¿›è¡Œå‰”é™¤ self.fc2 = nn.Linear(num_fc1, num_class) self.relu = nn.ReLU() # å®šä¹‰æ¿€æ´»å‡½æ•° RELU self.softmax = nn.log_softmax() # å®šä¹‰ æœ€åçš„è¾“å‡ºå‡½æ•° Softmax # å®šä¹‰å‘å‰ä¼ æ’­ def forward(self,x): out = self.layer1(x) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = out.view(-1, self.num_flatten) out = self.relu(self.fc1(out)) out = self.softmax(self.fc2(out), dim = 1) return out params_model={ \u0026#34;shape_in\u0026#34;: (3, 224, 224), \u0026#34;initial_filters\u0026#34;: 8, \u0026#34;num_fc1\u0026#34;: 100, \u0026#34;dropout_rate\u0026#34;: 0.25, \u0026#34;num_classes\u0026#34;: len(label_list)} # num_class,æ ¹æ®ç±»åˆ«çš„æ€»æ•°è€Œå®š # ä¼ è¾¾æ¨¡å‹ç»“æ„ç»™cnn_model cnn_model = Cnn(params_model) device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = cnn_model.to(device) # 9. å®šä¹‰æŸå¤±å‡½æ•° loss_func = nn.NLLLoss(reduction=\u0026#34;sum\u0026#34;) # 10. å®šä¹‰ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–å™¨å°†ä¿æŒå½“å‰çŠ¶æ€ï¼Œå¹¶æ ¹æ®è®¡ç®—çš„æ¢¯åº¦æ›´æ–°å‚æ•° opt = optim.Adam(cnn_model.parameters(), lr=3e-4) lr_scheduler = ReduceLROnPlateau(opt, mode=\u0026#39;min\u0026#39;,factor=0.5, patience=20,verbose=1) # 11. å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•° def train_val(model, params,verbose=False): # è·å–è®­ç»ƒå‚æ•° epochs = params[\u0026#34;epochs\u0026#34;] loss_func = params[\u0026#34;loss_func\u0026#34;] opt = params[\u0026#34;optimiser\u0026#34;] train_dl = params[\u0026#34;train\u0026#34;] val_dl = params[\u0026#34;val\u0026#34;] check = params[\u0026#34;check\u0026#34;] lr_scheduler = params[\u0026#34;lr_change\u0026#34;] weight_path = params[\u0026#34;weight_path\u0026#34;] loss_history = {\u0026#34;train\u0026#34;: [],\u0026#34;val\u0026#34;: []} # æ¯æ¬¡ epoch çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æŸå¤±å€¼ metric_history = {\u0026#34;train\u0026#34;: [],\u0026#34;val\u0026#34;: []} # æ¯æ¬¡ epoch çš„ metricå€¼ best_model_wts = copy.deepcopy(model.state_dict()) # æ·±åº¦å¤åˆ¶æœ€ä½³æ€§èƒ½æ¨¡å‹çš„æƒé‡ best_loss = float(\u0026#39;inf\u0026#39;) # å°†æœ€ä½³çš„æŸå¤±å€¼åˆå§‹åŒ–ä¸ºæå¤§å€¼ # è¿­ä»£å¾ªç¯ for epoch in range(epochs): # è·å–å­¦ä¹ ç‡ current_lr = get_lr(opt) if(verbose): print(\u0026#39;Epoch {}/{}, current lr={}\u0026#39;.format(epoch, epochs - 1, current_lr)) # ä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒ CNN æ¨¡å‹ model.train() train_loss, train_metric = loss_epoch(model,loss_func,train_dl,check,opt) # æ”¶é›†è®­ç»ƒæ•°æ®é›†çš„æŸå¤±å’Œè¡¡é‡æ ‡å‡† loss_history[\u0026#34;train\u0026#34;].append(train_loss) metric_history[\u0026#34;train\u0026#34;].append(train_metric) # ä½¿ç”¨éªŒè¯é›†å¯¹æ¨¡å‹ç»“æœè¿›è¡Œè¯„ä¼° model.eval() with torch.no_grad(): val_loss, val_metric = loss_epoch(model, loss_func, val_dl,check) # é€‰æ‹©æœ€å¥½çš„å‚æ•°æ¨¡å‹ if val_loss \u0026lt; best_loss: best_loss = val_loss best_model_wts = copy.deepcopy(model.state_dict()) # å­˜å‚¨æ¨¡å‹å‚æ•°è‡³æœ¬åœ°æ–‡ä»¶ torch.save(model.state_dict(), weight_path) if(verbose): print(\u0026#34;å·²ç»ä¿å­˜å®Œè®­ç»ƒå¾—åˆ°çš„æœ€å¥½æ¨¡å‹ï¼\u0026#34;) # å­˜å‚¨éªŒè¯æ•°æ®é›†çš„æŸå¤±å’Œè¡¡é‡æ ‡å‡† loss_history[\u0026#34;val\u0026#34;].append(val_loss) metric_history[\u0026#34;val\u0026#34;].append(val_metric) # å­¦ä¹ ç‡ç­›é€‰ lr_scheduler.step(val_loss) if current_lr != get_lr(opt): if(verbose): print(\u0026#34;å·²ç»åŠ è½½å®ŒCNNæ¨¡å‹ï¼\u0026#34;) model.load_state_dict(best_model_wts) if(verbose): print(f\u0026#34;train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\u0026#34;) print(\u0026#34;-\u0026#34;*10) # å­˜å‚¨æ¨¡å‹çš„æƒé‡å’Œå‚æ•°æ•°æ®è‡³æœ¬åœ° model.load_state_dict(best_model_wts) return model, loss_history, metric_history params_train={ \u0026#34;train\u0026#34;: train_dl,\u0026#34;val\u0026#34;: val_dl, \u0026#34;epochs\u0026#34;: 50, # è¿­ä»£ 50 æ¬¡ \u0026#34;optimiser\u0026#34;: optim.Adam(cnn_model.parameters(), lr=3e-4), \u0026#34;lr_change\u0026#34;: ReduceLROnPlateau(opt, mode = \u0026#39;min\u0026#39;, factor = 0.5, patience = 20, verbose = 0), \u0026#34;loss_func\u0026#34;: nn.NLLLoss(reduction = \u0026#34;sum\u0026#34;), \u0026#34;weight_path\u0026#34;: \u0026#34;weights.pt\u0026#34;, \u0026#34;check\u0026#34;: False, } # è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹ cnn_model,loss_hist,metric_hist = train_val(cnn_model, params_train) # è®­ç»ƒå‚æ•°è¿›ç¨‹ epochs = params_train[\u0026#34;epochs\u0026#34;] # ç»˜åˆ¶ç»“æœå›¾ fig = make_subplots(rows = 1, cols = 2, subplot_titles = [\u0026#39;æŸå¤±å€¼-æŠ˜çº¿å›¾\u0026#39;,\u0026#39;å‡†ç¡®ç‡-æŠ˜çº¿å›¾\u0026#39;]) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026#34;train\u0026#34;], name = \u0026#39;è®­ç»ƒé›†çš„æŸå¤±å€¼\u0026#39;), row = 1, col = 1) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026#34;val\u0026#34;], name = \u0026#39;éªŒè¯é›†çš„æŸå¤±å€¼\u0026#39;), row = 1, col = 1) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026#34;train\u0026#34;], name = \u0026#39;è®­ç»ƒé›†çš„å‡†ç¡®ç‡\u0026#39;), row = 1, col = 2) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026#34;val\u0026#34;], name = \u0026#39;éªŒè¯é›†çš„å‡†ç¡®ç‡\u0026#39;), row = 1, col = 2) fig.update_layout(template = \u0026#39;plotly_white\u0026#39;); fig.update_layout(margin = {\u0026#34;r\u0026#34;:0,\u0026#34;t\u0026#34;:60,\u0026#34;l\u0026#34;:0,\u0026#34;b\u0026#34;:0}, height= 300) fig.show() So_CNN_model(train_list, test_list) æ„Ÿè°¢å„ä½å‹å‹èƒ½çœ‹åˆ°æœ€åï¼é™„ä¸€å¼ æˆ‘è¶…çº§å–œæ¬¢çš„æ•°å­¦å®‡å®™gifä»£è¡¨ç»“æŸï¼\nThe Endï¼\n","permalink":"https://soso010816.github.io/posts/cnn-python/","summary":"â€œ ç®€å•çš„ä»‹ç» â€ åœ¨ç»Ÿè®¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿç»å¸¸é‡åˆ°é«˜ç»´æ•°æ®çš„é—®é¢˜ï¼Œæ¯”å¦‚å›¾ç‰‡å¤„ç†ï¼Œå›¾ç‰‡çš„å¤„ç†æŠ€æœ¯åœ¨ç›®å‰ä¹Ÿæ˜¯éå¸¸çƒ­é—¨ï¼Œä¸æ–­è¢«æ¢ç´¢çš„é¢†åŸŸï¼Œæœ¬æ¬¡å­¦ä¹ blog","title":"DLï¼šCNN With Python"},{"content":" â€œ ç®€çŸ­çš„ä»‹ç» â€ è¿™æ®µæ—¶é—´å‚åŠ çš„æ•°æ¨¡æ¨¡æ‹Ÿåˆšå¥½ç”¨åˆ°äº†æ·±åº¦å­¦ä¹ çš„åº•å±‚æ¶æ„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œäºæ˜¯è‡ªå·±ç´¢æ€§å°±å°†æˆ‘æ•°æ¨¡ä¸­ç”¨åˆ°çš„ä»£ç å°è£…äº†ä»¥ä¸‹ï¼Œåšæˆå¦‚ä¸‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹å‡½æ•°ä»¥åŠåŒ…æ‹¬ç»“æœçš„å¯è§†åŒ–ã€æ‹Ÿåˆåº¦çš„è®¡ç®—ç»“æœè¾“å‡ºï¼Œæ–¹ä¾¿å„ä½å‹å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\nåæœŸæœ‰æ—¶é—´çš„è¯ï¼Œæˆ‘ä¹Ÿä¼šå†™ä¸€ä¸ªé—ä¼ ç®—æ³•æˆ–è€…ç²’å­ç¾¤ç®—æ³•ï¼ˆmaybeæ˜¯å…¶ä»–å¯å‘å¼ç®—æ³•ï¼‰ç”¨æ¥å’Œä¸‹é¢çš„å‡½æ•°ç»“åˆï¼Œè‡ªåŠ¨å¸®å„ä½æ‰¾åˆ°é¢„æµ‹ç»“æœæœ€ä¼˜çš„æ¨¡å‹å‚æ•°ã€‚\nè¿™é‡Œæ˜¯ä¸€ç¯‡æˆ‘çš„å…³äºç¥ç»ç½‘ç»œçš„åŸç†ä»‹ç»ï¼Œé‡Œé¢æœ‰å…³äºç¥ç»ç½‘ç»œéå¸¸è¯¦ç»†çš„ä»‹ç»ï¼š ç¥ç»ç½‘ç»œç”±æ¥åŠåŸç†\nâ€œ Tips â€ å½“ç„¶é¢å¯¹ä¸åŒçš„æ•°æ®ï¼Œæœ€ä¼˜çš„ç¥ç»ç½‘ç»œç»“æ„å’Œå‚æ•°ä¼šæœ‰æ‰€ä¸åŒï¼Œå¤§å®¶å¯ä»¥æ ¹æ®è‡ªå·±çš„æ‹Ÿåˆç»“æœï¼Œä¿®æ”¹æˆ‘ä¸‹é¢çš„å‡½æ•°å‚æ•°ï¼Œä»è€Œè·å–æœ€ä¼˜æ¨¡å‹ã€‚\nå¦‚æœå„ä½å‹å‹é¢å¯¹çš„æ˜¯åˆ†ç±»é—®é¢˜ï¼Œåªéœ€æŠŠæ¿€æ´»å‡½æ•°æ”¹æˆsoftmaxå³å¯ï¼Œå½“ç„¶æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥è¿›è¡Œä¿®æ”¹ï¼Œå…³äºæŸå¤±å‡½æ•°çš„ç ”ç©¶æ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„Huber losså’ŒM-regressionï¼Œæ„Ÿå…´è¶£çš„å‹å‹å¯ä»¥è‡ªè¡ŒæŸ¥é˜…ç›¸å…³æ–‡çŒ®ã€‚\nâ€œ Simple Test â€ ä¸‹é¢æ˜¯æœ¬äººç”¨ä¸‹è¿°è‡ªå·±å†™çš„ä»£ç å»ºç«‹çš„å·¥ä¸šæ¨¡å‹æ‹Ÿåˆç»“æœï¼Œæ‹Ÿåˆåº¦è¾¾åˆ°äº† 95% ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚å’Œç»“æœçš„æ•ˆæœä¿®æ”¹å…¶ä¸­çš„å‚æ•°ï¼Œæˆ–è€…å¢åŠ éšè—å±‚ï¼Œä»è€Œä¼˜åŒ–è‡ªå·±çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚\nâ€œ pythonä»£ç  â€ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_scoreã€ import matplotlib.pyplot as plt from keras import regularizers from sklearn.preprocessing import MinMaxScaler from keras.models import Sequential from keras.layers import Dense, Dropout from sklearn import preprocessing def NN_Plot(i, X, Y, model_output = False): # i ä¸ºéœ€è¦è¾“å‡ºå›¾è¡¨çš„yè½´æ ‡ç­¾ï¼› # X ä¸ºè‡ªå˜é‡çš„æ•°æ®é›†ï¼› # Y ä¸ºè¾“å‡ºå˜é‡çš„æ•°æ®ï¼› # model_output è¡¨ç¤ºæ˜¯å¦è¿”å› è®­ç»ƒåçš„æ¨¡å‹ï¼› # 1. æ•°æ®é›†æ ‡å‡†åŒ– min_max_scaler = preprocessing.MinMaxScaler() X_scale = min_max_scaler.fit_transform(X) # 2.è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ’åˆ† X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, test_size=0.3, random_state = n) # 3. æ¨¡å‹çš„ç»“æ„è®¾è®¡ model = Sequential() # åˆå§‹åŒ–ï¼Œå¾ˆé‡è¦ model.add(Dense(units = 1000, # è¾“å‡ºå¤§å°ï¼Œä¹Ÿæ˜¯è¯¥å±‚ç¥ç»å…ƒçš„ä¸ªæ•° activation=\u0026#39;relu\u0026#39;, # æ¿€åŠ±å‡½æ•°-RELU input_shape=(X_train.shape[1],) # è¾“å…¥å¤§å°, ä¹Ÿå°±æ˜¯åˆ—çš„å¤§å° )) model.add(Dropout(0.3)) # ä¸¢å¼ƒç¥ç»å…ƒé“¾æ¥æ¦‚ç‡ model.add(Dense(units = 1000, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation=\u0026#39;relu\u0026#39; # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.15)) model.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation=\u0026#39;relu\u0026#39; # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.15)) model.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # æ–½åŠ åœ¨æƒé‡ä¸Šçš„æ­£åˆ™é¡¹ activity_regularizer=regularizers.l1(0.01), # æ–½åŠ åœ¨è¾“å‡ºä¸Šçš„æ­£åˆ™é¡¹ activation=\u0026#39;relu\u0026#39; # æ¿€åŠ±å‡½æ•° # bias_regularizer=keras.regularizers.l1_l2(0.01) # æ–½åŠ åœ¨åç½®å‘é‡ä¸Šçš„æ­£åˆ™é¡¹ )) model.add(Dropout(0.2)) model.add(Dense(units = 1, activation=\u0026#39;linear\u0026#39;, kernel_regularizer=regularizers.l2(0.01) # çº¿æ€§æ¿€åŠ±å‡½æ•°å›å½’ä¸€èˆ¬åœ¨è¾“å‡ºå±‚ç”¨è¿™ä¸ªæ¿€åŠ±å‡½æ•° )) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mse\u0026#39;, # æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·® metrics=[\u0026#39;accuracy\u0026#39;]) # 4. æ¨¡å‹çš„è®­ç»ƒï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹batchâ€”â€”sizeå¤§å°å’Œepochå¤§å° hist = model.fit(X_train, Y_train, batch_size = 32, epochs=250, verbose = 2, validation_data=(X_test, Y_test)) # 5. æ¨¡å‹çš„æŸå¤±å€¼å˜åŒ–å›¾ç»˜åˆ¶ plt.plot(hist.history[\u0026#39;loss\u0026#39;]) plt.plot(hist.history[\u0026#39;val_loss\u0026#39;]) plt.title(\u0026#39;Model loss\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.legend([\u0026#39;Train\u0026#39;, \u0026#39;Val\u0026#39;], loc=\u0026#39;upper right\u0026#39;) plt.show() y_pred = model.predict(X_test) y_pred_train = model.predict(X_train) # 6. æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ‹Ÿåˆç¨‹åº¦å›¾ç»˜åˆ¶ plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;è®­ç»ƒé›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, Y_train.iloc[:,0].tolist(), label = \u0026#39;è®­ç»ƒé›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;éªŒè¯é›†çš„é¢„æµ‹å€¼\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, Y_test.iloc[:,0].tolist(), label = \u0026#39;éªŒè¯é›†çš„çœŸå®å€¼\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # Ræ–¹çš„è®¡ç®— r2_train = R_2(Y_train.iloc[:,0].tolist(), y_pred_train) r2_test = R_2(Y_test.iloc[:,0].tolist(), y_pred) print([r2_train, r2_test, (r2_train+r2_test)/2 ]) # æ˜¯å¦è¿”å›è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ if model_output==True: return [model, min_max_scaler] def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst æ„Ÿè°¢è§‚çœ‹ï¼The Endï¼\n","permalink":"https://soso010816.github.io/posts/nn-python/","summary":"â€œ ç®€çŸ­çš„ä»‹ç» â€ è¿™æ®µæ—¶é—´å‚åŠ çš„æ•°æ¨¡æ¨¡æ‹Ÿåˆšå¥½ç”¨åˆ°äº†æ·±åº¦å­¦ä¹ çš„åº•å±‚æ¶æ„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œäºæ˜¯è‡ªå·±ç´¢æ€§å°±å°†æˆ‘æ•°æ¨¡ä¸­ç”¨åˆ°çš„ä»£ç å°è£…äº†ä»¥ä¸‹ï¼Œåšæˆå¦‚ä¸‹çš„ç¥ç»ç½‘","title":"DLï¼šNeural Network With Python"},{"content":"","permalink":"https://soso010816.github.io/posts/diary/","summary":"","title":""},{"content":"","permalink":"https://soso010816.github.io/about/","summary":"","title":"About"}]