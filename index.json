[{"content":" “ 简单的介绍 ” · 初见LSTM🤦‍♂️ 2022年的美赛之际，巨弱第一次邂逅LSTM，当时巨弱的美赛题是关于时序预测、制定交易策略方面，赛前队长让巨弱去接触RNN（循环神经网络）和 Transformer，于是乎在寒假了解这两模型时，巨弱碰巧也接触到了LSTM（长短期记忆模型），当时的巨弱难以启齿，什么记忆门、遗忘门，实在是让一个低水平的人能以理解，好在当时的巨弱有些神经网络的基础并且硬着头皮在生吃RNN资料后，再去理解LSTM最后终于算是入了个门。不过可惜后来美赛并未使用到LSTM，因此巨弱与它的关系算是告一段落。\n· 再见LSTM🤦‍♀️ 上天注定，机缘巧合，前段时间巨弱遇到了时间序列方面的预测问题需要解决，巨弱搜遍脑海除了ARIMA等，就是LSTM了，于是乎这段封尘已久的故事又再次续写。这次搭建的LSTM确实没让人失望，拟合度直接接近完美，不得不说这让我对LSTM的感情好像上升了一个级别(/ω＼)。 下面是一张巨弱使用python代码对 kaggle: ads 数据集搭建LSTM模型的训练拟合图，效果看上去是不是还可以😊：\n“ LSTM简介 ” LSTM（Long short-term memory, 长短期记忆） 是一种特殊的 RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。LSTM 通过门控状态来控制传输状态，从而记住需要长时间记忆的，同时忘记不重要的信息，对很多需要 “长期记忆” 的任务尤其好用。\n这里是巨弱找的一篇关于LSTM原理的完整讲解：Understanding LSTM Networks\n“ 代码说明 ” 只需要准备好index为时间且连续以及将需要拟合预测的时序变量单独作为1列的df_x数据集，即可运该模型代码，代码的末尾设置了look_back和predict_length参数，分别表示模型拟合预测的结果主要以过去 look_back = n天作为参考进行预测和使用训练得到的模型预测长度为 predict_length 的未来数据，默认为look_back = 1，predict_length = 30 大家可以根据需求进行修改。\n除此之外，大家也可以对lstm_model函数中的结构进行修改，以此根据需求优化并选择最优的LSTM模型。\n导入必要的包 1 2 3 4 5 6 7 8 9 10 import statsmodels.api as sm from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_absolute_error , mean_squared_error from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization from tensorflow.keras.models import Sequential from pylab import * import seaborn as sns from matplotlib.font_manager import FontProperties mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] 模型的搭建和训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def data_lstm(df): # 1. 时序数据的标准化处理及训练集和验证集划分 scale = MinMaxScaler(feature_range = (0, 1)) df = scale.fit_transform(df) train_size = int(len(df) * 0.80) test_size = len(df) - train_size # 2. 由于时序变量具有先后关系，因此划分数据集时一般先前作为训练集、后者作为验证集 train, test = df[0:train_size, :], df[train_size:len(df), :] return(train_size, test_size, train, test, df, scale) # 2. lookback 表示以过去的几个日期作为主要预测变量,这里我选择的默认为1 # 输入数据集 和 输出数据集 的的建立 def create_data_set(dataset, look_back=1): data_x, data_y = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i:(i+look_back), 0] data_x.append(a) data_y.append(dataset[i + look_back, 0]) return np.array(data_x), np.array(data_y) # 3. 训练集和验证集的数据转化 def lstm(train, test, look_back=1): X_train,Y_train,X_test,Y_test = [],[],[],[] X_train,Y_train = create_data_set(train, look_back) X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) X_test, Y_test = create_data_set(test, look_back) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) return (X_train, Y_train, X_test, Y_test) # 4. 定义LSTM模型结构， 内部的结构参数可以根模型的拟合结果进行修改 def lstm_model(X_train, Y_train, X_test, Y_test): # 第一层，256个神经元，以及0.3的概率dropout进行正则 regressor = Sequential() regressor.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1))) regressor.add(Dropout(0.3)) # 第二层，128个神经元，以及0.3的概率dropout进行正则 regressor.add(LSTM(units = 128, return_sequences = True)) regressor.add(Dropout(0.3)) # 第三层，128个神经元，以及0.3的概率dropout进行正则 regressor.add(LSTM(units = 128)) regressor.add(Dropout(0.3)) regressor.add(Dense(units = 1)) regressor.compile(optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mean_squared_error\u0026#39;) # 损失函数为均方误差 reduce_lr = ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, patience=5) # 下面的参数都可以进行修改，一般而言batchsize越大会越好些，epochs表示迭代次数，大家根据结果，大概何时收敛即可定为多少 history =regressor.fit(X_train, Y_train, epochs = 80, batch_size = 8,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False) return(regressor, history) # 5. 模型训练 def loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history): train_predict = regressor.predict(X_train) test_predict = regressor.predict(X_test) # 将预测值进行反标准化，即还原 train_predict = scale.inverse_transform(train_predict) Y_train = scale.inverse_transform([Y_train]) test_predict = scale.inverse_transform(test_predict) Y_test = scale.inverse_transform([Y_test]) # 输出训练集和验证集的绝对误差和均方误差 print(\u0026#39;Train Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_train[0], train_predict[:,0])) print(\u0026#39;Train Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))) print(\u0026#39;Test Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_test[0], test_predict[:,0])) print(\u0026#39;Test Root Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))) # 损失值结果 可视化 plt.figure(figsize=(16,8)) plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Train Loss\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;Test Loss\u0026#39;) plt.title(\u0026#39;model loss\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.xlabel(\u0026#39;epochs\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() return(train_predict, test_predict, Y_train, Y_test) # 6. 绘制拟合图，对未来进行预测 def Y_pre(Y_train, Y_test, train_predict, test_predict): Y_real = np.vstack((Y_train.reshape(-1,1), Y_test.reshape(-1,1))) Y_pred = np.vstack((train_predict[:,0].reshape(-1,1), test_predict[:,0].reshape(-1,1))) return(Y_real, Y_pred) def plot_compare(n, Y_real, Y_pred): aa=[x for x in range(n)] plt.figure(figsize=(14,6)) plt.plot(aa, Y_real, marker=\u0026#39;.\u0026#39;, label=\u0026#34;actual\u0026#34;) plt.plot(aa, Y_pred, \u0026#39;r\u0026#39;, label=\u0026#34;prediction\u0026#34;) plt.tight_layout() sns.despine(top=True) plt.subplots_adjust(left=0.07) plt.xticks(size= 15) plt.yticks(size= 15) plt.xlabel(\u0026#39;Time step\u0026#39;, size=15) plt.legend(fontsize=15) plt.show() # 7. 根据一个真实的值预测连续的长度 def predict_sequences_multiple(model, firstValue, length, look_back=1): prediction_seqs = [] curr_frame = firstValue for i in range(length): predicted = [] predicted.append(model.predict(curr_frame[-look_back:])[0,0]) curr_frame = np.insert(curr_frame, i+look_back, predicted[-1], axis=0) prediction_seqs.append(predicted[-1]) return prediction_seqs 运行模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026#39;\u0026#39;\u0026#39;只需要准备好index为时间且连续，时序变量单独为1列的df_x数据框，即可运该模型代码\u0026#39;\u0026#39;\u0026#39; look_back = 1 # 再次说明 look_back = n 表示预测结果主要以过去 n 天作为参考进行预测，大家可以根据想法自行修改 predict_length = 30 # 预测未来30天的数据，个人需要根据自己对未来预测天数的需求进行长度改变，正常而言短期内的预测会较为准确 train_size, test_size, train, test, df_x, scale = data_lstm(df_x) X_train, Y_train, X_test, Y_test = lstm(train[:, 0].reshape(train_size, 1), test[:, 0].reshape(test_size, 1), look_back = look_back) regressor, history = lstm_model(X_train, Y_train, X_test, Y_test) train_predict, test_predict, Y_train, Y_test = loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history) Y_real, Y_pre = Y_pre(Y_train, Y_test, train_predict, test_predict) plot_compare(len(Y_real), Y_real, Y_pre) predictions = predict_sequences_multiple(regressor, X_test[-1,:], predict_length, look_back = look_back) \u0026#39;\u0026#39;\u0026#39;预测未来30天的数据并保存至pre_30数据框\u0026#39;\u0026#39;\u0026#39; pre_30 = scale.inverse_transform(np.array(predictions).reshape(-1, 1)) ","permalink":"https://soso010816.github.io/posts/lstm/","summary":"“ 简单的介绍 ” · 初见LSTM🤦‍♂️ 2022年的美赛之际，巨弱第一次邂逅LSTM，当时巨弱的美赛题是关于时序预测、制定交易策略方面，赛前队长","title":"DL：LSTM With Python(Time Series Question)"},{"content":" “ 简单的介绍 ” 初次接触XGBoost 相信大部分接触过数模、机器学习的朋友都对XGBoost有所耳闻，巨弱我第一次接触它是在2021年12月第一次参加数模比赛时，当时自己的代码水平还只是入门，作为write 论文和分担部分建模的我为了能够给队友搭建的XGBoost进行表面润色和提升，巨弱只能一个劲地翻找各种资料、教程去理解关于XGBoost的原理。\n由于刚接触统计学不到半年，从它的底层架构决策树、GBDT梯度提升树再到XGBoost一个个慢慢消化，这阶段属实花了巨弱大量的时间。最后在将模型的内部结构、数学推导基本理解后，结合大佬队友的最优建模拿到了全国一等奖（当然我认为获奖主要的原因还是在解决另一个问题时用到了复杂的纯数学推导和概率论建模）。\n因此不可否认XGBoost作为“机器学习大杀器”的魅力所在。\nXGBoost基本介绍 XGBoost由GBDT优化而成，XGBoost在GBDT的基础之上，在损失函数中加入正则项，并对损失函数进行二阶泰勒展开、剔除常数项，最后使用贪心算法计算增益后对其树的最佳分割点进行分割，极大程度提高了目标的优化效率，减少了内存的消耗；而GBDT又是由多棵迭代的回归树累加构成，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的公式：残差 = 真实值 - 预测值。\n具体的原理介绍如下：XGBoost的基本介绍(出自知乎)\n“ python代码的介绍 ” 本次代码源于巨弱前段时间在解决一些低维数据分类的问题上多次使用到XGBoost，并且效果非常不错，微调参数之后准确率大部分都达到了 95% 以上，因此巨弱将其进行再封装，方便以后使用时能减少时间，提升效率。代码如下：\n导入包 1 2 3 4 from xgboost import plot_importance from sklearn.model_selection import train_test_split # 划分训练集测试集 from xgboost import XGBRegressor from sklearn.preprocessing import StandardScaler 只需要准备好df_x，df_y，分别为自变量集和输出值的数据集，即可运行分装好的XGBoost模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=7) #7 def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst def xgboost_plot(i = \u0026#39;数值\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False): # i 为输出变量名称，可以进行修改 # n为输出变量在df_y中第几列,默认是第一列 # model_ouput是否返回model # m是否改变模型参数 # scale是否对特征值进行标准化 scaler = StandardScaler() if scale == True: x_train = scaler.fit_transform(x_train) x_test = scaler.transform(x_test) if m == True: xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs = 4) y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train, early_stopping_rounds=5, eval_set=[(x_test, y_test)], verbose=False) else: xgb = XGBRegressor() y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train) # 是否使用标准化，xgboost 结果都一样 y_pred = model_xgb.predict(x_test) y_pred_train = model_xgb.predict(x_train) predictions = [round(value) for value in y_pred] plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;训练集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, y_train.tolist(), label = \u0026#39;训练集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;验证集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, y_test.tolist(), label = \u0026#39;验证集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # 绘制特征值图 plot_importance(model_xgb) plt.show() r2_train = R_2(y_train, y_pred_train) r2_test = R_2(y_test, y_pred) print([r2_train, r2_test]) if model_output==True: return model_xgb 大家可以根据需求自行修改其中的参数，以此优化模型效果。 1 model = xgboost_plot(i = \u0026#39;数值\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False) 下面放一张巨弱使用xgboost建立的一次工业模型图，由图可见模型在训练集和验证集上的拟合度高达 99.9% ，实在是强，大家可以手动试一试： ","permalink":"https://soso010816.github.io/posts/xgboost-python/","summary":"“ 简单的介绍 ” 初次接触XGBoost 相信大部分接触过数模、机器学习的朋友都对XGBoost有所耳闻，巨弱我第一次接触它是在2021年12月第","title":"ML：Simple XGBoost With Python"},{"content":" “ 简单的介绍 ” 在统计学习中，我们也经常遇到高维数据的问题，比如图片处理，图片的处理技术在目前也是非常热门，不断被探索的领域，本次学习blog为大家带来鄙人封装的卷积神经网络python代码，供大家处理基本的图片分类预测问题并将结果进行可视化，当然大家也可以根据自己需求修改代码中的parames参数，从而选择出预测效果最佳的模型。 “ CNN的基本原理 ” 关于CNN的基本介绍大家可以在 A Simple Introduction About CNN 里进行学习。\n“ python代码 ” 本次的卷积神经网络使用的pytorch包，只要求有友友所使用的训练图片数据集标记好并放至所在的文件夹目录，即可以运行鄙人的CNN函数。并且一般卷积神经网络（CNN）主要用于图像处理技术， 因此本代码针对的数据集为 jpg、png等图片数据。\n导入所需的库 pytorch、glob、numpy、sklearn、matplotlib、copy\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rfrom torch import optim\rfrom torchvision import utils\rfrom torch.optim.lr_scheduler import ReduceLROnPlateau\rfrom torchvision import datasets, models, transforms\rfrom torch.utils.data import DataLoader, Dataset\rimport copy\rimport glob\rimport numpy as np\rimport plotly.graph_objs as go\rimport matplotlib.pyplot as plt\rfrom plotly.subplots import make_subplots\rfrom sklearn.model_selection import train_test_split\r%matplotlib inline\r数据集的构建 CPU or CUDA\n# 1. 选择 CPU 还是 GPU 版的 pytorch 进行建模\rdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r# 2. 设置随机种子\rtorch.manual_seed(816)\rif device =='cuda':\rtorch.cuda.manual_seed_all(816)\r# 将所有图片数据的路径存储至列表中, train_dir和 test_dir为训练集图片和验证集图片所在的文件夹\rtrain_list = glob.glob(os.path.join(train_dir,'*.jpg')) # 如果图片为png形式，则将jpg改成png即可\rtest_list = glob.glob(os.path.join(test_dir, '*.jpg')) # 如果有测试集则使用这行代码，否则可以删除\r模型的搭建 数据增强、模型结构设计、模型的训练与保持、结果可视化\n# 3. 定义完整的 CNN 框架模型\rdef So_CNN_model(train_list, test_list = None):\r# 训练集和验证集的分割\rtrain_list, val_list = train_test_split(train_list, test_size=0.2)\r# 4. 图片的预处理，图片增强\rtrain_transforms = transforms.Compose([\rtransforms.Resize((224, 224)), # 设计训练图片转化为224*224图片大小，可以根据需求自行修改大小\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\r])\rval_transforms = transforms.Compose([\rtransforms.Resize((224, 224)), # 设计验证集图片转化为224*224图片大小\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\r])\r# 如果有测试集需要输出，则也对验证集进行图片转化增强\rif test_list != None:\rtest_transforms = transforms.Compose([ transforms.Resize((224, 224)),\rtransforms.RandomResizedCrop(224),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor()\r])\r# 5. 定义一个类进行数据转化、图片数据集的处理、以及图片的标注转化\rclass dataset(torch.utils.data.Dataset):\rdef __init__(self, file_list, transform=None):\rself.file_list = file_list # 图片名字列表\rself.transform = transform # 转化器\rself.label_list = label_list # 所有的标签种类\rdef __len__(self):\rself.filelength = len(self.file_list)\rreturn self.filelength # 图片数目\r# 对于本地图片的下载与标注\rdef __getitem__(self,idx):\rimg_path = self.file_list[idx]\rimg = Image.open(img_path) # 打开本地图片数据集所在的位置\rimg_transformed = self.transform(img) # 数据增强\rlabel = img_path.split('/')[-1].split('.')[0] # 对图片名字分割，前提是图片名字即为标注\rlabel = label_list.index(label) # 搜索该标签在列表中的位置，并将其进行数值标注，有几个种类数值就有几种\rreturn img_transformed, label\r# 6. 定义各个类\rtrain_data = dataset(train_list, transform=train_transforms)\rif test_list != None:\rtest_data = dataset(test_list, transform=test_transforms)\rval_data = dataset(val_list, transform=test_transforms)\r# 7. 数据加载\rbatch_size = 100\rtrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True )\rif test_list != None:\rtest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle=True)\rval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle=True)\r# 8. CNN模型架构的设计\rclass Cnn(nn.Module):\rdef __init__(self):\rsuper(Cnn,self).__init__()\r'''以下所有层的结构参数和层数数量都可以进行需修改\r可以比较参数不同产生的模型结果，从而选择最优参数'''\rc,h,w = params['shape_in'] # 初始 数据结构\rf = params['initial_filters'] # 初始 数据转化的层数\rnum_classes = params['num_classes'] # 需要分类的总数\rnum_fc1 = params['num_fc1'] # 全连接层的第一层\rdropout_rate = params['dropout_rate']\r# 第一层卷积层，将3通道 c 维数据转化为 f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0\rself.layer1 = nn.Sequential(\rnn.Conv2d(c, f, kernel_size = 3, padding=0, stride=2),\rnn.BatchNorm2d(f), # f 维 数据进行标准化处理\rnn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择\rnn.MaxPool2d(2) # 池化层，2*2 矩阵大小进行池化\r)\r# 第一层后，转化得到的维度\rh,w = findConv2dOutShape(h, w, nn.MaxPool2d(2))\rh,w = h/2, w/2\r# 第二层卷积层，将 f 维数据转化为 2*f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0\rself.layer2 = nn.Sequential(\rnn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(2*f), # 2*f 维 数据进行标准化处理\rnn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择\rnn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化\r)\r# 第二层后，转化得到的维度\rh,w = findConv2dOutShape(h, w, nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2))\rh,w = h/2, w/2\r# 第三层卷积层，将 32 维数据转化为 64 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0\rself.layer3 = nn.Sequential(\rnn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(4*f), # 4*f 维 数据进行标准化处理\rnn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择\rnn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化\r)\r# 第三层后，转化得到的维度\th,w = findConv2dOutShape(h, w, nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2))\rh,w = h/2, w/2\r# 第四层卷积层，将 4*f 维数据转化为 8*f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0\rself.layer4 = nn.Sequential(\rnn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2),\rnn.BatchNorm2d(8*f), # 8*f 维 数据进行标准化处理\rnn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择\rnn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化\r)\r# 第四层后，转化得到的维度\rh,w = findConv2dOutShape(h, w, nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2\r# 最后 我设计了2层全连接神经网络结构\rself.num_flatten= h * w* 8*f\rself.fc1 = nn.Linear(self.num_flatten, num_fc1) self.dropout = nn.Dropout(dropout_rate) # 以 0.5 的概率对其进行剔除\rself.fc2 = nn.Linear(num_fc1, num_class)\rself.relu = nn.ReLU() # 定义激活函数 RELU\rself.softmax = nn.log_softmax() # 定义 最后的输出函数 Softmax\r# 定义向前传播\rdef forward(self,x):\rout = self.layer1(x)\rout = self.layer2(out)\rout = self.layer3(out)\rout = self.layer4(out)\rout = out.view(-1, self.num_flatten)\rout = self.relu(self.fc1(out))\rout = self.softmax(self.fc2(out), dim = 1)\rreturn out\rparams_model={\r\u0026quot;shape_in\u0026quot;: (3, 224, 224), \u0026quot;initial_filters\u0026quot;: 8, \u0026quot;num_fc1\u0026quot;: 100,\r\u0026quot;dropout_rate\u0026quot;: 0.25,\r\u0026quot;num_classes\u0026quot;: len(label_list)} # num_class,根据类别的总数而定\r# 传达模型结构给cnn_model\rcnn_model = Cnn(params_model)\rdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\rmodel = cnn_model.to(device)\r# 9. 定义损失函数\rloss_func = nn.NLLLoss(reduction=\u0026quot;sum\u0026quot;)\r# 10. 定义一个优化器，优化器将保持当前状态，并根据计算的梯度更新参数\ropt = optim.Adam(cnn_model.parameters(), lr=3e-4)\rlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)\r# 11. 定义模型训练函数\rdef train_val(model, params,verbose=False):\r# 获取训练参数\repochs = params[\u0026quot;epochs\u0026quot;]\rloss_func = params[\u0026quot;loss_func\u0026quot;]\ropt = params[\u0026quot;optimiser\u0026quot;]\rtrain_dl = params[\u0026quot;train\u0026quot;]\rval_dl = params[\u0026quot;val\u0026quot;]\rcheck = params[\u0026quot;check\u0026quot;]\rlr_scheduler = params[\u0026quot;lr_change\u0026quot;]\rweight_path = params[\u0026quot;weight_path\u0026quot;]\rloss_history = {\u0026quot;train\u0026quot;: [],\u0026quot;val\u0026quot;: []} # 每次 epoch 的训练集和验证集的损失值\rmetric_history = {\u0026quot;train\u0026quot;: [],\u0026quot;val\u0026quot;: []} # 每次 epoch 的 metric值\rbest_model_wts = copy.deepcopy(model.state_dict()) # 深度复制最佳性能模型的权重\rbest_loss = float('inf') # 将最佳的损失值初始化为极大值\r# 迭代循环\rfor epoch in range(epochs):\r# 获取学习率\rcurrent_lr = get_lr(opt)\rif(verbose):\rprint('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\r# 使用训练集训练 CNN 模型\rmodel.train()\rtrain_loss, train_metric = loss_epoch(model,loss_func,train_dl,check,opt)\r# 收集训练数据集的损失和衡量标准\rloss_history[\u0026quot;train\u0026quot;].append(train_loss)\rmetric_history[\u0026quot;train\u0026quot;].append(train_metric)\r# 使用验证集对模型结果进行评估\rmodel.eval()\rwith torch.no_grad():\rval_loss, val_metric = loss_epoch(model, loss_func, val_dl,check)\r# 选择最好的参数模型\rif val_loss \u0026lt; best_loss:\rbest_loss = val_loss\rbest_model_wts = copy.deepcopy(model.state_dict())\r# 存储模型参数至本地文件\rtorch.save(model.state_dict(), weight_path)\rif(verbose):\rprint(\u0026quot;已经保存完训练得到的最好模型！\u0026quot;)\r# 存储验证数据集的损失和衡量标准\rloss_history[\u0026quot;val\u0026quot;].append(val_loss)\rmetric_history[\u0026quot;val\u0026quot;].append(val_metric)\r# 学习率筛选\rlr_scheduler.step(val_loss)\rif current_lr != get_lr(opt):\rif(verbose):\rprint(\u0026quot;已经加载完CNN模型！\u0026quot;)\rmodel.load_state_dict(best_model_wts) if(verbose):\rprint(f\u0026quot;train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\u0026quot;)\rprint(\u0026quot;-\u0026quot;*10) # 存储模型的权重和参数数据至本地\rmodel.load_state_dict(best_model_wts)\rreturn model, loss_history, metric_history\rparams_train={\r\u0026quot;train\u0026quot;: train_dl,\u0026quot;val\u0026quot;: val_dl,\r\u0026quot;epochs\u0026quot;: 50, # 迭代 50 次\r\u0026quot;optimiser\u0026quot;: optim.Adam(cnn_model.parameters(),\rlr=3e-4),\r\u0026quot;lr_change\u0026quot;: ReduceLROnPlateau(opt,\rmode = 'min',\rfactor = 0.5,\rpatience = 20,\rverbose = 0),\r\u0026quot;loss_func\u0026quot;: nn.NLLLoss(reduction = \u0026quot;sum\u0026quot;),\r\u0026quot;weight_path\u0026quot;: \u0026quot;weights.pt\u0026quot;,\r\u0026quot;check\u0026quot;: False, }\r# 训练和验证模型\rcnn_model,loss_hist,metric_hist = train_val(cnn_model, params_train)\r# 训练参数进程\repochs = params_train[\u0026quot;epochs\u0026quot;]\r# 绘制结果图\rfig = make_subplots(rows = 1, cols = 2, subplot_titles = ['损失值-折线图','准确率-折线图'])\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026quot;train\u0026quot;], name = '训练集的损失值'), row = 1, col = 1)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026quot;val\u0026quot;], name = '验证集的损失值'), row = 1, col = 1)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026quot;train\u0026quot;], name = '训练集的准确率'), row = 1, col = 2)\rfig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026quot;val\u0026quot;], name = '验证集的准确率'), row = 1, col = 2)\rfig.update_layout(template = 'plotly_white'); fig.update_layout(margin = {\u0026quot;r\u0026quot;:0,\u0026quot;t\u0026quot;:60,\u0026quot;l\u0026quot;:0,\u0026quot;b\u0026quot;:0}, height= 300)\rfig.show()\rSo_CNN_model(train_list, test_list)\r感谢各位友友能看到最后！附一张我超级喜欢的数学宇宙gif代表结束！\nThe End！\n","permalink":"https://soso010816.github.io/posts/cnn-python/","summary":"“ 简单的介绍 ” 在统计学习中，我们也经常遇到高维数据的问题，比如图片处理，图片的处理技术在目前也是非常热门，不断被探索的领域，本次学习blog","title":"DL：CNN With Python"},{"content":" “ 简短的介绍 ” 这段时间参加的数模模拟刚好用到了深度学习的底层架构神经网络模型，于是自己索性就将我数模中用到的代码封装了以下，做成如下的神经网络模型函数以及包括结果的可视化、拟合度的计算结果输出，方便各位友友可以直接使用。\n后期有时间的话，我也会写一个遗传算法或者粒子群算法（maybe是其他启发式算法）用来和下面的函数结合，自动帮各位找到预测结果最优的模型参数。\n这里是一篇我的关于神经网络的原理介绍，里面有关于神经网络非常详细的介绍： 神经网络由来及原理\n“ Tips ” 当然面对不同的数据，最优的神经网络结构和参数会有所不同，大家可以根据自己的拟合结果，修改我下面的函数参数，从而获取最优模型。\n如果各位友友面对的是分类问题，只需把激活函数改成softmax即可，当然损失函数也可以进行修改，关于损失函数的研究比较有代表性的Huber loss和M-regression，感兴趣的友友可以自行查阅相关文献。\n“ Simple Test ” 下面是本人用下述自己写的代码建立的工业模型拟合结果，拟合度达到了 95% ，大家也可以根据自己的需求和结果的效果修改其中的参数，或者增加隐藏层，从而优化自己的神经网络模型。\n“ python代码 ” from sklearn.model_selection import train_test_split\rfrom sklearn.metrics import accuracy_score、\rimport matplotlib.pyplot as plt\rfrom keras import regularizers\rfrom sklearn.preprocessing import MinMaxScaler\rfrom keras.models import Sequential\rfrom keras.layers import Dense, Dropout from sklearn import preprocessing\rdef NN_Plot(i, X, Y, model_output = False):\r# i 为需要输出图表的y轴标签；\r# X 为自变量的数据集；\r# Y 为输出变量的数据；\r# model_output 表示是否返回 训练后的模型；\r# 1. 数据集标准化\rmin_max_scaler = preprocessing.MinMaxScaler()\rX_scale = min_max_scaler.fit_transform(X)\r# 2.训练集和验证集的划分 X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, test_size=0.3, random_state = n)\r# 3. 模型的结构设计\rmodel = Sequential() # 初始化，很重要\rmodel.add(Dense(units = 1000, # 输出大小，也是该层神经元的个数 activation='relu', # 激励函数-RELU input_shape=(X_train.shape[1],) # 输入大小, 也就是列的大小 )) model.add(Dropout(0.3)) # 丢弃神经元链接概率 model.add(Dense(units = 1000, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation='relu' # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.15))\rmodel.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation='relu' # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.15))\rmodel.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation='relu' # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.2))\rmodel.add(Dense(units = 1, activation='linear',\rkernel_regularizer=regularizers.l2(0.01) # 线性激励函数回归一般在输出层用这个激励函数 )) model.compile(optimizer='adam',\rloss='mse', # 损失函数为均方误差\rmetrics=['accuracy'])\r# 4. 模型的训练，可以自行修改batch——size大小和epoch大小\rhist = model.fit(X_train, Y_train,\rbatch_size = 32, epochs=250, verbose = 2,\rvalidation_data=(X_test, Y_test))\r# 5. 模型的损失值变化图绘制\rplt.plot(hist.history['loss'])\rplt.plot(hist.history['val_loss'])\rplt.title('Model loss')\rplt.ylabel('Loss')\rplt.xlabel('Epoch')\rplt.legend(['Train', 'Val'], loc='upper right')\rplt.show()\ry_pred = model.predict(X_test)\ry_pred_train = model.predict(X_train)\r# 6. 模型在训练集和测试集上的拟合程度图绘制\rplt.figure(figsize=(30,9),dpi = 200)\rplt.subplot(1,2,1)\rls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)]\rplt.plot(ls_x_train, y_pred_train.tolist(), label = '训练集的预测值' , marker = 'o')\rplt.plot(ls_x_train, Y_train.iloc[:,0].tolist(), label = '训练集的真实值',linestyle='--', marker = 'o' )\rplt.ylabel(i, fontsize = 15)\rplt.legend(fontsize = 15)\rplt.xticks(fontsize = 12)\rplt.yticks(fontsize = 12)\rplt.subplot(1,2,2)\rls_x = [x for x in range(1, len(y_pred.tolist())+1)]\rplt.plot(ls_x, y_pred.tolist(), label = '验证集的预测值' , marker = 'o')\rplt.plot(ls_x, Y_test.iloc[:,0].tolist(), label = '验证集的真实值',linestyle='--',marker = 'o')\rplt.ylabel(i, fontsize = 15)\rplt.xticks(fontsize = 12)\rplt.yticks(fontsize = 12)\rplt.legend(fontsize = 15)\r# R方的计算\rr2_train = R_2(Y_train.iloc[:,0].tolist(), y_pred_train)\rr2_test = R_2(Y_test.iloc[:,0].tolist(), y_pred)\rprint([r2_train, r2_test, (r2_train+r2_test)/2 ])\r# 是否返回训练得到的模型\rif model_output==True:\rreturn [model, min_max_scaler]\rdef R_2(y, y_pred):\ry_mean = mean(y)\rsst = sum([(x-y_mean)**2 for x in y])\rssr = sum([(x-y_mean)**2 for x in y_pred])\rsse = sum([(x-y)**2 for x,y in zip(y_pred, y)])\rreturn 1-sse/sst\r感谢观看！The End！\n","permalink":"https://soso010816.github.io/posts/nn-python/","summary":"“ 简短的介绍 ” 这段时间参加的数模模拟刚好用到了深度学习的底层架构神经网络模型，于是自己索性就将我数模中用到的代码封装了以下，做成如下的神经网","title":"DL：Neural Network With Python"},{"content":"","permalink":"https://soso010816.github.io/about/","summary":"","title":"About"}]