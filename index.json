[{"content":" 在 [上一篇post](https://soso010816.github.io/posts/TM tree1) 中，我介绍了机器学习中树模型在时间序列中的运用方式，举例了如何构建特征变量并XGBoost进行时序预测，但并不是所有的时序预测都可以使用树模型进行建模。\n在这篇post中，我将带来一些对树模型在时序方面的运用条件的思考，该想法源于之前我在做时序预测时出现的收敛问题。\n1. 需要至少一个以上的特征变量，并且尽量保证特征变量足够多。可以从时序变量 Y 以外寻找相关的影响因素作为特征变量，也可以从 Y 中提取特征变量 树模型的基本运作原理是通过对变量不断进行分裂，每个特征分割点作为枝节点，所有的叶子即为最终树模型要输出的各种结果。如果没有特征变量或者特征变量过少，将导致无输出结果或者叶子太少，误差过大。\n2. 直接对时序变量预测时，要求时序变量基本平稳，无趋势 在上一篇post中的例子可以看到，xgboost的拟合预测效果不错，但是可以发现使用到的数据是基本平稳，无趋势的。\n为什么需要被预测的时序变量是平稳的呢\n对这个问题的思考在某次模拟时，我利用训练样本训练xgboost预测输出变量在某个区间的最大值发现的，我尝试不断的修改参数，利用启发式算法找区间内的模型最优输出解，但是结果都是：得到的最优解没法超过训练样本中的最大值，这说明模型输出来的最优解结果误差很大，于是我重新回忆树模型的构建原理，思考良久后，最终找到了原因：树模型没有办法进行“外推”。具体解释如下：\n树模型是通过启发式算法与目标函数、损失函数相结合，从而对训练数据求解出最佳分割点与最佳分割数，然后对该节点进行叶子分割，不断重复最后构建出树模型和模型中的 M 片有限的叶子（即 M 个结果），这也决定了不论输入模型的新特征值为多少，最后都将输出为 M 个结果中的一个，尤其当面对求最优解、预测趋势变量时，树模型通过查看数据点属于哪个“叶子”并将训练集中目标变量的平均值分配给该点来进行回归预测，即一旦模型训练结束，不管输入什么变量，结果：前者收敛，介于训练数据的最大和最小值之间；后者：结果也将位于 M 个数值中间，无法“推断”到模型尚未看到的数据，这也是为什么树形模型在时间序列预测时要求序列结果不存在趋势，否则泛化性很低、误差极大。\n举个例子：\n当使用树模型拟合预测下列图中的第二个时序数据时，树模型的拟合效果将会很好，但是一旦使用其预测红线以外的部分数据时，模型得到结果将可能时其他三幅图，此时按我们的直觉判断，红线之外，变量整体上大概率会继续呈上升趋势，因此模型预测的结果将的大概率与实际情况有较大的误差。\n但是这是不是表明不能使用树模型对时序变量进行预测呢？\n答案是肯定是否定的。\n查阅了一些资料后，发现可以换一种思路找到解决方式：\n重新构建预测的目标时序变量。例如，你可以先使用简单的线性模型与时间变量 Y 拟合，使用真实值与预测值之间的残差作为目标输出，从而训练树模型，即预测线性模型与时间变量之间的误差，此时得到最终预测结果为：树模型预测的误差+线性模型的预测值，同时也可以使用变化率作为特征等等。 对趋势的时间序列进行差分。使之转化为平稳的时间序列，以此再构建树模型对差分后的平稳序列进行预测；如下图： ","permalink":"https://soso010816.github.io/posts/tm-tree2/","summary":"在 [上一篇post](https://soso010816.github.io/posts/TM tree1) 中，我介绍了机器学习中树模型在时间序列中的","title":"ML|TS：Reflections on Tree models in Time Series"},{"content":" “ 关于ML树模型在时序方面运用的Post ” 一、对于单个时序变量的预测，如何进行特征变量的构建 1. 对时序变量 Y 进行滞后\n对于时序变量 Y 进行 滞后N步 ，从而形成一个滞后N期的特征变量，其中该列特征变量的 当前值 与在时序变量 Y 在 时间 t-N的值 一致。\n即如果我们对 Y 做一个滞后1步的转移，并在该特征上训练一个模型，则该模型将能够在观察到该系列的当前状态后 提前1步 进行预测。\n如果增加滞后期，比如：将滞后步数增加到 7，将允许模型提前7步进行预测。如果在7天前的未观察到的时间里，有某些外部因素从根本上改变了该系列，该模型将无法捕捉到这些变化，并将返回具有较大误差的预测结果。因此，在最初的滞后期选择中，必须在 最佳预测质量和预测范围的长度之间找到一个平衡。\n2. 计算观察窗口的统计量\n通过设置 N 天的观察窗口 ，通过计算 N 天内 时序变量 Y 的统计量 ，即移动统计量，以此作为当天的特征变量，从而构建出一个特征变量。\n例如：假如时序变量是以 “天” 为时间间隔，则我们可以以当前日期往前 7 天作为观察窗口，计算滞后7天至滞后1天内时序变量 Y 的方差，以此作为当前值所对应的一个特征值。使用该特征训练模型，则该模型将能够观察到该时序变量 Y 的当前状态的 前 7 天的波动情况 ，以此进行预测。\n除了方差之外，我们也可以选择观测窗口中的最大值、最小值、平均值、中位数等统计量作为特征值，并且每个特征值所解释的效果会有所不同，甚至也可以改变窗口内每一期的权重从而计算。\n3. 给日期和时间进行标记\n对于时间为具体日期或者时间的时序变量，我们可以对具体时间贴上标签，并转化为布尔值或者数值从而构建出特征值。\n例如：时间是具体的日期，时序变量 Y 为商城的人流量，则我们可以对 Y 所对应的每个具体日期按周标记或者特殊节假日标记，比如对每个日期是一周中的第几天并对其进行标记；或者该日期是否对应特殊的节假日，并将其标记为布尔值，从而构建出新的特征值。使用该特征训练得到的模型，将能够观察到工作日和周末、节假日对时序变量 Y 带来的影响。\n除此之外，面对不同类型的数据，我们还可以标记年份、季节、月份、一小时中的分钟、一天中的小时、特殊事件的发生等等以此类推，不同的标记会使模型观察得到不同的效果，可以根据预测效果的质量，设置标签，从而帮助模型优化预测效果。\n二、 树模型XGBoost在时间序列的运用例子 下例子使用的时序变量 Y 为PJM东部地区2001-2018年的每小时能耗。\n1. 导入包\n1 2 3 4 5 6 7 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import xgboost as xgb from xgboost import plot_importance, plot_tree from sklearn.metrics import mean_squared_error, mean_absolute_error 2. 读取数据、区分验证集和训练集\n1 2 3 4 5 6 7 8 9 10 pjme = pd.read_csv(\u0026#39;../input/PJME_hourly.csv\u0026#39;, index_col=[0], parse_dates=[0]) # 以2015年作为训练集和验证集的分割点 split_date = \u0026#39;01-Jan-2015\u0026#39; pjme_train = pjme.loc[pjme.index \u0026lt;= split_date].copy() pjme_test = pjme.loc[pjme.index \u0026gt; split_date].copy() # 绘制数据的观测图 color_pal = [\u0026#34;#F8766D\u0026#34;, \u0026#34;#D39200\u0026#34;, \u0026#34;#93AA00\u0026#34;, \u0026#34;#00BA38\u0026#34;, \u0026#34;#00C19F\u0026#34;, \u0026#34;#00B9E3\u0026#34;, \u0026#34;#619CFF\u0026#34;, \u0026#34;#DB72FB\u0026#34;] _ = pjme.plot(style=\u0026#39;.\u0026#39;, figsize=(15,5), color=color_pal[0], title=\u0026#39;PJM East\u0026#39;) 结果如下（可以看出基本为平稳序列）： 3. 读取数据，构建特征变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 以拆分、标记时间日期作为特征值 def create_features(df, label=None): df[\u0026#39;date\u0026#39;] = df.index df[\u0026#39;hour\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.hour df[\u0026#39;dayofweek\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.dayofweek df[\u0026#39;quarter\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.quarter df[\u0026#39;month\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.month df[\u0026#39;year\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.year df[\u0026#39;dayofyear\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.dayofyear df[\u0026#39;dayofmonth\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.day df[\u0026#39;weekofyear\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.weekofyear X = df[[\u0026#39;hour\u0026#39;,\u0026#39;dayofweek\u0026#39;,\u0026#39;quarter\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;year\u0026#39;, \u0026#39;dayofyear\u0026#39;,\u0026#39;dayofmonth\u0026#39;,\u0026#39;weekofyear\u0026#39;]] if label: y = df[label] return X, y return X X_train, y_train = create_features(pjme_train, label=\u0026#39;PJME_MW\u0026#39;) X_test, y_test = create_features(pjme_test, label=\u0026#39;PJME_MW\u0026#39;) 4. 建立和训练模型\n1 2 3 4 5 reg = xgb.XGBRegressor(n_estimators=1000) reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=False) 5. 在验证集上验证结果\n1 2 3 4 pjme_test[\u0026#39;MW_Prediction\u0026#39;] = reg.predict(X_test) pjme_all = pd.concat([pjme_test, pjme_train], sort=False) # 结果可视化 pjme_all[[\u0026#39;PJME_MW\u0026#39;,\u0026#39;MW_Prediction\u0026#39;]].plot(figsize=(15, 5)) 结果如下： 6. 输出模型在验证集上的均方误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）\n均方误差计算公式：$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2$ 平均绝对误差计算公式：$MAE =\\frac{\\sum_{i=1}^{n}|y_i-\\hat{y_i}|}{n}$ 平均百分比误差计算公式：$\\frac{100}{n}\\sum_{i=1}^n\\frac{|y_i-\\hat{y_i}|}{y_i}$ 1 2 3 4 5 6 7 8 9 10 11 12 13 # 均方误差 mean_squared_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) # 平均绝对误差 mean_absolute_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) # 平均绝对百分比误差 def mean_absolute_percentage_error(y_true, y_pred): \u0026#34;\u0026#34;\u0026#34;Calculates MAPE given y_true and y_pred\u0026#34;\u0026#34;\u0026#34; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 mean_absolute_percentage_error(y_true=pjme_test[\u0026#39;PJME_MW\u0026#39;], y_pred=pjme_test[\u0026#39;MW_Prediction\u0026#39;]) 结果为：RMSE 为 13780445；MAE 为2848.89；MAPE 为*8.9%\n","permalink":"https://soso010816.github.io/posts/tm-tree1/","summary":"“ 关于ML树模型在时序方面运用的Post ” 一、对于单个时序变量的预测，如何进行特征变量的构建 1. 对时序变量 Y 进行滞后 对于时序变量 Y 进行 滞后N步","title":"ML|TS：Tree Models With Time Series"},{"content":" “ 简单的介绍 ” · 初见LSTM🤦‍♂️ 2022年的美赛之际，巨弱第一次邂逅LSTM，当时巨弱的美赛题是关于时序预测、制定交易策略方面，赛前队长让巨弱去接触RNN（循环神经网络）和 Transformer，于是乎在寒假了解这两模型时，巨弱碰巧也接触到了LSTM（长短期记忆模型），当时的巨弱难以启齿，什么记忆门、遗忘门，实在是让一个低水平的人能以理解，好在当时的巨弱有些神经网络的基础并且硬着头皮在生吃RNN资料后，再去理解LSTM最后终于算是入了个门。不过可惜后来美赛并未使用到LSTM，因此巨弱与它的关系算是告一段落。\n· 再见LSTM🤦‍♀️ 上天注定，机缘巧合，前段时间巨弱遇到了时间序列方面的预测问题需要解决，巨弱搜遍脑海除了ARIMA等，就是LSTM了，于是乎这段封尘已久的故事又再次续写。这次搭建的LSTM确实没让人失望，拟合度直接接近完美，不得不说这让我对LSTM的感情好像上升了一个级别(/ω＼)。 下面是一张巨弱使用python代码对 kaggle: ads 数据集搭建LSTM模型的训练拟合图，效果看上去是不是还可以😊：\n“ LSTM简介 ” LSTM（Long short-term memory, 长短期记忆） 是一种特殊的 RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。LSTM 通过门控状态来控制传输状态，从而记住需要长时间记忆的，同时忘记不重要的信息，对很多需要 “长期记忆” 的任务尤其好用。\n这里是巨弱找的一篇关于LSTM原理的完整讲解：Understanding LSTM Networks\n“ 代码说明 ” 只需要准备好index为时间且连续以及将需要拟合预测的时序变量单独作为1列的df_x数据集，即可运该模型代码，代码的末尾设置了look_back和predict_length参数，分别表示模型拟合预测的结果主要以过去 look_back = n天作为参考进行预测和使用训练得到的模型预测长度为 predict_length 的未来数据，默认为look_back = 1，predict_length = 30 大家可以根据需求进行修改。\n除此之外，大家也可以对lstm_model函数中的结构进行修改，以此根据需求优化并选择最优的LSTM模型。\n导入必要的包 1 2 3 4 5 6 7 8 9 10 import statsmodels.api as sm from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_absolute_error , mean_squared_error from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization from tensorflow.keras.models import Sequential from pylab import * import seaborn as sns from matplotlib.font_manager import FontProperties mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] 模型的搭建和训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def data_lstm(df): # 1. 时序数据的标准化处理及训练集和验证集划分 scale = MinMaxScaler(feature_range = (0, 1)) df = scale.fit_transform(df) train_size = int(len(df) * 0.80) test_size = len(df) - train_size # 2. 由于时序变量具有先后关系，因此划分数据集时一般先前作为训练集、后者作为验证集 train, test = df[0:train_size, :], df[train_size:len(df), :] return(train_size, test_size, train, test, df, scale) # 2. lookback 表示以过去的几个日期作为主要预测变量,这里我选择的默认为1 # 输入数据集 和 输出数据集 的的建立 def create_data_set(dataset, look_back=1): data_x, data_y = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i:(i+look_back), 0] data_x.append(a) data_y.append(dataset[i + look_back, 0]) return np.array(data_x), np.array(data_y) # 3. 训练集和验证集的数据转化 def lstm(train, test, look_back=1): X_train,Y_train,X_test,Y_test = [],[],[],[] X_train,Y_train = create_data_set(train, look_back) X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) X_test, Y_test = create_data_set(test, look_back) X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) return (X_train, Y_train, X_test, Y_test) # 4. 定义LSTM模型结构， 内部的结构参数可以根模型的拟合结果进行修改 def lstm_model(X_train, Y_train, X_test, Y_test): # 第一层，256个神经元，以及0.3的概率dropout进行正则 regressor = Sequential() regressor.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1))) regressor.add(Dropout(0.3)) # 第二层，128个神经元，以及0.3的概率dropout进行正则 regressor.add(LSTM(units = 128, return_sequences = True)) regressor.add(Dropout(0.3)) # 第三层，128个神经元，以及0.3的概率dropout进行正则 regressor.add(LSTM(units = 128)) regressor.add(Dropout(0.3)) regressor.add(Dense(units = 1)) regressor.compile(optimizer = \u0026#39;adam\u0026#39;, loss = \u0026#39;mean_squared_error\u0026#39;) # 损失函数为均方误差 reduce_lr = ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, patience=5) # 下面的参数都可以进行修改，一般而言batchsize越大会越好些，epochs表示迭代次数，大家根据结果，大概何时收敛即可定为多少 history =regressor.fit(X_train, Y_train, epochs = 80, batch_size = 8,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False) return(regressor, history) # 5. 模型训练 def loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history): train_predict = regressor.predict(X_train) test_predict = regressor.predict(X_test) # 将预测值进行反标准化，即还原 train_predict = scale.inverse_transform(train_predict) Y_train = scale.inverse_transform([Y_train]) test_predict = scale.inverse_transform(test_predict) Y_test = scale.inverse_transform([Y_test]) # 输出训练集和验证集的绝对误差和均方误差 print(\u0026#39;Train Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_train[0], train_predict[:,0])) print(\u0026#39;Train Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))) print(\u0026#39;Test Mean Absolute Error:\u0026#39;, mean_absolute_error(Y_test[0], test_predict[:,0])) print(\u0026#39;Test Root Mean Squared Error:\u0026#39;,np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))) # 损失值结果 可视化 plt.figure(figsize=(16,8)) plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Train Loss\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;Test Loss\u0026#39;) plt.title(\u0026#39;model loss\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.xlabel(\u0026#39;epochs\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() return(train_predict, test_predict, Y_train, Y_test) # 6. 绘制拟合图，对未来进行预测 def Y_pre(Y_train, Y_test, train_predict, test_predict): Y_real = np.vstack((Y_train.reshape(-1,1), Y_test.reshape(-1,1))) Y_pred = np.vstack((train_predict[:,0].reshape(-1,1), test_predict[:,0].reshape(-1,1))) return(Y_real, Y_pred) def plot_compare(n, Y_real, Y_pred): aa=[x for x in range(n)] plt.figure(figsize=(14,6)) plt.plot(aa, Y_real, marker=\u0026#39;.\u0026#39;, label=\u0026#34;actual\u0026#34;) plt.plot(aa, Y_pred, \u0026#39;r\u0026#39;, label=\u0026#34;prediction\u0026#34;) plt.tight_layout() sns.despine(top=True) plt.subplots_adjust(left=0.07) plt.xticks(size= 15) plt.yticks(size= 15) plt.xlabel(\u0026#39;Time step\u0026#39;, size=15) plt.legend(fontsize=15) plt.show() # 7. 根据一个真实的值预测连续的长度 def predict_sequences_multiple(model, firstValue, length, look_back=1): prediction_seqs = [] curr_frame = firstValue for i in range(length): predicted = [] predicted.append(model.predict(curr_frame[-look_back:])[0,0]) curr_frame = np.insert(curr_frame, i+look_back, predicted[-1], axis=0) prediction_seqs.append(predicted[-1]) return prediction_seqs 运行模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026#39;\u0026#39;\u0026#39;只需要准备好index为时间且连续，时序变量单独为1列的df_x数据框，即可运该模型代码\u0026#39;\u0026#39;\u0026#39; look_back = 1 # 再次说明 look_back = n 表示预测结果主要以过去 n 天作为参考进行预测，大家可以根据想法自行修改 predict_length = 30 # 预测未来30天的数据，个人需要根据自己对未来预测天数的需求进行长度改变，正常而言短期内的预测会较为准确 train_size, test_size, train, test, df_x, scale = data_lstm(df_x) X_train, Y_train, X_test, Y_test = lstm(train[:, 0].reshape(train_size, 1), test[:, 0].reshape(test_size, 1), look_back = look_back) regressor, history = lstm_model(X_train, Y_train, X_test, Y_test) train_predict, test_predict, Y_train, Y_test = loss_epoch(regressor, X_train, Y_train, X_test, Y_test, scale, history) Y_real, Y_pre = Y_pre(Y_train, Y_test, train_predict, test_predict) plot_compare(len(Y_real), Y_real, Y_pre) predictions = predict_sequences_multiple(regressor, X_test[-1,:], predict_length, look_back = look_back) \u0026#39;\u0026#39;\u0026#39;预测未来30天的数据并保存至pre_30数据框\u0026#39;\u0026#39;\u0026#39; pre_30 = scale.inverse_transform(np.array(predictions).reshape(-1, 1)) ","permalink":"https://soso010816.github.io/posts/lstm/","summary":"“ 简单的介绍 ” · 初见LSTM🤦‍♂️ 2022年的美赛之际，巨弱第一次邂逅LSTM，当时巨弱的美赛题是关于时序预测、制定交易策略方面，赛前队长","title":"DL|TS：LSTM With Python(Time Series Question)"},{"content":" “ 简单的介绍 ” 初次接触XGBoost 相信大部分接触过数模、机器学习的朋友都对XGBoost有所耳闻，巨弱我第一次接触它是在2021年12月第一次参加数模比赛时，当时自己的代码水平还只是入门，作为write 论文和分担部分建模的我为了能够给队友搭建的XGBoost进行表面润色和提升，巨弱只能一个劲地翻找各种资料、教程去理解关于XGBoost的原理。\n由于刚接触统计学不到半年，从它的底层架构决策树、GBDT梯度提升树再到XGBoost一个个慢慢消化，这阶段属实花了巨弱大量的时间。最后在将模型的内部结构、数学推导基本理解后，结合大佬队友的最优建模拿到了全国一等奖（当然我认为获奖主要的原因还是在解决另一个问题时用到了复杂的纯数学推导和概率论建模）。\n因此不可否认XGBoost作为“机器学习大杀器”的魅力所在。\nXGBoost基本介绍 XGBoost由GBDT优化而成，XGBoost在GBDT的基础之上，在损失函数中加入正则项，并对损失函数进行二阶泰勒展开、剔除常数项，最后使用贪心算法计算增益后对其树的最佳分割点进行分割，极大程度提高了目标的优化效率，减少了内存的消耗；而GBDT又是由多棵迭代的回归树累加构成，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的公式：残差 = 真实值 - 预测值。\n具体的原理介绍如下：XGBoost的基本介绍(出自知乎)\n“ python代码的介绍 ” 本次代码源于巨弱前段时间在解决一些低维数据分类的问题上多次使用到XGBoost，并且效果非常不错，微调参数之后准确率大部分都达到了 95% 以上，因此巨弱将其进行再封装，方便以后使用时能减少时间，提升效率。代码如下：\n导入包 1 2 3 4 from xgboost import plot_importance from sklearn.model_selection import train_test_split # 划分训练集测试集 from xgboost import XGBRegressor from sklearn.preprocessing import StandardScaler 只需要准备好df_x，df_y，分别为自变量集和输出值的数据集，即可运行分装好的XGBoost模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=7) #7 def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst def xgboost_plot(i = \u0026#39;数值\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False): # i 为输出变量名称，可以进行修改 # n为输出变量在df_y中第几列,默认是第一列 # model_ouput是否返回model # m是否改变模型参数 # scale是否对特征值进行标准化 scaler = StandardScaler() if scale == True: x_train = scaler.fit_transform(x_train) x_test = scaler.transform(x_test) if m == True: xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs = 4) y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train, early_stopping_rounds=5, eval_set=[(x_test, y_test)], verbose=False) else: xgb = XGBRegressor() y_train = y_train.iloc[:, n] y_test = y_test.iloc[:, n] model_xgb = xgb.fit(x_train, y_train) # 是否使用标准化，xgboost 结果都一样 y_pred = model_xgb.predict(x_test) y_pred_train = model_xgb.predict(x_train) predictions = [round(value) for value in y_pred] plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;训练集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, y_train.tolist(), label = \u0026#39;训练集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;验证集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, y_test.tolist(), label = \u0026#39;验证集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # 绘制特征值图 plot_importance(model_xgb) plt.show() r2_train = R_2(y_train, y_pred_train) r2_test = R_2(y_test, y_pred) print([r2_train, r2_test]) if model_output==True: return model_xgb 大家可以根据需求自行修改其中的参数，以此优化模型效果。 1 model = xgboost_plot(i = \u0026#39;数值\u0026#39;, n=0, y_train, y_test, x_train, x_test, model_output = False, m=False, scale = False) 下面放一张巨弱使用xgboost建立的一次工业模型图，由图可见模型在训练集和验证集上的拟合度高达 99.9% ，实在是强，大家可以手动试一试： ","permalink":"https://soso010816.github.io/posts/xgboost-python/","summary":"“ 简单的介绍 ” 初次接触XGBoost 相信大部分接触过数模、机器学习的朋友都对XGBoost有所耳闻，巨弱我第一次接触它是在2021年12月第","title":"ML：Simple XGBoost With Python"},{"content":" “ 简单的介绍 ” 在统计学习中，我们也经常遇到高维数据的问题，比如图片处理，图片的处理技术在目前也是非常热门，不断被探索的领域，本次学习blog为大家带来鄙人封装的卷积神经网络python代码，供大家处理基本的图片分类预测问题并将结果进行可视化，当然大家也可以根据自己需求修改代码中的parames参数，从而选择出预测效果最佳的模型。 “ CNN的基本原理 ” 关于CNN的基本介绍大家可以在 A Simple Introduction About CNN 里进行学习。\n“ python代码 ” 本次的卷积神经网络使用的pytorch包，只要求有友友所使用的训练图片数据集标记好并放至所在的文件夹目录，即可以运行鄙人的CNN函数。并且一般卷积神经网络（CNN）主要用于图像处理技术， 因此本代码针对的数据集为 jpg、png等图片数据。\n导入所需的库 pytorch、glob、numpy、sklearn、matplotlib、copy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch import torch.nn as nn import torch.nn.functional as F from torch import optim from torchvision import utils from torch.optim.lr_scheduler import ReduceLROnPlateau from torchvision import datasets, models, transforms from torch.utils.data import DataLoader, Dataset import copy import glob import numpy as np import plotly.graph_objs as go import matplotlib.pyplot as plt from plotly.subplots import make_subplots from sklearn.model_selection import train_test_split %matplotlib inline 数据集的构建 CPU or CUDA\n1 2 3 4 5 6 7 8 9 10 11 # 1. 选择 CPU 还是 GPU 版的 pytorch 进行建模 device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; # 2. 设置随机种子 torch.manual_seed(816) if device ==\u0026#39;cuda\u0026#39;: torch.cuda.manual_seed_all(816) # 将所有图片数据的路径存储至列表中, train_dir和 test_dir为训练集图片和验证集图片所在的文件夹 train_list = glob.glob(os.path.join(train_dir,\u0026#39;*.jpg\u0026#39;)) # 如果图片为png形式，则将jpg改成png即可 test_list = glob.glob(os.path.join(test_dir, \u0026#39;*.jpg\u0026#39;)) # 如果有测试集则使用这行代码，否则可以删除 模型的搭建 数据增强、模型结构设计、模型的训练与保持、结果可视化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 # 3. 定义完整的 CNN 框架模型 def So_CNN_model(train_list, test_list = None): # 训练集和验证集的分割 train_list, val_list = train_test_split(train_list, test_size=0.2) # 4. 图片的预处理，图片增强 train_transforms = transforms.Compose([ transforms.Resize((224, 224)), # 设计训练图片转化为224*224图片大小，可以根据需求自行修改大小 transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ]) val_transforms = transforms.Compose([ transforms.Resize((224, 224)), # 设计验证集图片转化为224*224图片大小 transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ]) # 如果有测试集需要输出，则也对验证集进行图片转化增强 if test_list != None: test_transforms = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor() ]) # 5. 定义一个类进行数据转化、图片数据集的处理、以及图片的标注转化 class dataset(torch.utils.data.Dataset): def __init__(self, file_list, transform=None): self.file_list = file_list # 图片名字列表 self.transform = transform # 转化器 self.label_list = label_list # 所有的标签种类 def __len__(self): self.filelength = len(self.file_list) return self.filelength # 图片数目 # 对于本地图片的下载与标注 def __getitem__(self,idx): img_path = self.file_list[idx] img = Image.open(img_path) # 打开本地图片数据集所在的位置 img_transformed = self.transform(img) # 数据增强 label = img_path.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;.\u0026#39;)[0] # 对图片名字分割，前提是图片名字即为标注 label = label_list.index(label) # 搜索该标签在列表中的位置，并将其进行数值标注，有几个种类数值就有几种 return img_transformed, label # 6. 定义各个类 train_data = dataset(train_list, transform=train_transforms) if test_list != None: test_data = dataset(test_list, transform=test_transforms) val_data = dataset(val_list, transform=test_transforms) # 7. 数据加载 batch_size = 100 train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True ) if test_list != None: test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle=True) val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle=True) # 8. CNN模型架构的设计 class Cnn(nn.Module): def __init__(self): super(Cnn,self).__init__() \u0026#39;\u0026#39;\u0026#39;以下所有层的结构参数和层数数量都可以进行需修改 可以比较参数不同产生的模型结果，从而选择最优参数\u0026#39;\u0026#39;\u0026#39; c,h,w = params[\u0026#39;shape_in\u0026#39;] # 初始 数据结构 f = params[\u0026#39;initial_filters\u0026#39;] # 初始 数据转化的层数 num_classes = params[\u0026#39;num_classes\u0026#39;] # 需要分类的总数 num_fc1 = params[\u0026#39;num_fc1\u0026#39;] # 全连接层的第一层 dropout_rate = params[\u0026#39;dropout_rate\u0026#39;] # 第一层卷积层，将3通道 c 维数据转化为 f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0 self.layer1 = nn.Sequential( nn.Conv2d(c, f, kernel_size = 3, padding=0, stride=2), nn.BatchNorm2d(f), # f 维 数据进行标准化处理 nn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择 nn.MaxPool2d(2) # 池化层，2*2 矩阵大小进行池化 ) # 第一层后，转化得到的维度 h,w = findConv2dOutShape(h, w, nn.MaxPool2d(2)) h,w = h/2, w/2 # 第二层卷积层，将 f 维数据转化为 2*f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0 self.layer2 = nn.Sequential( nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(2*f), # 2*f 维 数据进行标准化处理 nn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择 nn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化 ) # 第二层后，转化得到的维度 h,w = findConv2dOutShape(h, w, nn.Conv2d(f, 2*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # 第三层卷积层，将 32 维数据转化为 64 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0 self.layer3 = nn.Sequential( nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(4*f), # 4*f 维 数据进行标准化处理 nn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择 nn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化 ) # 第三层后，转化得到的维度\th,w = findConv2dOutShape(h, w, nn.Conv2d(2*f, 4*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # 第四层卷积层，将 4*f 维数据转化为 8*f 维数据，卷积的矩阵大小为3*3，步长为2，填白大小为0 self.layer4 = nn.Sequential( nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2), nn.BatchNorm2d(8*f), # 8*f 维 数据进行标准化处理 nn.ReLU(), # 激活函数为 ReLU 函数，也可以根据需要对其进行重新选择 nn.MaxPool2d(2) # 池化层，2*2矩阵大小进行池化 ) # 第四层后，转化得到的维度 h,w = findConv2dOutShape(h, w, nn.Conv2d(4*f, 8*f, kernel_size=3, padding=0, stride=2)) h,w = h/2, w/2 # 最后 我设计了2层全连接神经网络结构 self.num_flatten= h * w* 8*f self.fc1 = nn.Linear(self.num_flatten, num_fc1) self.dropout = nn.Dropout(dropout_rate) # 以 0.5 的概率对其进行剔除 self.fc2 = nn.Linear(num_fc1, num_class) self.relu = nn.ReLU() # 定义激活函数 RELU self.softmax = nn.log_softmax() # 定义 最后的输出函数 Softmax # 定义向前传播 def forward(self,x): out = self.layer1(x) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = out.view(-1, self.num_flatten) out = self.relu(self.fc1(out)) out = self.softmax(self.fc2(out), dim = 1) return out params_model={ \u0026#34;shape_in\u0026#34;: (3, 224, 224), \u0026#34;initial_filters\u0026#34;: 8, \u0026#34;num_fc1\u0026#34;: 100, \u0026#34;dropout_rate\u0026#34;: 0.25, \u0026#34;num_classes\u0026#34;: len(label_list)} # num_class,根据类别的总数而定 # 传达模型结构给cnn_model cnn_model = Cnn(params_model) device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = cnn_model.to(device) # 9. 定义损失函数 loss_func = nn.NLLLoss(reduction=\u0026#34;sum\u0026#34;) # 10. 定义一个优化器，优化器将保持当前状态，并根据计算的梯度更新参数 opt = optim.Adam(cnn_model.parameters(), lr=3e-4) lr_scheduler = ReduceLROnPlateau(opt, mode=\u0026#39;min\u0026#39;,factor=0.5, patience=20,verbose=1) # 11. 定义模型训练函数 def train_val(model, params,verbose=False): # 获取训练参数 epochs = params[\u0026#34;epochs\u0026#34;] loss_func = params[\u0026#34;loss_func\u0026#34;] opt = params[\u0026#34;optimiser\u0026#34;] train_dl = params[\u0026#34;train\u0026#34;] val_dl = params[\u0026#34;val\u0026#34;] check = params[\u0026#34;check\u0026#34;] lr_scheduler = params[\u0026#34;lr_change\u0026#34;] weight_path = params[\u0026#34;weight_path\u0026#34;] loss_history = {\u0026#34;train\u0026#34;: [],\u0026#34;val\u0026#34;: []} # 每次 epoch 的训练集和验证集的损失值 metric_history = {\u0026#34;train\u0026#34;: [],\u0026#34;val\u0026#34;: []} # 每次 epoch 的 metric值 best_model_wts = copy.deepcopy(model.state_dict()) # 深度复制最佳性能模型的权重 best_loss = float(\u0026#39;inf\u0026#39;) # 将最佳的损失值初始化为极大值 # 迭代循环 for epoch in range(epochs): # 获取学习率 current_lr = get_lr(opt) if(verbose): print(\u0026#39;Epoch {}/{}, current lr={}\u0026#39;.format(epoch, epochs - 1, current_lr)) # 使用训练集训练 CNN 模型 model.train() train_loss, train_metric = loss_epoch(model,loss_func,train_dl,check,opt) # 收集训练数据集的损失和衡量标准 loss_history[\u0026#34;train\u0026#34;].append(train_loss) metric_history[\u0026#34;train\u0026#34;].append(train_metric) # 使用验证集对模型结果进行评估 model.eval() with torch.no_grad(): val_loss, val_metric = loss_epoch(model, loss_func, val_dl,check) # 选择最好的参数模型 if val_loss \u0026lt; best_loss: best_loss = val_loss best_model_wts = copy.deepcopy(model.state_dict()) # 存储模型参数至本地文件 torch.save(model.state_dict(), weight_path) if(verbose): print(\u0026#34;已经保存完训练得到的最好模型！\u0026#34;) # 存储验证数据集的损失和衡量标准 loss_history[\u0026#34;val\u0026#34;].append(val_loss) metric_history[\u0026#34;val\u0026#34;].append(val_metric) # 学习率筛选 lr_scheduler.step(val_loss) if current_lr != get_lr(opt): if(verbose): print(\u0026#34;已经加载完CNN模型！\u0026#34;) model.load_state_dict(best_model_wts) if(verbose): print(f\u0026#34;train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\u0026#34;) print(\u0026#34;-\u0026#34;*10) # 存储模型的权重和参数数据至本地 model.load_state_dict(best_model_wts) return model, loss_history, metric_history params_train={ \u0026#34;train\u0026#34;: train_dl,\u0026#34;val\u0026#34;: val_dl, \u0026#34;epochs\u0026#34;: 50, # 迭代 50 次 \u0026#34;optimiser\u0026#34;: optim.Adam(cnn_model.parameters(), lr=3e-4), \u0026#34;lr_change\u0026#34;: ReduceLROnPlateau(opt, mode = \u0026#39;min\u0026#39;, factor = 0.5, patience = 20, verbose = 0), \u0026#34;loss_func\u0026#34;: nn.NLLLoss(reduction = \u0026#34;sum\u0026#34;), \u0026#34;weight_path\u0026#34;: \u0026#34;weights.pt\u0026#34;, \u0026#34;check\u0026#34;: False, } # 训练和验证模型 cnn_model,loss_hist,metric_hist = train_val(cnn_model, params_train) # 训练参数进程 epochs = params_train[\u0026#34;epochs\u0026#34;] # 绘制结果图 fig = make_subplots(rows = 1, cols = 2, subplot_titles = [\u0026#39;损失值-折线图\u0026#39;,\u0026#39;准确率-折线图\u0026#39;]) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026#34;train\u0026#34;], name = \u0026#39;训练集的损失值\u0026#39;), row = 1, col = 1) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = loss_hist[\u0026#34;val\u0026#34;], name = \u0026#39;验证集的损失值\u0026#39;), row = 1, col = 1) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026#34;train\u0026#34;], name = \u0026#39;训练集的准确率\u0026#39;), row = 1, col = 2) fig.add_trace(go.Scatter(x = [*range(1,epochs+1)], y = metric_hist[\u0026#34;val\u0026#34;], name = \u0026#39;验证集的准确率\u0026#39;), row = 1, col = 2) fig.update_layout(template = \u0026#39;plotly_white\u0026#39;); fig.update_layout(margin = {\u0026#34;r\u0026#34;:0,\u0026#34;t\u0026#34;:60,\u0026#34;l\u0026#34;:0,\u0026#34;b\u0026#34;:0}, height= 300) fig.show() So_CNN_model(train_list, test_list) 感谢各位友友能看到最后！附一张我超级喜欢的数学宇宙gif代表结束！\nThe End！\n","permalink":"https://soso010816.github.io/posts/cnn-python/","summary":"“ 简单的介绍 ” 在统计学习中，我们也经常遇到高维数据的问题，比如图片处理，图片的处理技术在目前也是非常热门，不断被探索的领域，本次学习blog","title":"DL：CNN With Python"},{"content":" “ 简短的介绍 ” 这段时间参加的数模模拟刚好用到了深度学习的底层架构神经网络模型，于是自己索性就将我数模中用到的代码封装了以下，做成如下的神经网络模型函数以及包括结果的可视化、拟合度的计算结果输出，方便各位友友可以直接使用。\n后期有时间的话，我也会写一个遗传算法或者粒子群算法（maybe是其他启发式算法）用来和下面的函数结合，自动帮各位找到预测结果最优的模型参数。\n这里是一篇我的关于神经网络的原理介绍，里面有关于神经网络非常详细的介绍： 神经网络由来及原理\n“ Tips ” 当然面对不同的数据，最优的神经网络结构和参数会有所不同，大家可以根据自己的拟合结果，修改我下面的函数参数，从而获取最优模型。\n如果各位友友面对的是分类问题，只需把激活函数改成softmax即可，当然损失函数也可以进行修改，关于损失函数的研究比较有代表性的Huber loss和M-regression，感兴趣的友友可以自行查阅相关文献。\n“ Simple Test ” 下面是本人用下述自己写的代码建立的工业模型拟合结果，拟合度达到了 95% ，大家也可以根据自己的需求和结果的效果修改其中的参数，或者增加隐藏层，从而优化自己的神经网络模型。\n“ python代码 ” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score、 import matplotlib.pyplot as plt from keras import regularizers from sklearn.preprocessing import MinMaxScaler from keras.models import Sequential from keras.layers import Dense, Dropout from sklearn import preprocessing def NN_Plot(i, X, Y, model_output = False): # i 为需要输出图表的y轴标签； # X 为自变量的数据集； # Y 为输出变量的数据； # model_output 表示是否返回 训练后的模型； # 1. 数据集标准化 min_max_scaler = preprocessing.MinMaxScaler() X_scale = min_max_scaler.fit_transform(X) # 2.训练集和验证集的划分 X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, test_size=0.3, random_state = n) # 3. 模型的结构设计 model = Sequential() # 初始化，很重要 model.add(Dense(units = 1000, # 输出大小，也是该层神经元的个数 activation=\u0026#39;relu\u0026#39;, # 激励函数-RELU input_shape=(X_train.shape[1],) # 输入大小, 也就是列的大小 )) model.add(Dropout(0.3)) # 丢弃神经元链接概率 model.add(Dense(units = 1000, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation=\u0026#39;relu\u0026#39; # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.15)) model.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation=\u0026#39;relu\u0026#39; # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.15)) model.add(Dense(units = 500, kernel_regularizer=regularizers.l2(0.01), # 施加在权重上的正则项 activity_regularizer=regularizers.l1(0.01), # 施加在输出上的正则项 activation=\u0026#39;relu\u0026#39; # 激励函数 # bias_regularizer=keras.regularizers.l1_l2(0.01) # 施加在偏置向量上的正则项 )) model.add(Dropout(0.2)) model.add(Dense(units = 1, activation=\u0026#39;linear\u0026#39;, kernel_regularizer=regularizers.l2(0.01) # 线性激励函数回归一般在输出层用这个激励函数 )) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mse\u0026#39;, # 损失函数为均方误差 metrics=[\u0026#39;accuracy\u0026#39;]) # 4. 模型的训练，可以自行修改batch——size大小和epoch大小 hist = model.fit(X_train, Y_train, batch_size = 32, epochs=250, verbose = 2, validation_data=(X_test, Y_test)) # 5. 模型的损失值变化图绘制 plt.plot(hist.history[\u0026#39;loss\u0026#39;]) plt.plot(hist.history[\u0026#39;val_loss\u0026#39;]) plt.title(\u0026#39;Model loss\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.legend([\u0026#39;Train\u0026#39;, \u0026#39;Val\u0026#39;], loc=\u0026#39;upper right\u0026#39;) plt.show() y_pred = model.predict(X_test) y_pred_train = model.predict(X_train) # 6. 模型在训练集和测试集上的拟合程度图绘制 plt.figure(figsize=(30,9),dpi = 200) plt.subplot(1,2,1) ls_x_train = [x for x in range(1, len(y_pred_train.tolist())+1)] plt.plot(ls_x_train, y_pred_train.tolist(), label = \u0026#39;训练集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x_train, Y_train.iloc[:,0].tolist(), label = \u0026#39;训练集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;, marker = \u0026#39;o\u0026#39; ) plt.ylabel(i, fontsize = 15) plt.legend(fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.subplot(1,2,2) ls_x = [x for x in range(1, len(y_pred.tolist())+1)] plt.plot(ls_x, y_pred.tolist(), label = \u0026#39;验证集的预测值\u0026#39; , marker = \u0026#39;o\u0026#39;) plt.plot(ls_x, Y_test.iloc[:,0].tolist(), label = \u0026#39;验证集的真实值\u0026#39;,linestyle=\u0026#39;--\u0026#39;,marker = \u0026#39;o\u0026#39;) plt.ylabel(i, fontsize = 15) plt.xticks(fontsize = 12) plt.yticks(fontsize = 12) plt.legend(fontsize = 15) # R方的计算 r2_train = R_2(Y_train.iloc[:,0].tolist(), y_pred_train) r2_test = R_2(Y_test.iloc[:,0].tolist(), y_pred) print([r2_train, r2_test, (r2_train+r2_test)/2 ]) # 是否返回训练得到的模型 if model_output==True: return [model, min_max_scaler] def R_2(y, y_pred): y_mean = mean(y) sst = sum([(x-y_mean)**2 for x in y]) ssr = sum([(x-y_mean)**2 for x in y_pred]) sse = sum([(x-y)**2 for x,y in zip(y_pred, y)]) return 1-sse/sst 感谢观看！The End！\n","permalink":"https://soso010816.github.io/posts/nn-python/","summary":"“ 简短的介绍 ” 这段时间参加的数模模拟刚好用到了深度学习的底层架构神经网络模型，于是自己索性就将我数模中用到的代码封装了以下，做成如下的神经网","title":"DL：Neural Network With Python"},{"content":"","permalink":"https://soso010816.github.io/posts/diary/","summary":"","title":""},{"content":"","permalink":"https://soso010816.github.io/about/","summary":"","title":"About"}]